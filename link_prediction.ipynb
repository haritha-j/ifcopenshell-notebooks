{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRX0fNk4ObZo"
   },
   "source": [
    "\n",
    "Relationship Prediction\n",
    "===========================================\n",
    "\n",
    "Notebook for training and inference of element connectivity prediction using GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzV-X6jJObZk"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifRsGSMuObZq"
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "import scipy.sparse as sp\n",
    "#from google.colab import drive\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.graph import IndustrialFacilityDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f05yJmDObZq"
   },
   "source": [
    "### Load graph dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "039Csi_hW9yX"
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "\n",
    "#model_path = '/content/drive/MyDrive/graph/'\n",
    "model_path = 'gnn_params/'\n",
    "model_name = \"model_4sage_3mlp.pth\"\n",
    "pred_name = \"pred_4sage_3mlp.pth\"\n",
    "data_path = \"/mnt/c/data/3D_CAD/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUYGNHzduB0E"
   },
   "outputs": [],
   "source": [
    "types = ['FLANGE', 'ELBOW', 'TEE', 'TUBE', 'BEND']\n",
    "np.random.seed(42)\n",
    "test_mode = False\n",
    "use_params = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdY9VscEuNM-"
   },
   "outputs": [],
   "source": [
    "if not test_mode:\n",
    "    path = Path('output/west_ref/')\n",
    "    dataset = IndustrialFacilityDataset(data_path, \"westdeckbox_ref\", use_params, path, 'west')\n",
    "else:\n",
    "    path = Path('output/east_ref/')\n",
    "    dataset = IndustrialFacilityDataset(data_path, \"eastdeckbox\", use_params, path, 'east')\n",
    "g = dataset[0]\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### debugging element ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node ids don't seem to match - probably the graph dataset was extracted from a different version of ifc file with different element ids\n",
    "# first, check ids in the merged file - compare with the ids from the param data\n",
    "# also check ids in the merged file with node dataset\n",
    "\n",
    "# import ifcopenshell\n",
    "# from ifcopenshell.util.selector import Selector\n",
    "\n",
    "# ifc = ifcopenshell.open(data_path +\"merged.ifc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ifc_tee = ifcopenshell.open(data_path +\"deckboxtee_ref.ifc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element_type = 'IFCPIPEFITTING'\n",
    "# #element_type = 'IFCPIPESEGMENT'\n",
    "# selector = Selector()\n",
    "# elements = selector.parse(ifc, '.' + element_type)\n",
    "# print(len(elements))\n",
    "# ids= []\n",
    "# for e in elements:\n",
    "#     ids.append(e.id())\n",
    "# print(len(ids), ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path + 'nodes_westdeckbox_ref.pkl', 'rb') as f:\n",
    "#     node_info = pickle.load(f)\n",
    "#     nodes = node_info[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids_tee = [n[4] for n in nodes if n[0]==1]\n",
    "# #ids_tee = [n[4] for n in nodes]\n",
    "# print(len(ids_tee))\n",
    "# # node_ids = [n[4] for n in nodes if n[0]==3 or n[0]==4]\n",
    "# # print(len(node_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# elements_tee = selector.parse(ifc_tee, '.' + element_type)\n",
    "# print(len(elements_tee))\n",
    "# ids_tee= []\n",
    "# for e in elements_tee:\n",
    "#     ids_tee.append(e.id())\n",
    "# print(len(ids_tee), ids_tee[:10], max(ids_tee), max(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches, non_matches = [], []\n",
    "\n",
    "# for id1 in tqdm(ids_tee):\n",
    "#     found = False\n",
    "#     for i, id2 in enumerate(ids):\n",
    "#         if id1 == id2:\n",
    "#             matches.append((id1, i))\n",
    "#             found = True\n",
    "#             break\n",
    "#     if not found:\n",
    "#         non_matches.append(id1)\n",
    "               \n",
    "# print(len(matches), len(non_matches))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xq8E_7erObZs"
   },
   "source": [
    "Prepare training and testing sets\n",
    "---------------------------------\n",
    "\n",
    "This cell randomly picks 10% of the edges for positive examples in\n",
    "the test set, and leaves the rest for the training set. It then samples\n",
    "the same number of edges for negative examples in both sets.\n",
    "\n",
    "Ignore for evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RI6Beil3ObZt"
   },
   "outputs": [],
   "source": [
    "# Split edge set for training and testing\n",
    "if not test_mode:\n",
    "    u, v = g.edges()\n",
    "\n",
    "    eids = np.arange(g.number_of_edges())\n",
    "    eids = np.random.permutation(eids)\n",
    "    test_size = int(len(eids) * 0.1)\n",
    "    train_size = g.number_of_edges() - test_size\n",
    "    test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "    train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
    "\n",
    "    # Find all negative edges and split them for training and testing\n",
    "    #print(u.numpy().shape, v.numpy().shape)\n",
    "\n",
    "    adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())), \n",
    "                        shape=(g.number_of_nodes(), g.number_of_nodes()))\n",
    "    #print(adj.shape, adj.todense().shape, np.eye(g.number_of_nodes()).shape)\n",
    "    adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
    "    neg_u, neg_v = np.where(adj_neg != 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_negative_graph(g, total_train_neg_eids, train_size):\n",
    "#     t1 = time.perf_counter()\n",
    "    train_neg_eids_eids = np.random.choice(len(total_train_neg_eids), train_size)\n",
    "    train_neg_eids = total_train_neg_eids[train_neg_eids_eids]\n",
    "    train_neg_u = neg_u[train_neg_eids] \n",
    "    train_neg_v = neg_v[train_neg_eids]\n",
    "    train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
    "#     t2 = time.perf_counter()\n",
    "#     print(\"time\", t2-t1)\n",
    "#     print(train_neg_eids[:10])\n",
    "    return train_neg_g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not test_mode:\n",
    "    test_neg_eids = np.random.choice(len(neg_u), test_size)\n",
    "    test_neg_eids = np.sort(test_neg_eids)\n",
    "\n",
    "    total_train_neg_eids = []\n",
    "    k = 0\n",
    "    for i in tqdm(range(len(neg_u))):\n",
    "        if k==len(test_neg_eids):\n",
    "            total_train_neg_eids.append(i)\n",
    "            continue\n",
    "        if i != test_neg_eids[k]:\n",
    "            total_train_neg_eids.append(i)\n",
    "        else:\n",
    "            k+=1\n",
    "    print(len(test_neg_eids) + len(total_train_neg_eids) - len(neg_u))\n",
    "    \n",
    "    test_neg_u, test_neg_v = neg_u[test_neg_eids], neg_v[test_neg_eids]\n",
    "    total_train_neg_eids = np.array(total_train_neg_eids)\n",
    "\n",
    "    #train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhcyOy04ObZu"
   },
   "outputs": [],
   "source": [
    "# remove edges of testset for training\n",
    "if not test_mode:\n",
    "    train_g = dgl.remove_edges(g, eids[:test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfqj5PUFObZw"
   },
   "outputs": [],
   "source": [
    "# construct the positive graph and the negative graph for the training set and the test set respectively.\n",
    "if not test_mode:\n",
    "    train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\n",
    "    #train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "    test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
    "    test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxfARbi9ObZv"
   },
   "source": [
    "GraphSAGE model\n",
    "-------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07x2B1_RObZv"
   },
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "# ----------- 2. create model -------------- #\n",
    "# build a 3-layer GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "        self.conv3 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "#         self.conv4 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "#         self.conv5 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "#         self.conv6 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "#         self.conv7 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "#         self.conv8 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "    \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv3(g, h)\n",
    "#         h = F.relu(h)\n",
    "#         h = self.conv4(g, h)\n",
    "#         h = F.relu(h)\n",
    "#         h = self.conv5(g, h)\n",
    "#         h = F.relu(h)\n",
    "#         h = self.conv6(g, h)\n",
    "#         h = F.relu(h)\n",
    "#         h = self.conv7(g, h)\n",
    "#         h = F.relu(h)\n",
    "#         h = self.conv8(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mk5OgXZZObZx"
   },
   "outputs": [],
   "source": [
    "# compute edge features using dot product\n",
    "\n",
    "import dgl.function as fn\n",
    "\n",
    "class DotPredictor(nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Compute a new edge feature named 'score' by a dot-product between the\n",
    "            # source node feature 'h' and destination node feature 'h'.\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return g.edata['score'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frksQEAnObZx"
   },
   "outputs": [],
   "source": [
    "# ALTERNATIVE: compute edge features using an MLP\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, h_feats):\n",
    "        super().__init__()\n",
    "#         self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
    "#         self.W2 = nn.Linear(h_feats, 1)\n",
    "\n",
    "#         self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
    "#         self.W2 = nn.Linear(h_feats, int(h_feats/2))\n",
    "#         self.W3 = nn.Linear(int(h_feats/2), 1)\n",
    "\n",
    "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
    "        self.W2 = nn.Linear(h_feats, int(h_feats/2))\n",
    "        self.W3 = nn.Linear(int(h_feats/2), 4)\n",
    "        self.W4 = nn.Linear(4, 1)\n",
    "\n",
    "#         self.W1 = nn.Linear(h_feats*2, h_feats*8)\n",
    "#         self.W2 = nn.Linear(h_feats*8, h_feats*2)\n",
    "#         self.W3 = nn.Linear(h_feats*2, int(h_feats/2))\n",
    "#         self.W4 = nn.Linear(int(h_feats/2), 4)\n",
    "#         self.W5 = nn.Linear(4, 1)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        \"\"\"\n",
    "        Computes a scalar score for each edge of the given graph.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        edges :\n",
    "            Has three members ``src``, ``dst`` and ``data``, each of\n",
    "            which is a dictionary representing the features of the\n",
    "            source nodes, the destination nodes, and the edges\n",
    "            themselves.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary of new edge features.\n",
    "        \"\"\"\n",
    "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
    "        #return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
    "        #return {'score': self.W3(F.relu(self.W2(F.relu(self.W1(h))))).squeeze(1)}\n",
    "        return {'score': self.W4(F.relu(self.W3(F.relu(self.W2(F.relu(self.W1(h))))))).squeeze(1)}\n",
    "        #return {'score': self.W5(F.relu(self.W4(F.relu(self.W3(F.relu(self.W2(F.relu(self.W1(h))))))))).squeeze(1)}\n",
    "\n",
    "        #return {'score': self.W1(h).squeeze(1)}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(self.apply_edges)\n",
    "            return g.edata['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytKvY1PPObZy"
   },
   "source": [
    "Training setup\n",
    "-------------\n",
    "\n",
    "\n",
    "\n",
    "The loss function is binary cross entropy loss.\n",
    "\n",
    "\\begin{align}\\mathcal{L} = -\\sum_{u\\sim v\\in \\mathcal{D}}\\left( y_{u\\sim v}\\log(\\hat{y}_{u\\sim v}) + (1-y_{u\\sim v})\\log(1-\\hat{y}_{u\\sim v})) \\right)\\end{align}\n",
    "\n",
    "The evaluation metric  is AUC.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.ndata['feat'].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TELwQMhoObZy"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "feat_size = 16\n",
    "\n",
    "if test_mode:\n",
    "  model = GraphSAGE(g.ndata['feat'].shape[1], feat_size)\n",
    "else:\n",
    "  model = GraphSAGE(train_g.ndata['feat'].shape[1], feat_size)\n",
    "\n",
    "model = model.to(torch.double)\n",
    "# You can replace DotPredictor with MLPPredictor.\n",
    "pred = MLPPredictor(feat_size)\n",
    "# pred = DotPredictor()\n",
    "pred = pred.to(torch.double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    #print(scores)\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "    #w = torch.as_tensor([0.01 for i in range(len(labels))])\n",
    "    #return F.binary_cross_entropy_with_logits(scores, labels, w)\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "    return roc_auc_score(labels, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQb634YtQHfU"
   },
   "outputs": [],
   "source": [
    "from numpy.lib.function_base import average\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pos_score, neg_score, threshold=0.5):\n",
    "    # for x in torch.cat([pos_score, neg_score]):\n",
    "    #   print(x)\n",
    "\n",
    "    scores = torch.sigmoid(torch.cat([pos_score, neg_score])).numpy()\n",
    "    #scores = np.rint(scores).astype(int)\n",
    "    scores[scores > threshold] = 1\n",
    "    scores[scores != 1] = 0\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "    #print(np.average(scores), scores.shape, labels.shape)\n",
    "    return (accuracy_score(labels, scores), precision_score(labels, scores), \n",
    "            recall_score(labels, scores), f1_score(labels, scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "load pre-trained model and run on entire dataset in batches (due to the large size of negative dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmXVcu8u96yN"
   },
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "\n",
    "    # load model\n",
    "    model.load_state_dict(torch.load(model_path + model_name))\n",
    "    pred.load_state_dict(torch.load(model_path + pred_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkrHm4V6Eoni"
   },
   "outputs": [],
   "source": [
    "# validation set\n",
    "if test_mode:\n",
    "  u, v = g.edges()\n",
    "  eids = np.arange(g.number_of_edges())\n",
    "  \n",
    "  adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())), \n",
    "                      shape=(g.number_of_nodes(), g.number_of_nodes()))\n",
    "  adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
    "  neg_u_full, neg_v_full = np.where(adj_neg != 0)\n",
    "  print(len(neg_u_full))\n",
    "  #neg_eids = np.random.choice(len(neg_u), g.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:    \n",
    "    with open(model_path + 'eval/pos_edges_test.pkl', 'wb') as f:\n",
    "                pickle.dump([u,v], f)\n",
    "    with open(model_path + 'eval/neg_edges_test.pkl', 'wb') as f:\n",
    "                pickle.dump([neg_u_full,neg_v_full], f)\n",
    "    print(len(u), len(neg_u_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hSAxeePMgPk"
   },
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    with torch.no_grad():\n",
    "        h = model(g, g.ndata['feat'])\n",
    "        \n",
    "        # evaluate positive examples\n",
    "        val_pos_g = dgl.graph((u, v), num_nodes=g.number_of_nodes())\n",
    "        pos_score = pred(val_pos_g, h).numpy()\n",
    "        with open(model_path + '/eval/pos_score_test.pkl', 'wb') as f:\n",
    "            pickle.dump(pos_score, f)\n",
    "\n",
    "        # evaluate negative examples\n",
    "        sample_size = 10000000\n",
    "        limit = math.ceil(len(neg_u_full)/sample_size)\n",
    "        #print(len(neg_u_full), limit)\n",
    "        neg_scores = []\n",
    "        for i in range(limit):\n",
    "            if i == limit-1:\n",
    "                neg_u, neg_v = neg_u_full[i*sample_size:], neg_v_full[i*sample_size:]\n",
    "            else:\n",
    "                neg_u = neg_u_full[i*sample_size:(i+1)*sample_size]\n",
    "                neg_v = neg_v_full[i*sample_size:(i+1)*sample_size]\n",
    "            print(i, len(neg_u), neg_u[0],  neg_v[0])\n",
    "\n",
    "            val_neg_g = dgl.graph((neg_u, neg_v), num_nodes=g.number_of_nodes())\n",
    "            neg_scores.append(pred(val_neg_g, h).numpy())\n",
    "        \n",
    "        neg_score = np.concatenate(neg_scores)\n",
    "        with open(model_path + '/eval/neg_score_test.pkl', 'wb') as f:\n",
    "            pickle.dump(neg_score, f)\n",
    "#             print('AUC', compute_auc(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "\n",
    "    with open(model_path + '/eval/pos_score_test.pkl', 'rb') as f:\n",
    "        pos_score = pickle.load(f)\n",
    "\n",
    "    with open(model_path + '/eval/neg_score_test.pkl', 'rb') as f:\n",
    "        neg_score = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    print(len(pos_score), len(neg_score), sum(pos_score)/len(pos_score), sum(neg_score)/len(neg_score), neg_score[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    metrics = compute_metrics(torch.from_numpy(pos_score), torch.from_numpy(neg_score))\n",
    "    print('accuracy', metrics[0])\n",
    "    print('precision', metrics[1])\n",
    "    print('recall', metrics[2])\n",
    "    print('f1_score', metrics[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiVgzAd7ObZy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----------- 3. set up loss and optimizer -------------- #\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.001)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "accuracies = []\n",
    "losses = []\n",
    "\n",
    "for e in tqdm(range(1000)):\n",
    "    # forward\n",
    "    #print(len(c))\n",
    "    train_neg_g = construct_negative_graph(g, total_train_neg_eids, train_size)\n",
    "    \n",
    "    h = model(train_g, train_g.ndata['feat'])\n",
    "    pos_score = pred(train_pos_g, h)\n",
    "    neg_score = pred(train_neg_g, h)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "          pos_score = pred(test_pos_g, h)\n",
    "          neg_score = pred(test_neg_g, h)\n",
    "          auc =  compute_auc(pos_score, neg_score)\n",
    "          metrics = compute_metrics(pos_score, neg_score)\n",
    "          test_loss = compute_loss(pos_score, neg_score)\n",
    "          print('epoch {}, training loss: {:.3f}, test loss: {:.3f}, auc: {:.3f}, f1: {:.3f}'.format(e, loss, test_loss, auc, float(metrics[3])))\n",
    "\n",
    "          # metrics = compute_metrics(pos_score, neg_score)\n",
    "          accuracies.append(auc)\n",
    "          losses.append(loss.item())\n",
    "\n",
    "\n",
    "# ----------- 5. check results ------------------------ #\n",
    "print(\"max AUC\", max(accuracies))\n",
    "with torch.no_grad():\n",
    "    pos_score = pred(test_pos_g, h)\n",
    "    neg_score = pred(test_neg_g, h)\n",
    "    print('AUC', compute_auc(pos_score, neg_score))\n",
    "    \n",
    "# for i in range(len(losses)):\n",
    "#     print(i*5, losses[i], accuracies[i])\n",
    "#     print('AUC', compute_auc(pos_score, neg_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XEs4t_LtHHb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    #h = model(train_g, train_g.ndata['feat'])\n",
    "    pos_score = pred(test_pos_g, h)\n",
    "    neg_score = pred(test_neg_g, h)\n",
    "    metrics = compute_metrics(pos_score, neg_score)\n",
    "    print('accuracy', metrics[0])\n",
    "    print('precision', metrics[1])\n",
    "    print('recall', metrics[2])\n",
    "    print('f1_score', metrics[3])\n",
    "    print('auc', compute_auc(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_pos_u), len(test_neg_u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate separate metrics based on element classes\n",
    "# print(g.ndata[\"feat\"][test_pos_u[0]])\n",
    "# print(np.unique(g.ndata[\"feat\"].numpy()[:,0]))\n",
    "\n",
    "type_pairs = [(0., 0.), (0., 1.), (0., 2.), (0., 3.), (0., 4.), \n",
    "              (1., 1.), (1., 2.), (1., 3.), (1., 4.), (2., 2.), \n",
    "              (2., 3.), (2., 4.), (3., 3.), (3., 4.), (4., 4.)]\n",
    "\n",
    "def split_graph(u, v, node_features, type_pairs):\n",
    "    subgraph_edges = {pair: ([], []) for pair in type_pairs}\n",
    "    subgraphs = {}\n",
    "    \n",
    "    for i in range(len(u)):\n",
    "        u_type = node_features[u[i], 0]\n",
    "        v_type = node_features[v[i], 0]\n",
    "        \n",
    "        edge_type = (u_type, v_type)\n",
    "        if edge_type in subgraph_edges:\n",
    "            subgraph_edges[edge_type][0].append(u[i])\n",
    "            subgraph_edges[edge_type][1].append(v[i])\n",
    "\n",
    "    \n",
    "    for pair, (sub_u, sub_v) in subgraph_edges.items():\n",
    "        pos_g = dgl.graph((sub_u, sub_v), num_nodes=g.number_of_nodes())\n",
    "        #test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n",
    "        subgraphs[pair] = pos_g\n",
    "        \n",
    "    return subgraphs\n",
    "\n",
    "pos_subgraphs = split_graph(test_pos_u, test_pos_v, g.ndata[\"feat\"].numpy(), type_pairs)\n",
    "neg_subgraphs = split_graph(test_neg_u, test_neg_v, g.ndata[\"feat\"].numpy(), type_pairs)\n",
    "#print(neg_subgraphs)\n",
    "\n",
    "\n",
    "for pair, pos_sub_g in pos_subgraphs.items():\n",
    "    print((types[int(pair[0])], types[int(pair[1])]))\n",
    "    print(\"positive edges\", pos_sub_g.num_edges())\n",
    "    print(\"negative edges\", neg_subgraphs[pair].num_edges())\n",
    "#     print(f\"Subgraph for type pair {pair}:\")\n",
    "#     print(f\"u: {sub_u}\")\n",
    "#     print(f\"v: {sub_v}\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pos_score = pred(pos_sub_g, h)\n",
    "        neg_score = pred(neg_subgraphs[pair], h)\n",
    "        try:\n",
    "            metrics = compute_metrics(pos_score, neg_score)\n",
    "            print('accuracy', metrics[0])\n",
    "            print('precision', metrics[1])\n",
    "            print('recall', metrics[2])\n",
    "            print('f1_score', metrics[3])\n",
    "            print('auc', compute_auc(pos_score, neg_score))\n",
    "        except:\n",
    "            print(\"metric error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1jeKlDB_K_n"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "# torch.save(model.state_dict(), model_path + model_name)\n",
    "# torch.save(pred.state_dict(), model_path + pred_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4_link_predict.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
