{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpvL68OfBEQC"
   },
   "source": [
    "# Element Parameter Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z7n_pw4SMWl"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ47VNF7fmTS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import os.path\n",
    "import torch\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import importlib\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import functorch\n",
    "from numpy.random import default_rng\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import interact \n",
    "import gc\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from chamferdist import ChamferDistance\n",
    "from pathlib import Path\n",
    "\n",
    "import ifcopenshell\n",
    "import open3d as o3d\n",
    "\n",
    "# from src.elements import *\n",
    "# from src.ifc import *\n",
    "# from src.preparation import *\n",
    "# from src.dataset import *\n",
    "# from src.pointnet import *\n",
    "# from src.visualisation import *\n",
    "# from src.geometry import sq_distance, get_oriented_bbox_from_points\n",
    "# from src.icp import icp_finetuning\n",
    "# from src.chamfer import *\n",
    "# from src.utils import *\n",
    "# from src.plots import plot_error_graph, plot_parameter_errors\n",
    "# from src.pca import testset_PCA\n",
    "# from src.finetune import chamfer_fine_tune, mahalanobis_fine_tune\n",
    "# from src.cloud import add_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpzTlKjmlr2q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed = 42\n",
    "rng = default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xyu78RWIQEQJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path = Path(\"ModelNet10\")\n",
    "# path = Path('/content/drive/MyDrive/ElementNet/')\n",
    "path = Path(\"output/\")\n",
    "# savepath = '/content/drive/MyDrive/ElementNet/'\n",
    "savepath = \"models/\"\n",
    "cuda = torch.device(\"cuda\")\n",
    "\n",
    "noise = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blueprint = \"data/sample.ifc\"\n",
    "temp_dir = \"output/temp/\"\n",
    "target_dir = \"output/tee/test/\"\n",
    "\n",
    "ifcConvert_executable = \"scripts/./IfcConvert\"\n",
    "cloudCompare_executable = \"cloudcompare.CloudCompare\"\n",
    "sample_size = 2048\n",
    "threshold = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Isb_97zOA8Tl"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8W4gOI_P9a9"
   },
   "source": [
    "## Test\n",
    "\n",
    "Analyze results statistically\n",
    "\n",
    "POINTNET++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pOl95glmphX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        Normalize(),\n",
    "        #                    RandomNoise(),\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data and model\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"industrial-facility-relationships/\"))\n",
    "BASE_DIR = os.path.join(BASE_DIR, \"pointnet2\")\n",
    "ROOT_DIR = BASE_DIR\n",
    "sys.path.append(os.path.join(ROOT_DIR, \"models\"))\n",
    "\n",
    "inference = True\n",
    "cloi = True\n",
    "\n",
    "if cloi:\n",
    "    return_coefficients = True\n",
    "\n",
    "if inference:\n",
    "    if cloi:\n",
    "        path = Path(\"cloi/\")\n",
    "        ext = \".pcd\"\n",
    "    else:\n",
    "        #     path = Path('output/bp_data/')\n",
    "        # path = Path('output/east_ref/')\n",
    "        path = Path(\"occluded/west/\")\n",
    "        # path = Path('output/')\n",
    "\n",
    "        # path = Path('/mnt/c/data/3D_CAD/east_clouds/')\n",
    "        ext = \".pcd\"\n",
    "\n",
    "else:\n",
    "    if not noise:\n",
    "        # path = Path(\"occluded/\")\n",
    "        path = Path(\"output/\")\n",
    "    else:\n",
    "        path = Path(\"output/noisy/\")\n",
    "    ext = \".pcd\"\n",
    "\n",
    "cat = \"pipe\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_normals = False\n",
    "cat_targets = {\"elbow\": 14, \"bend\": 14, \"tee\": 19, \"pipe\": 11, \"flange\": 13}\n",
    "\n",
    "if inference:\n",
    "\n",
    "    test_ds = PointCloudData(\n",
    "        path,\n",
    "        valid=True,\n",
    "        folder=\"test\",\n",
    "        category=cat,\n",
    "        transform=train_transforms,\n",
    "        inference=True,\n",
    "        return_coefficients=return_coefficients,\n",
    "    )\n",
    "    targets = cat_targets[cat]\n",
    "else:\n",
    "    test_ds = PointCloudData(\n",
    "        path, valid=True, folder=\"test\", category=cat, transform=train_transforms\n",
    "    )\n",
    "    targets = test_ds.targets\n",
    "\n",
    "testDataLoader = torch.utils.data.DataLoader(dataset=test_ds, batch_size=128)\n",
    "test_criterion = nn.MSELoss()\n",
    "\n",
    "model_name = \"pointnet2_cls_ssg\"\n",
    "model_path = Path(\"pointnet2/log/classification/pointnet2_cls_ssg/\")\n",
    "model = importlib.import_module(model_name)\n",
    "\n",
    "\n",
    "predictor = model.get_model(targets, normal_channel=use_normals)\n",
    "if device != \"cpu\":\n",
    "    predictor = predictor.cuda()\n",
    "\n",
    "# checkpoint = torch.load(model_path/'checkpoints/best_model.pth')\n",
    "checkpoint = torch.load(model_path / \"checkpoints/models/best_model_p_chamfer_0005.pth\")\n",
    "# checkpoint = torch.load(model_path/'checkpoints/models/best_model_t_chamfer_00005_bp.pth')\n",
    "predictor.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_inference(\n",
    "    model, loader, device, calculate_score=False, return_coefficients=False\n",
    "):\n",
    "    predictor = model.eval()\n",
    "    predictions_list, pcd_list, id_list, mean_list, norm_factor_list = (\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for j, data in tqdm(enumerate(loader), total=len(loader)):\n",
    "            if return_coefficients:\n",
    "                points, ids, mean, norm_factor = (\n",
    "                    data[\"pointcloud\"].to(device).float(),\n",
    "                    data[\"id\"].to(device),\n",
    "                    data[\"mean\"].to(device),\n",
    "                    data[\"norm_factor\"].to(device),\n",
    "                )\n",
    "            else:\n",
    "                points, ids = data[\"pointcloud\"].to(device).float(), data[\"id\"].to(\n",
    "                    device\n",
    "                )\n",
    "            points = points.transpose(2, 1)\n",
    "            preds, _ = predictor(points)\n",
    "            preds, points, ids = (\n",
    "                preds.to(torch.device(\"cpu\")),\n",
    "                points.to(torch.device(\"cpu\")),\n",
    "                data[\"id\"].to(torch.device(\"cpu\")),\n",
    "            )\n",
    "            if return_coefficients:\n",
    "                mean = mean.to(torch.device(\"cpu\"))\n",
    "                norm_factor = norm_factor.to(torch.device(\"cpu\"))\n",
    "\n",
    "            for i, pr in enumerate(preds):\n",
    "                predictions_list.append(pr.numpy())\n",
    "                pcd_list.append(points[i].numpy())\n",
    "                id_list.append(ids[i].numpy())\n",
    "\n",
    "                if return_coefficients:\n",
    "                    mean_list.append(mean[i].numpy())\n",
    "                    norm_factor_list.append(norm_factor[i].numpy())\n",
    "\n",
    "        if return_coefficients:\n",
    "            return (predictions_list, pcd_list, id_list, mean_list, norm_factor_list)\n",
    "        return (predictions_list, pcd_list, id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if inference:\n",
    "\n",
    "    if return_coefficients:\n",
    "        predictions_list, cloud_list, id_list, mean_list, norm_factor_list = (\n",
    "            model_inference(\n",
    "                predictor.eval(), testDataLoader, device, return_coefficients=True\n",
    "            )\n",
    "        )\n",
    "        print(norm_factor_list)\n",
    "    else:\n",
    "        predictions_list, cloud_list, id_list = model_inference(\n",
    "            predictor.eval(), testDataLoader, device\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(model, loader, device, criterion):\n",
    "    losses = []\n",
    "    predictor = model.eval()\n",
    "    cloud_list = []\n",
    "    label_list = []\n",
    "    output_list = []\n",
    "    predictions_list = []\n",
    "    inputs_list = []\n",
    "    id_list = []\n",
    "    parameter_id = 0\n",
    "    tot = 0\n",
    "    count = 0\n",
    "\n",
    "    for j, data in tqdm(enumerate(loader), total=len(loader)):\n",
    "        inputs, labels, ids = (\n",
    "            data[\"pointcloud\"].to(device).float(),\n",
    "            data[\"properties\"].to(device),\n",
    "            data[\"id\"].to(device),\n",
    "        )\n",
    "        points, target, ids = (\n",
    "            data[\"pointcloud\"].to(device).float(),\n",
    "            data[\"properties\"].to(device),\n",
    "            data[\"id\"].to(device),\n",
    "        )\n",
    "        points = points.transpose(2, 1)\n",
    "        outputs, _ = predictor(points)\n",
    "        outputs = outputs.to(torch.device(\"cpu\"))\n",
    "        inputs = points.to(torch.device(\"cpu\"))\n",
    "        labels = target.to(torch.device(\"cpu\"))\n",
    "        ids = ids.to(torch.device(\"cpu\"))\n",
    "        # print(data['pointcloud'].size(), labels.size(), outputs.size())\n",
    "\n",
    "        for i in range(outputs.size(0)):\n",
    "            label_list.append(labels[i].numpy())\n",
    "            id_list.append(ids[i].item())\n",
    "            output_list.append(outputs[i][parameter_id].item())\n",
    "            predictions_list.append(outputs[i].numpy())\n",
    "            inputs_list.append(labels[i].numpy())\n",
    "            cloud_list.append(inputs[i].numpy())\n",
    "            ratio = (\n",
    "                (labels[i][parameter_id] - outputs[i][parameter_id])\n",
    "                / labels[i][parameter_id]\n",
    "            ).item()\n",
    "            # print('r', i+count, ids[i].item(), labels[i][parameter_id].item(), outputs[i][parameter_id].item(), ratio)\n",
    "            tot += np.absolute(ratio)\n",
    "            # print('l', labels[i][1].item(), outputs[i][1].item(), ((labels[i][1]-outputs[i][1])/labels[i][1]).item())\n",
    "\n",
    "        count += outputs.size(0)\n",
    "    print(tot / count)\n",
    "\n",
    "    return predictions_list, inputs_list, label_list, output_list, id_list, cloud_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not inference:\n",
    "    with torch.no_grad():\n",
    "        (\n",
    "            predictions_list,\n",
    "            inputs_list,\n",
    "            label_list,\n",
    "            output_list,\n",
    "            id_list,\n",
    "            cloud_list,\n",
    "        ) = test(predictor.eval(), testDataLoader, device, test_criterion)\n",
    "\n",
    "    print(len(predictions_list), len(inputs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not inference:\n",
    "    label_list, output_list, id_list = (\n",
    "        np.array(label_list),\n",
    "        np.array(output_list),\n",
    "        np.array(id_list),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(label_list[:10], len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix negative radii, result of training on noisy data. TODO: figure out why?\n",
    "for i in range(len(predictions_list)):\n",
    "    predictions_list[i][0] = abs(predictions_list[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([p[0] for p in label_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([p[0] for p in predictions_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visually analyse predictions and Fine tune with ICP, calculate chamfer distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate chamfer distance for set of results\n",
    "def chamfer_evaluate(predictions_list, cloud_list, cat):\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    preds_t = torch.tensor(\n",
    "        predictions_list, requires_grad=True, device=cuda, dtype=torch.float\n",
    "    )\n",
    "    cloud_t = torch.tensor(cloud_list, device=cuda, dtype=torch.float)\n",
    "\n",
    "    chamfer_dists = get_chamfer_loss_tensor(preds_t, cloud_t, cat, reduce=False)\n",
    "    chamfer_dists = chamfer_dists.detach().cpu().numpy()\n",
    "\n",
    "    return chamfer_dists\n",
    "\n",
    "\n",
    "# # scaling up and down is required for icp calculations\n",
    "# def chamfer_evaluate_with_icp(predictions_list, cloud_list, id_list, cat, blueprint,  ifcConvert_executable,\n",
    "#                      cloudCompare_executable, temp_dir, target_dir, sample_size,\n",
    "#                      threshold, icp_correction = False):\n",
    "\n",
    "#     preds_list, pcd_list = [], []\n",
    "#     error_count = 0\n",
    "\n",
    "#     # get predictions and pcds\n",
    "#     for i in tqdm(range(len(predictions_list))):\n",
    "#     #for i in tqdm(range(50)):\n",
    "#         pcd_id = id_list[i]\n",
    "#         pcd, preds = cloud_list[i].transpose(1, 0), copy.deepcopy(predictions_list[i])\n",
    "#         #print(preds, inputs_list[i])\n",
    "\n",
    "#         preds = scale_preds(preds.tolist(), cat)\n",
    "#         #pcd, preds = prepare_visualisation(pcd_id, cat, i, cloud_list, predictions_list, path, ext)\n",
    "\n",
    "#         try:\n",
    "#             if  icp_correction:\n",
    "#                 # note: preds are updated in place during ICP\n",
    "#                 _, _ = icp_finetuning(o3d.utility.Vector3dVector(pcd), pcd_id, cat, preds, blueprint, temp_dir, target_dir,\n",
    "#                                      ifcConvert_executable, cloudCompare_executable, sample_size, threshold, False)\n",
    "\n",
    "#             preds_list.append(preds)\n",
    "#             pcd_list.append(pcd)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(\"ICP error\", pcd_id, e)\n",
    "#             error_count += 1\n",
    "\n",
    "#     # calculate chamfer distances\n",
    "#     cuda = torch.device('cuda')\n",
    "#     rescaled_preds = [scale_preds(preds, cat, up=0) for preds in preds_list]\n",
    "#     preds_t = torch.tensor(rescaled_preds, requires_grad=True, device=cuda)\n",
    "#     cloud_t = torch.tensor(cloud_list, device=cuda)\n",
    "\n",
    "#     chamfer_dists = get_chamfer_loss_tensor(preds_t, cloud_t, cat, reduce=False)\n",
    "#     chamfer_dists = chamfer_dists.detach().cpu().numpy()\n",
    "\n",
    "#     print(\"error_count\", error_count)\n",
    "#     return chamfer_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dists = chamfer_evaluate(predictions_list, cloud_list, id_list, cat, blueprint,  ifcConvert_executable,\n",
    "#                      cloudCompare_executable, temp_dir, target_dir, sample_size, threshold, icp_correction = False)\n",
    "dists = chamfer_evaluate(predictions_list, cloud_list, cat)\n",
    "# if inference:\n",
    "#     with open(model_path + 'preds_' + cat + '.pkl', 'wb') as f:\n",
    "#         pickle.dump([predictions_list, id_list, dists], f)\n",
    "\n",
    "plot_error_graph(dists, \"Binned chamfer loss\", max_val=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scaling up and down is required for icp calculations\n",
    "def _visualise_predictions(\n",
    "    predictions_list,\n",
    "    cloud_list,\n",
    "    id_list,\n",
    "    cat,\n",
    "    blueprint,\n",
    "    ifcConvert_executable,\n",
    "    cloudCompare_executable,\n",
    "    temp_dir,\n",
    "    target_dir,\n",
    "    sample_size,\n",
    "    threshold,\n",
    "    icp_correction=False,\n",
    "):\n",
    "    preds_list, pcd_list = [], []\n",
    "    viewer_list, ifc_list = [], []\n",
    "    error_count = 0\n",
    "\n",
    "    # get predictions and pcds\n",
    "    # for i in tqdm(range(len(predictions_list))):\n",
    "    for i in tqdm(range(50)):\n",
    "        try:\n",
    "            pcd_id = id_list[i].item()\n",
    "            pcd, preds = cloud_list[i].transpose(1, 0).tolist(), copy.deepcopy(\n",
    "                predictions_list[i]\n",
    "            )\n",
    "            # print(preds, inputs_list[i])\n",
    "\n",
    "            preds = scale_preds(preds.tolist(), cat)\n",
    "            # print(preds)\n",
    "            # pcd, preds = prepare_visualisation(pcd_id, cat, i, cloud_list, inputs_list, ext)\n",
    "\n",
    "            #         try:\n",
    "            if icp_correction:\n",
    "                # note: preds are updated in place during ICP\n",
    "                viewer, ifc = icp_finetuning(\n",
    "                    o3d.utility.Vector3dVector(pcd),\n",
    "                    pcd_id,\n",
    "                    cat,\n",
    "                    preds,\n",
    "                    blueprint,\n",
    "                    temp_dir,\n",
    "                    target_dir,\n",
    "                    ifcConvert_executable,\n",
    "                    cloudCompare_executable,\n",
    "                    sample_size,\n",
    "                    threshold,\n",
    "                    True,\n",
    "                )\n",
    "            else:\n",
    "                # print(type(preds[0]))\n",
    "                # print(\"lp\", preds)\n",
    "                viewer, ifc = visualize_predictions(\n",
    "                    [pcd], cat, [preds], blueprint, visualize=True\n",
    "                )\n",
    "\n",
    "            preds_list.append(preds)\n",
    "            pcd_list.append(pcd)\n",
    "            viewer_list.append(viewer)\n",
    "            ifc_list.append(ifc)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\" error\", pcd_id, e)\n",
    "            error_count += 1\n",
    "\n",
    "    print(\"error_count\", error_count)\n",
    "    return viewer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viewers = _visualise_predictions(\n",
    "    predictions_list,\n",
    "    cloud_list,\n",
    "    id_list,\n",
    "    cat,\n",
    "    blueprint,\n",
    "    ifcConvert_executable,\n",
    "    cloudCompare_executable,\n",
    "    temp_dir,\n",
    "    target_dir,\n",
    "    sample_size,\n",
    "    threshold,\n",
    "    icp_correction=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for v in viewers:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_parameter_errors(inputs_list, predictions_list, cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BP data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch_visualise(model_path, blueprint, path, ext, device, ifc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge_clouds(path, 'pipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualise robust loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pcd_tensor = get_shape_cloud_tensor(\n",
    "    torch.tensor([predictions_list[0]], device=cuda), cat\n",
    ")\n",
    "\n",
    "# visaulise clouds\n",
    "source = o3d.geometry.PointCloud()\n",
    "tgt = o3d.geometry.PointCloud()\n",
    "robust = o3d.geometry.PointCloud()\n",
    "\n",
    "tgt.points = o3d.utility.Vector3dVector(\n",
    "    target_pcd_tensor[0].detach().cpu().numpy().astype(np.double)\n",
    ")\n",
    "source.points = o3d.utility.Vector3dVector(cloud_list[0].transpose())\n",
    "\n",
    "source.paint_uniform_color([0.0, 0.706, 1])\n",
    "tgt.paint_uniform_color([0.7, 0.70, 0])\n",
    "\n",
    "o3d.visualization.draw_geometries([tgt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "loss, non_robust_fwd, non_robust_bwd = calc_robust_chamfer_loss_tensor(torch.tensor([cloud_list[0].transpose()], device=cuda), target_pcd_tensor)\n",
    "print(non_robust_bwd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.points = o3d.utility.Vector3dVector(cloud_list[0].transpose())\n",
    "tgt.points = o3d.utility.Vector3dVector(\n",
    "    target_pcd_tensor[0].detach().cpu().numpy().astype(np.double)\n",
    ")\n",
    "robust.points = o3d.utility.Vector3dVector(\n",
    "    non_robust_bwd.detach().cpu().numpy().astype(np.double)\n",
    ")\n",
    "\n",
    "source.paint_uniform_color([0.0, 0.706, 1])\n",
    "tgt.paint_uniform_color([0.7, 0.70, 0])\n",
    "robust.paint_uniform_color([1, 0.0, 0.0])\n",
    "\n",
    "o3d.visualization.draw_geometries([source, tgt, robust])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictions_list))\n",
    "vis = 0\n",
    "\n",
    "print(cat)\n",
    "\n",
    "if cat == \"elbow\":\n",
    "    elbow_fix = False\n",
    "else:\n",
    "    elbow_fix = True\n",
    "\n",
    "if cat == \"bend\":\n",
    "    cat = \"elbow\"\n",
    "\n",
    "limit = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gmm parameters\n",
    "gmm_path = Path(\"gaussians/gaussians_128.pkl\")\n",
    "with open(gmm_path, \"rb\") as f:\n",
    "    means, covs, gmm_ids, weights = pickle.load(f)\n",
    "    # means, covs = torch.tensor(means).cuda(), torch.tensor(covs).cuda()\n",
    "\n",
    "# sort gmms by predictions id\n",
    "sorted_means = []\n",
    "sorted_covs = []\n",
    "\n",
    "for i in range(len(id_list)):\n",
    "    idx = gmm_ids.index(id_list[i])\n",
    "    sorted_means.append(means[idx])\n",
    "    sorted_covs.append(covs[idx])\n",
    "\n",
    "means, covs = (\n",
    "    torch.tensor(np.array(sorted_means)).cuda(),\n",
    "    torch.tensor(np.array(sorted_covs)).cuda(),\n",
    ")\n",
    "print(means.shape, covs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# optimise using mahalanobis distance\n",
    "\n",
    "cl_v, v, mahal_modified_preds = mahalanobis_fine_tune(\n",
    "    100,\n",
    "    0.01,\n",
    "    predictions_list[:limit],\n",
    "    cloud_list[:limit],\n",
    "    means[:limit],\n",
    "    covs[:limit],\n",
    "    cat,\n",
    "    blueprint,\n",
    "    alpha=3,\n",
    "    visualise=True,\n",
    "    elbow_fix=elbow_fix,\n",
    "    robust=None,\n",
    "    delta=0.0001,\n",
    "    chamfer=30000\n",
    ")\n",
    "\n",
    "if cat == \"elbow\":\n",
    "    mahal_modified_preds = mahal_modified_preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chamfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# optimise using direction weighted chamfer loss\n",
    "\n",
    "cl_v, v, mahal_modified_preds = chamfer_fine_tune(\n",
    "        100,\n",
    "        0.01,\n",
    "        predictions_list[:limit],\n",
    "        cloud_list[:limit],\n",
    "        cat,\n",
    "        blueprint,\n",
    "        alpha=3,\n",
    "        visualise=True,\n",
    "        elbow_fix=elbow_fix,\n",
    "        robust=None,\n",
    "        delta=0.0001,\n",
    "        k=3,\n",
    "        direction_weight=0.5\n",
    "    )\n",
    "\n",
    "if cat == \"elbow\":\n",
    "    mahal_modified_preds = mahal_modified_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# optimise using 3 neighbour chamfer loss\n",
    "\n",
    "cl_v, v, mahal_modified_preds = chamfer_fine_tune(\n",
    "        100,\n",
    "        0.01,\n",
    "        predictions_list[:limit],\n",
    "        cloud_list[:limit],\n",
    "        cat,\n",
    "        blueprint,\n",
    "        alpha=3,\n",
    "        visualise=True,\n",
    "        elbow_fix=elbow_fix,\n",
    "        robust=None,\n",
    "        delta=0.0001,\n",
    "        k=3,\n",
    "        direction_weight=0.5\n",
    "    )\n",
    "\n",
    "if cat == \"elbow\":\n",
    "    mahal_modified_preds = mahal_modified_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# optimise using pair loss or reverse weighted cd or infocd or blaanced cd\n",
    "loss_func = \"balanced\"\n",
    "cl_v, v, mahal_modified_preds = chamfer_fine_tune(\n",
    "        100,\n",
    "        0.01,\n",
    "        predictions_list[:limit],\n",
    "        cloud_list[:limit],\n",
    "        cat,\n",
    "        blueprint,\n",
    "        alpha=1,\n",
    "        visualise=True,\n",
    "        elbow_fix=elbow_fix,\n",
    "        robust=None,\n",
    "        delta=0.0001,\n",
    "        k=32,\n",
    "        direction_weight=None,\n",
    "        loss_func=loss_func\n",
    "    )\n",
    "\n",
    "if cat == \"elbow\":\n",
    "    mahal_modified_preds = mahal_modified_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# optimise using chamfer loss\n",
    "\n",
    "# non-robust\n",
    "# torch.autograd.set_detect_anomaly(False)\n",
    "if vis:\n",
    "    limit = 64\n",
    "    cl_v_c, v_c, modified_preds = chamfer_fine_tune(\n",
    "        100,\n",
    "        0.01,\n",
    "        predictions_list[:limit],\n",
    "        cloud_list[:limit],\n",
    "        cat,\n",
    "        blueprint,\n",
    "        alpha=8,\n",
    "        visualise=True,\n",
    "        elbow_fix=elbow_fix,\n",
    "        robust=None,\n",
    "        delta=0.0001,\n",
    "    )\n",
    "else:\n",
    "    limit = len(predictions_list)\n",
    "    modified_preds = chamfer_fine_tune(\n",
    "        100,\n",
    "        0.01,\n",
    "        predictions_list[:limit],\n",
    "        cloud_list[:limit],\n",
    "        cat,\n",
    "        blueprint,\n",
    "        alpha=8,\n",
    "        visualise=False,\n",
    "        elbow_fix=elbow_fix,\n",
    "        robust=None,\n",
    "        delta=0.0001,\n",
    "    )\n",
    "    \n",
    "if cat == \"elbow\":\n",
    "    modified_preds = modified_preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cloud_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# optimise using EMD loss\n",
    "print(cat)\n",
    "\n",
    "cl_v, v, mahal_modified_preds = chamfer_fine_tune(\n",
    "        100,\n",
    "        0.01,\n",
    "        predictions_list[:limit],\n",
    "        cloud_list[:limit],\n",
    "        cat,\n",
    "        blueprint,\n",
    "        visualise=True,\n",
    "        elbow_fix=elbow_fix,\n",
    "        loss_func=\"emd\"\n",
    "    )\n",
    "\n",
    "if cat == \"elbow\":\n",
    "    mahal_modified_preds = mahal_modified_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # robust\n",
    "# #torch.autograd.set_detect_anomaly(False)\n",
    "# print(cat)\n",
    "# if cat == 'bend':\n",
    "#     cat = 'elbow'\n",
    "\n",
    "# if cat == \"elbow\":\n",
    "#     elbow_fix = False\n",
    "# else:\n",
    "#     elbow_fix = True\n",
    "\n",
    "# if vis:\n",
    "#     limit = 50\n",
    "#     cl_v, v, modified_preds = chamfer_fine_tune(100, 0.01, predictions_list[:limit],\n",
    "#                                                 cloud_list[:limit], cat, blueprint,\n",
    "#                                                 alpha=1, visualise=True, elbow_fix=elbow_fix,\n",
    "#                                                robust=\"winsor\", delta=0.05,\n",
    "#                                                 bidirectional_robust=False)\n",
    "# else:\n",
    "#     limit = len(predictions_list)\n",
    "#     modified_preds = chamfer_fine_tune(100, 0.01, predictions_list[:limit],\n",
    "#                                        cloud_list[:limit], cat, blueprint, alpha=1,\n",
    "#                                        visualise=False, elbow_fix=elbow_fix,\n",
    "#                                        robust=\"winsor\", delta=0.02,\n",
    "#                                       bidirectional_robust=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if vis:\n",
    "    print(v_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if vis:\n",
    "    for i in range(40, 80, 2):\n",
    "        print(v[i], v[i + 1], v_c[i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if vis:\n",
    "#     print(v_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dists = chamfer_evaluate(modified_preds, cloud_list[:limit], id_list, cat, blueprint,  ifcConvert_executable,\n",
    "#                      cloudCompare_executable, temp_dir, target_dir, sample_size, threshold, icp_correction = False)\n",
    "dists = chamfer_evaluate(modified_preds, cloud_list[:limit], cat)\n",
    "\n",
    "if inference:\n",
    "    with open(path / (\"preds_finetuned_\" + \"pipe\" + \".pkl\"), \"wb\") as f:\n",
    "        pickle.dump([modified_preds, id_list, dists], f)\n",
    "\n",
    "plot_error_graph(dists, \"Fitting Error\", max_val=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robust\n",
    "plot_parameter_errors(inputs_list, modified_preds, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(v), 2):\n",
    "    print(v[i], v[i + 1], dists[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = torch.tensor([predictions_list[1]]).cuda()\n",
    "print(predictions_list[1])\n",
    "pcd = generate_tee_cloud_tensor(preds)\n",
    "# tee = generate_tee_cloud(predictions_list[0])\n",
    "tee = pcd[0].cpu().numpy()\n",
    "tee = o3d.utility.Vector3dVector(tee)\n",
    "tee_cloud = o3d.geometry.PointCloud()\n",
    "tee_cloud.points = tee\n",
    "o3d.io.write_point_cloud(\"tee_cl.pcd\", tee_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original = cloud_list[2]\n",
    "points = o3d.utility.Vector3dVector(original.transpose(1, 0))\n",
    "tee_cloud.points = points\n",
    "o3d.io.write_point_cloud(\"tee_cl_inp.pcd\", tee_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# undo normalisation opf the bp tee dataset created for inference, only for comparison with the inferred tee results\n",
    "tee_path = \"tee_fix/tee/test/\"\n",
    "metadata_file = open(\"tee_fix/tee/metadata.json\", \"r\")\n",
    "metadata = json.load(metadata_file)\n",
    "output_path = \"tee_fix/tee/unnormalised/\"\n",
    "\n",
    "files = os.listdir(tee_path)\n",
    "new_points = []\n",
    "for f in tqdm(files):\n",
    "    cloud_data = metadata[f.split(\".\")[0]]\n",
    "    points = np.array(o3d.io.read_point_cloud(tee_path + f).points)\n",
    "    print(\"a\", points[0])\n",
    "    print(cloud_data[\"norm_factor\"], cloud_data[\"mean\"])\n",
    "    points *= cloud_data[\"norm_factor\"]\n",
    "    print(\"b\", points[0])\n",
    "\n",
    "    for i, pnt in enumerate(points):\n",
    "        pnt += cloud_data[\"mean\"]\n",
    "    print(\"c\", points[10])\n",
    "    new_points.append(points)\n",
    "\n",
    "new_points = o3d.utility.Vector3dVector(np.concatenate(new_points))\n",
    "new_cloud = o3d.geometry.PointCloud()\n",
    "new_cloud.points = new_points\n",
    "o3d.io.write_point_cloud(output_path + \"tee_bp_unnormalised.pcd\", new_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLOI data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalise_pipe_preds(preds, mean, norm_factor):\n",
    "    r, l = preds[0] * norm_factor, preds[1] * norm_factor\n",
    "    p0 = [\n",
    "        (preds[2] * norm_factor) + mean[0],\n",
    "        (preds[3] * norm_factor) + mean[1],\n",
    "        (preds[4] * norm_factor) + mean[2],\n",
    "    ]\n",
    "\n",
    "    return [r, l, p0[0], p0[1], p0[2]] + preds[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to visualise an element, if it fails, remove it from final combined visualisation\n",
    "def check_visualisations(preds_list, cat, blueprint):\n",
    "    successful_preds = []\n",
    "    error_count = 0\n",
    "\n",
    "    for i, pred in tqdm(enumerate(preds_list)):\n",
    "        try:\n",
    "            visualize_predictions([], cat, [pred], blueprint)\n",
    "            successful_preds.append(pred)\n",
    "\n",
    "        except:\n",
    "            error_count += 1\n",
    "\n",
    "    print(\"e\", error_count)\n",
    "    return successful_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_root = \"cloi/\" + cat + \"/test/\"\n",
    "files = os.listdir(file_root)\n",
    "\n",
    "merged_cloud = o3d.geometry.PointCloud()\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(file_root, file)\n",
    "    cloud = o3d.io.read_point_cloud(file_path)\n",
    "    merged_cloud += cloud\n",
    "\n",
    "# mp = getattr(predictions_list[0], \"tolist\", lambda: predictions_list[0])()\n",
    "\n",
    "scaled_preds = [scale_preds(p.tolist(), cat) for p in modified_preds[0]]\n",
    "print(\"nm\", norm_factor_list[0], mean_list[0])\n",
    "pps = [\n",
    "    unnormalise_pipe_preds(scaled_preds[i], mean_list[i], norm_factor_list[i])\n",
    "    for i in range(len(scaled_preds))\n",
    "]\n",
    "pps = [[float(i) for i in pp] for pp in pps]\n",
    "print(\"pp\", len(pps))\n",
    "print(\"scaled\", scaled_preds[0])\n",
    "\n",
    "# filter out visualisations that cannot produce an ifc shape\n",
    "filtered_preds = check_visualisations(pps, cat, blueprint)\n",
    "vis = visualize_predictions(\n",
    "    [merged_cloud.points], cat, filtered_preds, blueprint, visualize=True\n",
    ")\n",
    "# vis = visualize_predictions([merged_cloud.points], cat, [pp], blueprint, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate ifc outputs for CLOI visualisation\n",
    "%autoreload 2\n",
    "\n",
    "# optimise using chamfer loss\n",
    "\n",
    "# non-robust\n",
    "# torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "limit = 64\n",
    "ifcs = chamfer_fine_tune(\n",
    "    100,\n",
    "    0.01,\n",
    "    predictions_list[:limit],\n",
    "    cloud_list[:limit],\n",
    "    cat,\n",
    "    blueprint,\n",
    "    alpha=8,\n",
    "    visualise=True,\n",
    "    elbow_fix=elbow_fix,\n",
    "    robust=None,\n",
    "    delta=0.0001,\n",
    "    return_ifc =True\n",
    ")\n",
    "\n",
    "print(len(ifcs), ifcs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"cloi/ifcs/\" + cat + \"/\"\n",
    "for i, file in enumerate(ifcs):\n",
    "    file.write(save_path + str(i) + \".ifc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANSAC benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = False\n",
    "noise_size = 256\n",
    "benchmark_file = \"ransac/benchmark.pkl\"\n",
    "data_path = \"ransac/build/output/\"\n",
    "save_path = \"ransac/build/input/\"\n",
    "no_lines = 3 if cat == \"tee\" else 2\n",
    "params_file = \"ransac/build/\" + cat + \"_params.txt\"\n",
    "cloud = o3d.geometry.PointCloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dataloader to generate a batch of examples with noise\n",
    "with torch.no_grad():\n",
    "    predictor = predictor.eval()\n",
    "    cloud_list = []\n",
    "    predictions_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for j, data in tqdm(enumerate(testDataLoader), total=1):\n",
    "        if j == 10:\n",
    "            break\n",
    "\n",
    "        #         # introduce noise\n",
    "        #         if noise:\n",
    "        #             noisy_clouds = []\n",
    "        #             dataset = data['pointcloud'].numpy()\n",
    "        #             for cl in dataset:\n",
    "        #                 noisy_clouds.append(add_noise(cl, noise_size, rng))\n",
    "        #             points = torch.Tensor(np.array(noisy_clouds)).to(device).float()\n",
    "        #         else:\n",
    "        points = data[\"pointcloud\"].to(device).float()\n",
    "\n",
    "        # get predictions\n",
    "        labels = data[\"properties\"].to(torch.device(\"cpu\"))\n",
    "        points = points.transpose(2, 1)\n",
    "        outputs, _ = predictor(points)\n",
    "        outputs = outputs.to(torch.device(\"cpu\"))\n",
    "        points = points.to(torch.device(\"cpu\"))\n",
    "\n",
    "        for i in range(outputs.size(0)):\n",
    "            predictions_list.append(outputs[i].numpy())\n",
    "            labels_list.append(labels[i].numpy())\n",
    "            cloud_list.append(points[i].numpy())\n",
    "\n",
    "predictions_list = np.array(predictions_list)\n",
    "cloud_list = np.array(cloud_list).transpose((0, 2, 1))\n",
    "label_list = np.array(labels_list)\n",
    "\n",
    "# save data\n",
    "for i, cl in enumerate(cloud_list):\n",
    "    pnts = o3d.utility.Vector3dVector(cl)\n",
    "    cloud.points = pnts\n",
    "    o3d.io.write_point_cloud(save_path + str(i) + \".pcd\", cloud)\n",
    "with open(benchmark_file, \"wb\") as f:\n",
    "    pickle.dump([predictions_list, labels_list, cloud_list], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(benchmark_file, \"rb\") as f:\n",
    "    predictions_list, labels_list, cloud_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictions_list), predictions_list[0].shape, labels_list[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check RANSAC results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load  parameters\n",
    "files = os.listdir(data_path)\n",
    "# print(files)\n",
    "\n",
    "ransac_f = open(params_file, \"r\")\n",
    "ransac_params = ransac_f.readlines()\n",
    "ransac_params = [x.strip() for x in ransac_params]\n",
    "# print(ransac_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find params of cylinder, given set of points and ransac outputs (point on axis, axis, r)\n",
    "def cylinder_parameters_from_ransac(points, params):\n",
    "    axis = np.array([params[3], params[4], params[5]])\n",
    "    axis_a = np.array([params[0], params[1], params[2]])\n",
    "    r = params[6]\n",
    "\n",
    "    # find projection of each point on cylinder axis\n",
    "    dists = []\n",
    "    for i, p in enumerate(points):\n",
    "        t = np.dot(axis, axis_a - p) / np.dot(axis, axis)\n",
    "        dists.append(t)\n",
    "\n",
    "    # find center, length of cylinder\n",
    "    min_dist, max_dist = min(dists), max(dists)\n",
    "    center = (2 * axis_a - (min_dist + max_dist) * axis) / 2\n",
    "    l = max_dist - min_dist\n",
    "\n",
    "    #     # debugging\n",
    "    #     min_p = np.array([min(points[:,0]), min(points[:,1]), min(points[:,2])])\n",
    "    #     max_p = np.array([max(points[:,0]), max(points[:,1]), max(points[:,2])])\n",
    "    #     center_p = (min_p + max_p)/2\n",
    "    #     print(\"c\", center, \"c2\", center_p, \"min\", min_p, \"max\", max_p, axis_a)\n",
    "\n",
    "    return (axis, r, center, l)\n",
    "\n",
    "\n",
    "# iterate through testset results, compute error\n",
    "sorted_labels_list, sorted_preds_list, sorted_ransac_list, sorted_points_list = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "for i in range(0, len(ransac_params), no_lines):\n",
    "    f_name = ransac_params[i]\n",
    "    idx = int(f_name.split(\".\")[0])\n",
    "    points = np.array(o3d.io.read_point_cloud(save_path + f_name).points)\n",
    "\n",
    "    if ransac_params[i + 1] != \"\":\n",
    "        # load inliers\n",
    "        primary_inliers = np.array(\n",
    "            o3d.io.read_point_cloud(data_path + \"primary_\" + f_name).points\n",
    "        )\n",
    "\n",
    "        # convert to model prediction format (axis, radius and position)\n",
    "        primary_params = [float(j) for j in ransac_params[i + 1].split(\",\")[:-1]]\n",
    "        p_axis, p_r, p_center, p_l = cylinder_parameters_from_ransac(\n",
    "            primary_inliers, primary_params\n",
    "        )\n",
    "        # print(len(primary_inliers))\n",
    "    else:\n",
    "        # get parameters from MOBB instead\n",
    "        p_axis, p_l, p_lengths, p_center = get_oriented_bbox_from_points(points)\n",
    "        p_l *= 2\n",
    "        p_lengths = np.sort(p_lengths)\n",
    "        p_r = (p_lengths[0] + p_lengths[1]) / 2\n",
    "        # p_r, p_l, p_center, p_axis = 0.5, 1, [0., 0.,0.], [1.0, 0.,0.]\n",
    "        # print(\"p\", p_axis)\n",
    "\n",
    "    param_array = [p_r, p_l, p_center[0], p_center[1], p_center[2]]\n",
    "    for i in range(3):\n",
    "        param_array.append(math.sin(p_axis[i]))\n",
    "        param_array.append(math.cos(p_axis[i]))\n",
    "    param_array = np.array(param_array)\n",
    "\n",
    "    if cat == \"tee\":\n",
    "        # load inliers\n",
    "        if ransac_params[i + 1] != \"\":\n",
    "            secondary_inliers = np.array(\n",
    "                o3d.io.read_point_cloud(data_path + \"secondary_\" + f_name).points\n",
    "            )\n",
    "            # print(len(secondary_inliers))\n",
    "        else:\n",
    "            # TODO: take axis perpendicular to primary axis as secondary axis\n",
    "            pass\n",
    "\n",
    "        # convert to model prediction format (axis, radius and position)\n",
    "        secondary_params = [float(j) for j in ransac_params[i + 2].split(\",\")[:-1]]\n",
    "        s_axis, s_r, s_center, s_l = cylinder_parameters_from_ransac(\n",
    "            secondary_inliers, secondary_params\n",
    "        )\n",
    "\n",
    "    sorted_ransac_list.append(param_array)\n",
    "    sorted_preds_list.append(predictions_list[idx])\n",
    "    sorted_labels_list.append(labels_list[idx])\n",
    "    sorted_points_list.append(points)\n",
    "    # print(\"id\", idx, labels_list[idx])\n",
    "    # print(param_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_errors(sorted_labels_list, sorted_preds_list, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_errors(sorted_labels_list, sorted_ransac_list, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize predictions\n",
    "vis = []\n",
    "error_count = 0\n",
    "for i, pr in enumerate(tqdm(sorted_ransac_list[:100])):\n",
    "    try:\n",
    "        pr = scale_preds(pr.tolist(), cat)\n",
    "        v, _ = visualize_predictions([sorted_points_list[i]], cat, [pr], blueprint)\n",
    "        vis.append(v)\n",
    "    except:\n",
    "        error_count += 1\n",
    "\n",
    "print(error_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_points_list = [points.transpose((1, 0)) for points in sorted_points_list]\n",
    "\n",
    "dists = chamfer_evaluate(sorted_preds_list, inverted_points_list, cat)\n",
    "plot_error_graph(dists, \"Binned chamfer loss\", max_val=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = chamfer_evaluate(sorted_ransac_list, inverted_points_list, cat)\n",
    "plot_error_graph(dists, \"Binned chamfer loss\", max_val=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visusalise correpsondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visaulise losses\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "# load clouds\n",
    "cld1_name = \"output/elbow/test/24102.pcd\"\n",
    "cld2_name = \"output/elbow/test/24106.pcd\"\n",
    "\n",
    "cld1 = np.array(o3d.io.read_point_cloud(cld1_name).points)\n",
    "cld2 = np.array(o3d.io.read_point_cloud(cld2_name).points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure pairing loss\n",
    "src = torch.Tensor([cld1, cld1])\n",
    "tgt = torch.Tensor([cld2, cld2])\n",
    "\n",
    "chamferDist = ChamferDistance()\n",
    "nn = chamferDist(src, tgt, bidirectional=True, return_nn=True, k=1)\n",
    "dist = torch.sum(nn[0].dists) + torch.sum(nn[1].dists)\n",
    "\n",
    "# compute the cyclical index (closest point of closest point). this should ideally be 0->n_points in order\n",
    "perfect_idx = torch.range(0, nn[0].idx.shape[1] - 1, dtype=int)  # 0-> N_points\n",
    "perfect_idx = perfect_idx[:, None]  # add extra dimension\n",
    "perfect_idx = perfect_idx.repeat(nn[0].idx.shape[0], 1, 1)  # batch_size\n",
    "true_idx_fwd = torch.gather(nn[0].idx, 1, nn[1].idx)  # tgt[[src[match]]]\n",
    "true_idx_bwd = torch.gather(nn[1].idx, 1, nn[0].idx)  # tgt[[src[match]]]\n",
    "pair_loss = torch.sum(perfect_idx == true_idx_fwd)\n",
    "\n",
    "# print(nn[0].knn.shape, true_idx.shape, torch.flatten(true_idx[0]).shape)\n",
    "# pairs = torch.gather(nn[1].knn, 1, true_idx)\n",
    "paired_points_fwd = torch.stack(\n",
    "    [nn[0].knn[i][torch.flatten(nn[1].idx[i])] for i in range(nn[1].idx.shape[0])]\n",
    ")\n",
    "# paired_points_fwd = torch.stack([nn[1].knn[i][torch.flatten(true_idx_fwd[i])] for i in range(true_idx_fwd.shape[0])])\n",
    "# paired_points_fwd = torch.stack([tgt[i][torch.flatten(true_idx_fwd[i])] for i in range(true_idx_fwd.shape[0])])\n",
    "paired_points_fwd = paired_points_fwd.reshape(\n",
    "    (paired_points_fwd.shape[0], paired_points_fwd.shape[1], paired_points_fwd.shape[3])\n",
    ")\n",
    "pair_dist_fwd = torch.sum(torch.square(paired_points_fwd - tgt))\n",
    "\n",
    "\n",
    "print(\"DS\", true_idx_bwd.shape, nn[0].knn.shape, src.shape, nn[0].idx.shape)\n",
    "paired_points_bwd = torch.stack(\n",
    "    [nn[1].knn[i][torch.flatten(nn[0].idx[i])] for i in range(nn[0].idx.shape[0])]\n",
    ")\n",
    "# paired_points_bwd = torch.stack([src[i][torch.flatten(true_idx_bwd[i])] for i in range(true_idx_bwd.shape[0])])\n",
    "print(paired_points_bwd.shape)\n",
    "# paired_points_bwd = torch.stack([nn[0].knn[i][torch.flatten(true_idx_bwd[i])] for i in range(true_idx_bwd.shape[0])])\n",
    "paired_points_bwd = paired_points_bwd.reshape(\n",
    "    (paired_points_bwd.shape[0], paired_points_bwd.shape[1], paired_points_bwd.shape[3])\n",
    ")\n",
    "pair_dist_bwd = torch.sum(torch.square(paired_points_bwd - src))\n",
    "\n",
    "print(\"pair\", true_idx_bwd[0].flatten().shape)\n",
    "print(dist, pair_loss, pair_dist_fwd, pair_dist_bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise cyclical pairs\n",
    "v = visualise_loss(\n",
    "    cld1, cld2, blueprint, pairs=true_idx_bwd[0].flatten(), loss=\"pair\", same_cloud=True\n",
    ")\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_idx_fwd = torch.gather(nn[0].idx, 1, nn[1].idx)  # tgt[[src[match]]]\n",
    "true_idx_bwd = torch.gather(nn[1].idx, 1, nn[0].idx)  # tgt[[src[match]]]\n",
    "\n",
    "# manual chamfer loss\n",
    "paired_points_bwd = torch.stack(\n",
    "    [tgt[i][torch.flatten(nn[0].idx[i])] for i in range(nn[0].idx.shape[0])]\n",
    ")\n",
    "pair_dist_bwd = paired_points_bwd - src\n",
    "# pair_dist_bwd = torch.sum(torch.square(paired_points_bwd - x))\n",
    "# paired_points_fwd = torch.stack([x[i][torch.flatten(nn[1].idx[i])] for i in range(nn[1].idx.shape[0])])\n",
    "paired_points_fwd = torch.stack(\n",
    "    [src[i][torch.flatten(true_idx_bwd[i])] for i in range(true_idx_bwd.shape[0])]\n",
    ")\n",
    "pair_dist_fwd = paired_points_fwd - paired_points_bwd\n",
    "\n",
    "pair_dist = pair_dist_bwd + pair_dist_fwd\n",
    "pair_dist = torch.mul(torch.square(pair_dist), torch.square(pair_dist_bwd))\n",
    "pair_dist = torch.sum(pair_dist)\n",
    "print(pair_dist, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise EMD\n",
    "src = torch.Tensor([cld1]).cuda()\n",
    "tgt = torch.Tensor([cld2]).cuda()\n",
    "loss, ass = calc_emd(src, tgt, 0.05, 1000)\n",
    "\n",
    "ass = ass.detach().cpu().numpy()\n",
    "unique = np.unique(ass)\n",
    "print(\"unique\", len(unique))\n",
    "\n",
    "v = visualise_loss(\n",
    "    cld1, cld2, blueprint, pairs=ass.flatten(), loss=\"pair\", same_cloud=False\n",
    ")\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise regular chamfer loss\n",
    "v = visualise_loss(cld1, cld2, blueprint)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise reverse weighted chamfer loss\n",
    "src = torch.Tensor([cld1]).cuda()\n",
    "tgt = torch.Tensor([cld2]).cuda()\n",
    "loss, ass = calc_reverse_weighted_cd_tensor(src, tgt, k=32, return_assignment=True)\n",
    "\n",
    "ass = ass[0].detach().cpu().numpy()\n",
    "\n",
    "unique = np.unique(ass)\n",
    "print(\"unique\", len(unique))\n",
    "\n",
    "v = visualise_loss(cld1, cld2, blueprint, pairs=ass, loss=\"pair\", same_cloud=False)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clouds\n",
    "cld1_name = \"data/24102s.pcd\"\n",
    "cld2_name = \"data/24103s.pcd\"\n",
    "\n",
    "cld1 = np.array(o3d.io.read_point_cloud(cld1_name).points)\n",
    "cld2 = np.array(o3d.io.read_point_cloud(cld2_name).points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualise chamfer loss with coplanarity\n",
    "target_pcd_tensor = torch.tensor([cld2], device=cuda)\n",
    "src_pcd_tensor = torch.tensor([cld1], device=cuda)\n",
    "\n",
    "vect, dists = knn_vectors(src_pcd_tensor, target_pcd_tensor, 3)\n",
    "coplanarity = check_coplanarity(vect)\n",
    "coplanarity = coplanarity[0].detach().cpu().numpy()\n",
    "print(\"shapes\", coplanarity.shape, dists.shape)\n",
    "\n",
    "v = visualise_loss(cld1, cld2, blueprint, strength=coplanarity, k=3)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visaulise direct correspondence loss\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "v = visualise_parameter_pair(predictions_list, label_list, cat, blueprint, 4)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v_s = [\n",
    "    visualise_parameter_pair(predictions_list, label_list, cat, blueprint, i)\n",
    "    for i in range(20)\n",
    "]\n",
    "v_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualise cross section correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_o3d_correspondences(a, b, ass):\n",
    "    points = np.vstack([a, b])\n",
    "    start_idx = np.arange(a.shape[0])\n",
    "    end_idx = ass + a.shape[0]\n",
    "    lines = np.stack([start_idx, end_idx], axis=1)\n",
    "    print(points.shape, lines.shape, lines[:3])\n",
    "\n",
    "    colors = [[1, 0, 0] for i in range(len(lines))]\n",
    "    line_set = o3d.geometry.LineSet(\n",
    "        points=o3d.utility.Vector3dVector(points),\n",
    "        lines=o3d.utility.Vector2iVector(lines),\n",
    "    )\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    return line_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_path = \"sphere/plane_slice2.pcd\"\n",
    "points = np.array(o3d.io.read_point_cloud(cloud_path).points)\n",
    "# flatten\n",
    "points[:, 1] = 0\n",
    "gt = o3d.geometry.PointCloud()\n",
    "gt.points = o3d.utility.Vector3dVector(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points with some random variations\n",
    "source = o3d.geometry.PointCloud()\n",
    "\n",
    "points2 = np.copy(points)\n",
    "variation = (np.random.rand(points2.shape[0], points2.shape[1]) - 0.5) / 10\n",
    "variation[:, 1] = 0\n",
    "points2 += variation\n",
    "source.points = o3d.utility.Vector3dVector(points2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get chamfer correspondences\n",
    "# chamferDist = ChamferDistance()\n",
    "\n",
    "# nn = chamferDist(\n",
    "#     x, y, bidirectional=True, return_nn=True)\n",
    "# assignment = nn[0].idx[:,:,0][0].cpu().numpy()\n",
    "# print(assignment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get balanced correspondences\n",
    "# chamferDist = ChamferDistance()\n",
    "# eps = 0.00001\n",
    "\n",
    "# k = 16\n",
    "# k2 = 8 # reduce k to check density in smaller patches\n",
    "# power = 1\n",
    "\n",
    "# # add a loss term for mismatched pairs\n",
    "# nn = chamferDist(\n",
    "#     x, y, bidirectional=True, return_nn=True, k=k\n",
    "# )\n",
    "\n",
    "# # measure density with itself\n",
    "# nn_x = chamferDist(x, x, bidirectional=False, return_nn=True, k=k2)\n",
    "# density_x = torch.mean(nn_x[0].dists[:,:,1:], dim=2)\n",
    "# density_x = 1 / (density_x + eps)\n",
    "# high, low = torch.max(density_x), torch.min(density_x)\n",
    "# diff = high - low\n",
    "# density_x = (density_x - low) / diff\n",
    "\n",
    "# # measure density with other cloud\n",
    "# density_xy = torch.mean(nn[0].dists[:,:,:k2-1], dim=2)\n",
    "# density_xy = 1 / (density_xy + eps)\n",
    "# high, low = torch.max(density_xy), torch.min(density_xy)\n",
    "# diff = high - low\n",
    "# density_xy = (density_xy - low) / diff\n",
    "# w_x = torch.div(density_xy, density_x)\n",
    "# #print(\"w\", w_x.shape, w_x[0])\n",
    "# w_x = torch.pow(w_x, power)\n",
    "# scaling_factors_1 = w_x.unsqueeze(2).repeat(1, 1, k)\n",
    "# multiplier = torch.gather(scaling_factors_1, 1, nn[1].idx)\n",
    "\n",
    "# scaled_dist_1 = torch.mul(nn[1].dists, multiplier)\n",
    "# scaled_dist_1x, i1 = torch.min(scaled_dist_1, 2)\n",
    "\n",
    "# # measure density with itself\n",
    "# nn_y = chamferDist(y, y, bidirectional=False, return_nn=True, k=k2)\n",
    "# density_y = torch.mean(nn_y[0].dists[:,:,1:], dim=2)\n",
    "# density_y = 1 / (density_y + eps)\n",
    "# high, low = torch.max(density_y), torch.min(density_y)\n",
    "# diff = high - low\n",
    "# density_y = (density_y - low) / diff\n",
    "\n",
    "# # measure density with other cloud\n",
    "# density_yx = torch.mean(nn[1].dists[:,:,:k2-1], dim=2)\n",
    "# density_yx = 1 / (density_yx + eps)\n",
    "# high, low = torch.max(density_yx), torch.min(density_yx)\n",
    "# diff = high - low\n",
    "# density_yx = (density_yx - low) / diff\n",
    "# w_y = torch.div(density_yx, density_y)\n",
    "# #print(\"w\", w_x.shape, w_x[0])\n",
    "# w_y = torch.pow(w_y, power)\n",
    "# scaling_factors_0 = w_y.unsqueeze(2).repeat(1, 1, k)\n",
    "# multiplier = torch.gather(scaling_factors_0, 1, nn[0].idx)\n",
    "\n",
    "# scaled_dist_0 = torch.mul(nn[0].dists, multiplier)\n",
    "# scaled_dist_0x, i0 = torch.min(scaled_dist_0, 2)\n",
    "\n",
    "# min_ind_0 = torch.gather(nn[0].idx, 2, i0.unsqueeze(2).repeat(1,1,k))[:, :, 0]\n",
    "# min_ind_1 = torch.gather(nn[1].idx, 2, i1.unsqueeze(2).repeat(1,1,k))[:, :, 0]\n",
    "\n",
    "# assignment = min_ind_0[0].cpu().numpy()\n",
    "# print(assignment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_direct(x, y):\n",
    "    return torch.sum(torch.square(x - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "# morph a sphere into the shape of an input point cloud\n",
    "# by optimising chamfer loss iteratively\n",
    "# total points = num_points**2\n",
    "def optimise_shape(src_pcd_tensor, tgt_pcd_tensor, iterations=5, learning_rate=0.01, loss_func= \"chamfer\"):\n",
    "    \n",
    "    cuda = torch.device(\"cuda\")\n",
    "    \n",
    "    # optimise\n",
    "    optimizer = torch.optim.Adam([tgt_pcd_tensor], lr=learning_rate)\n",
    "    intermediate, losses, assingments = [], [], []\n",
    "    chamferDist = ChamferDistance()\n",
    "    assignments = []\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if loss_func == \"chamfer\":\n",
    "            nn = chamferDist(\n",
    "                src_pcd_tensor, tgt_pcd_tensor, bidirectional=True, return_nn=True)\n",
    "            loss = torch.sum(nn[1].dists) + torch.sum(nn[0].dists)\n",
    "            assignment = [nn[0].idx[:,:,0].detach().cpu().numpy(), nn[1].idx[:,:,0].detach().cpu().numpy()]\n",
    "        elif loss_func == \"emd\":\n",
    "            loss, assignment = calc_emd(tgt_pcd_tensor, src_pcd_tensor, 0.05, 50)\n",
    "            assignment = assignment.detach().cpu().numpy()\n",
    "        elif loss_func == \"balanced\":\n",
    "            loss, assignment = calc_balanced_chamfer_loss_tensor(src_pcd_tensor, tgt_pcd_tensor, return_assignment=True, k=4)\n",
    "        elif loss_func == \"infocd\":\n",
    "            loss, assignment = calc_cd_like_InfoV2(src_pcd_tensor, tgt_pcd_tensor, return_assignment=True)\n",
    "        elif loss_func == \"direct\":\n",
    "            loss = calc_direct(src_pcd_tensor, tgt_pcd_tensor)\n",
    "        else:\n",
    "            print(\"unspecified loss\")\n",
    "            \n",
    "        #print(\"a\", assignment[0].shape)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"iteration\", i, \"loss\", loss.item())\n",
    "        \n",
    "        intermediate.append(tgt_pcd_tensor.clone())\n",
    "\n",
    "    # calculate final chamfer loss\n",
    "    dist = chamferDist(src_pcd_tensor, tgt_pcd_tensor, bidirectional=True)\n",
    "    emd_loss, _ = calc_emd(tgt_pcd_tensor, src_pcd_tensor, 0.05, 50)\n",
    "    print(\"final chamfer dist\", dist.item(), \"emd\", emd_loss.item())\n",
    "    \n",
    "    # save assignments for analysis\n",
    "#     if measure_consistency:\n",
    "#         with open(\"sphere/assignments_\" + loss_func + \".pkl\", \"wb\") as f:\n",
    "#             pickle.dump(assingments, f)\n",
    "            \n",
    "    intermediate = torch.stack(intermediate)\n",
    "    return intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create tensors\n",
    "cuda = torch.device(\"cuda\")\n",
    "x = torch.tensor([points], device=cuda)\n",
    "y = torch.tensor([points2], device=cuda, requires_grad=True)\n",
    "\n",
    "iterations = 50\n",
    "intermediate = optimise_shape(\n",
    "    x, y, iterations=iterations, learning_rate=0.01, loss_func=\"chamfer\"\n",
    ")\n",
    "print(intermediate.shape)\n",
    "intermediate = (\n",
    "    intermediate.reshape((iterations, x.shape[1], x.shape[2])).detach().cpu().numpy()\n",
    ")\n",
    "print(intermediate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_o3d_paths(a, stops):\n",
    "    steps = stops.shape[0]\n",
    "    print(stops.shape)\n",
    "    points = np.vstack([a] + [stops[i] for i in range(steps)])\n",
    "    line_sets = []\n",
    "    # print(points.shape)\n",
    "    for j in range(steps):\n",
    "        if j == 0:\n",
    "            start_idx = np.arange(a.shape[0])\n",
    "        else:\n",
    "            start_idx = start_idx + a.shape[0]\n",
    "        end_idx = start_idx + a.shape[0]\n",
    "        lines = np.stack([start_idx, end_idx], axis=1)\n",
    "        # print(points.shape, lines.shape, lines[:3])\n",
    "\n",
    "        colors = [[1, 1 - 0.1 * j, 0] for i in range(len(lines))]\n",
    "        line_set = o3d.geometry.LineSet(\n",
    "            points=o3d.utility.Vector3dVector(points),\n",
    "            lines=o3d.utility.Vector2iVector(lines),\n",
    "        )\n",
    "        line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "        line_sets.append(line_set)\n",
    "\n",
    "    return line_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interm = o3d.geometry.PointCloud()\n",
    "interm.points = o3d.utility.Vector3dVector(intermediate[0])\n",
    "\n",
    "end = o3d.geometry.PointCloud()\n",
    "end.points = o3d.utility.Vector3dVector(intermediate[-1])\n",
    "\n",
    "\n",
    "source.paint_uniform_color([0.0, 0.706, 1])\n",
    "end.paint_uniform_color([0.7, 0.70, 0])\n",
    "interm.paint_uniform_color([0.5, 0.5, 0])\n",
    "gt.paint_uniform_color([1.0, 0.706, 1])\n",
    "\n",
    "# line_sets = draw_o3d_paths(points2, intermediate[1:])\n",
    "# o3d.visualization.draw_geometries([gt, source, interm] + line_sets)\n",
    "\n",
    "# start from the 2nd step onwards\n",
    "line_sets = draw_o3d_paths(points2, intermediate)\n",
    "# o3d.visualization.draw_geometries([source, gt, end] + line_sets)\n",
    "o3d.visualization.draw_geometries([gt, end])\n",
    "# o3d.visualization.draw_geometries([interm, gt, end] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.paint_uniform_color([0.0, 0.706, 1])\n",
    "gt.paint_uniform_color([1.0, 0.706, 1])\n",
    "gt.paint_uniform_color([1.0, 0.706, 1])\n",
    "\n",
    "line_set = draw_o3d_correspondences(points2, points, assignment)\n",
    "o3d.visualization.draw_geometries([gt, source, line_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate autoencoder results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise npy point cloud set\n",
    "\n",
    "# chamfer\n",
    "ch_savepath = \"/home/haritha/documents/experiments/PointSWD/logs2/reconstruction/model/modelnet40/\"\n",
    "ch_inp_list = np.load(os.path.join(ch_savepath, \"input.npy\"))\n",
    "ch_rec_list = np.load(os.path.join(ch_savepath, \"reconstruction.npy\"))\n",
    "\n",
    "# new\n",
    "savepath = \"/home/haritha/documents/experiments/PointSWD/logs24/reconstruction/model/modelnet40/\"\n",
    "inp_list = np.load(os.path.join(savepath, \"input.npy\"))\n",
    "rec_list = np.load(os.path.join(savepath, \"reconstruction.npy\"))\n",
    "\n",
    "print(rec_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blueprint = \"data/sample.ifc\"\n",
    "ifc = setup_ifc_file(blueprint)\n",
    "limit = 10\n",
    "vis = []\n",
    "\n",
    "for i in range(limit):\n",
    "    vis.append(\n",
    "        vis_ifc_and_cloud(\n",
    "            ifc, [ch_inp_list[i].astype(\"float64\"), ch_rec_list[i].astype(\"float64\")]\n",
    "        )\n",
    "    )\n",
    "\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blueprint = \"data/sample.ifc\"\n",
    "ifc = setup_ifc_file(blueprint)\n",
    "limit = 10\n",
    "vis = []\n",
    "\n",
    "for i in range(limit):\n",
    "    vis.append(\n",
    "        vis_ifc_and_cloud(\n",
    "            ifc, [inp_list[i].astype(\"float64\"), rec_list[i].astype(\"float64\")]\n",
    "        )\n",
    "    )\n",
    "\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare losses\n",
    "def evaluate_autoencoder_results(inp_list, rec_list, loss_funcs, batch_size=None):\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    if batch_size == None:\n",
    "        batch_size = len(inp_list)\n",
    "\n",
    "    # create empty dict\n",
    "    all_losses = {func: [] for func in loss_funcs}\n",
    "\n",
    "    # split into batches\n",
    "    for i in tqdm(range(math.ceil(len(inp_list) / batch_size))):\n",
    "        a = i * batch_size\n",
    "        b = min(a + batch_size, len(inp_list))\n",
    "        # print(a,b)\n",
    "\n",
    "        # compute losses\n",
    "        inp_tensor = torch.tensor(inp_list[a:b], device=cuda)\n",
    "        rec_tensor = torch.tensor(rec_list[a:b], device=cuda)\n",
    "        losses = calculate_3d_loss(inp_tensor, rec_tensor, loss_funcs)\n",
    "\n",
    "        # sum losses\n",
    "        for k, v in losses.items():\n",
    "            all_losses[k].append(v)\n",
    "\n",
    "    # average losses\n",
    "    for k in all_losses:\n",
    "        all_losses[k] = np.average(np.array(all_losses[k]))\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "loss_funcs = [\"chamfer\", \"reverse\", \"emd\"]\n",
    "\n",
    "# chamfer\n",
    "ch_loss = evaluate_autoencoder_results(ch_inp_list, ch_rec_list, loss_funcs, 512)\n",
    "\n",
    "# new\n",
    "loss = evaluate_autoencoder_results(inp_list, rec_list, loss_funcs, 512)\n",
    "\n",
    "print(\"chamfer tuned\", ch_loss)\n",
    "print(\"new\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### completion evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DCD (VCN plus, MVP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce density colourmaps that are normalised with their pairs\n",
    "\n",
    "\n",
    "# get densities\n",
    "def get_density(clouds):\n",
    "    # compute nearest neighbours to calculated density\n",
    "    clouds = torch.tensor(clouds, device=\"cuda\")\n",
    "    chamferDist = ChamferDistance()\n",
    "    nn = chamferDist(clouds, clouds, bidirectional=False, return_nn=True, k=32)\n",
    "\n",
    "    density = torch.mean(nn[0].dists[:, :, 1:], dim=2)\n",
    "    eps = 0.00001\n",
    "    density = 1 / (density + eps)\n",
    "    return density\n",
    "\n",
    "\n",
    "# normalise for each example across prediction sets\n",
    "def normalise_densities(density_sets):\n",
    "    densities = torch.stack(density_sets)\n",
    "    highs, lows = torch.max(densities, 2).values, torch.min(densities, 2).values\n",
    "    highs, lows = torch.max(highs, 0).values, torch.min(lows, 0).values\n",
    "    # print(densities.shape, highs.shape)\n",
    "\n",
    "    # highs = torch.reshape(highs, densities.shape)\n",
    "    highs = highs.unsqueeze(0).unsqueeze(-1)\n",
    "    highs = highs.expand(densities.shape[0], densities.shape[1], densities.shape[2])\n",
    "    lows = lows.unsqueeze(0).unsqueeze(-1)\n",
    "    lows = lows.expand(densities.shape[0], densities.shape[1], densities.shape[2])\n",
    "    diff = highs - lows\n",
    "    densities = (densities - lows) / diff\n",
    "\n",
    "    return densities[0], densities[1], densities[2], densities[3]\n",
    "\n",
    "\n",
    "# represent density with colour and combine with point cloud\n",
    "def get_coloured_clouds(clouds, density, colormap_name=\"plasma_r\"):\n",
    "    density = density.detach().cpu().numpy()\n",
    "    colours = np.zeros((density.shape[0], density.shape[1], 4))\n",
    "    colormap = plt.get_cmap(colormap_name)\n",
    "\n",
    "    for i, cloud in enumerate(density):\n",
    "        for j, pt in enumerate(cloud):\n",
    "            colours[i, j] = colormap(pt)\n",
    "\n",
    "    #     clouds = clouds.detach().cpu().numpy()\n",
    "    colours = colours[:, :, :3]\n",
    "    pcds = []\n",
    "\n",
    "    for i, cl in enumerate(clouds):\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(cl)\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colours[i])\n",
    "        pcds.append(pcd)\n",
    "\n",
    "    return pcds, colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cloud_list_vcn(path, prefix):\n",
    "    limit = 20\n",
    "    cloud_sets = []\n",
    "\n",
    "    for i in range(limit):\n",
    "        f_name = \"fine\" + str(i) + \".pkl\"\n",
    "        with open(path + f_name, \"rb\") as f:\n",
    "            pred, partial, gt = pickle.load(f)\n",
    "            if prefix == \"pred\":\n",
    "                clouds = pred\n",
    "            elif prefix == \"gt\":\n",
    "                clouds = gt\n",
    "            else:\n",
    "                clouds = partial\n",
    "        cloud_sets.append(clouds.detach().cpu().numpy())\n",
    "    cloud_sets = np.vstack(cloud_sets)\n",
    "    print(cloud_sets.shape)\n",
    "\n",
    "    return cloud_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_path = \"/home/haritha/documents/experiments/Density_aware_Chamfer_Distance/outputs_balanced/\"\n",
    "dcd_path = (\n",
    "    \"/home/haritha/documents/experiments/Density_aware_Chamfer_Distance/outputs_dcd/\"\n",
    ")\n",
    "\n",
    "cld_balanced = get_cloud_list_vcn(balanced_path, \"pred\")\n",
    "cld_dcd = get_cloud_list_vcn(dcd_path, \"pred\")\n",
    "cld_gt = get_cloud_list_vcn(dcd_path, \"gt\")\n",
    "cld_partial = get_cloud_list_vcn(dcd_path, \"partial\")\n",
    "\n",
    "d_balanced = get_density(cld_balanced)\n",
    "d_dcd = get_density(cld_dcd)\n",
    "d_gt = get_density(cld_gt)\n",
    "d_partial = get_density(cld_partial)\n",
    "\n",
    "print(d_partial.shape)\n",
    "\n",
    "# produce density colourmaps that are normalised with their pairs\n",
    "d_balanced, d_dcd, d_gt, d_partial = normalise_densities(\n",
    "    [d_balanced, d_dcd, d_gt, d_partial]\n",
    ")\n",
    "\n",
    "v_balanced, _ = get_coloured_clouds(cld_balanced, d_balanced)\n",
    "v_dcd, _ = get_coloured_clouds(cld_dcd, d_dcd)\n",
    "v_gt, col = get_coloured_clouds(cld_gt, d_gt)\n",
    "v_partial, _ = get_coloured_clouds(cld_partial, d_partial)\n",
    "# c_dcd = get_colours(d_dcd)\n",
    "# c_gt = get_colours(d_gt)\n",
    "# c_partial = get_colours(d_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pointAttN\n",
    "# balanced_path = \"/home/haritha/documents/experiments/PointAttN/outputs/\"\n",
    "# dcd_path = \"/home/haritha/documents/experiments/PointAttN/outputs_cd/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_side_by_side(v1, v2, v3, v4, i):\n",
    "    # shift points\n",
    "    cl1, cl2, cl3, cl4 = v1[i], v2[i], v3[i], v4[i]\n",
    "    cl1.points = o3d.utility.Vector3dVector(np.array(cl1.points) - np.array([1, 0, 0]))\n",
    "    cl3.points = o3d.utility.Vector3dVector(np.array(cl3.points) + np.array([1, 0, 0]))\n",
    "    cl4.points = o3d.utility.Vector3dVector(np.array(cl4.points) + np.array([2, 0, 0]))\n",
    "\n",
    "    o3d.visualization.draw_geometries([cl1, cl2, cl3, cl4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(v_balanced))\n",
    "# shortlist = [226,227,228,102,103,104,0,1,2] = [39,140,219,106,1,7,148,156,11,43,53,194,4,17,91,44,66,3,27,274,87,55,158,35,103,129,112,170,195]\n",
    "shortlist = [39, 140, 7, 148, 53, 194, 91, 274, 55, 158, 129, 112, 195]\n",
    "# for i in range(250,300):\n",
    "for i in shortlist:\n",
    "    print(i)\n",
    "    view_side_by_side(v_gt, v_dcd, v_balanced, v_partial, i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a desired set of clouds\n",
    "def save_cloud(clouds, name, i, colours=None):\n",
    "    directory = \"mitsuba/\"\n",
    "    #     pcd = o3d.geometry.PointCloud()\n",
    "    #     pcd.points = o3d.utility.Vector3dVector(clouds[i])\n",
    "    #     o3d.io.write_point_cloud(directory + name + str(i) + \".ply\", pcd)\n",
    "    with open(directory + name + str(i) + \".npy\", \"wb\") as f:\n",
    "        np.save(f, clouds[i])\n",
    "    if colours is not None:\n",
    "        with open(directory + name + str(i) + \"c_.npy\", \"wb\") as f:\n",
    "            np.save(f, colours[i])\n",
    "\n",
    "\n",
    "i = 4\n",
    "# save_cloud(cld_balanced, \"balanced\", i)\n",
    "# save_cloud(cld_dcd, \"dcd\", i)\n",
    "# save_cloud(cld_partial, \"partial\", i)\n",
    "# save_cloud(cld_gt, \"gt\", i)\n",
    "\n",
    "for i in shortlist:\n",
    "    save_cloud(cld_gt, \"gt\", i + 1)\n",
    "    save_cloud(cld_balanced, \"bal\", i + 1)\n",
    "    save_cloud(cld_dcd, \"dcd\", i + 1)\n",
    "    save_cloud(cld_partial, \"part\", i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_cloud_list_vcn(path, prefix, ifc, col=0):\n",
    "    limit = 2\n",
    "    vis = []\n",
    "\n",
    "    for i in range(limit):\n",
    "        f_name = \"fine\" + str(i) + \".pkl\"\n",
    "        with open(path + f_name, \"rb\") as f:\n",
    "            pred, partial, gt = pickle.load(f)\n",
    "            if prefix == \"pred\":\n",
    "                clouds = pred\n",
    "            elif prefix == \"gt\":\n",
    "                clouds = gt\n",
    "            else:\n",
    "                clouds = partial\n",
    "        for cl in clouds:\n",
    "            cloud_list = [None, None, None]\n",
    "            cloud_list[col] = cl.detach().cpu().numpy().astype(np.double)\n",
    "            vis.append(vis_ifc_and_cloud(ifc, cloud_list))\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_path = \"/home/haritha/documents/experiments/Density_aware_Chamfer_Distance/outputs_balanced/\"\n",
    "dcd_path = (\n",
    "    \"/home/haritha/documents/experiments/Density_aware_Chamfer_Distance/outputs_dcd/\"\n",
    ")\n",
    "blueprint = \"data/sample.ifc\"\n",
    "ifc = setup_ifc_file(blueprint)\n",
    "\n",
    "v_balanced = view_cloud_list_vcn(balanced_path, \"pred\", ifc, 0)\n",
    "v_dcd = view_cloud_list_vcn(dcd_path, \"pred\", ifc, 1)\n",
    "v_gt = view_cloud_list_vcn(dcd_path, \"gt\", ifc, 2)\n",
    "v_partial = view_cloud_list_vcn(dcd_path, \"partial\", ifc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(v_balanced))\n",
    "for i in range(len(v_balanced)):\n",
    "    print(v_balanced[i], v_dcd[i], v_gt[i], v_partial[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([point_cloud, gt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"devices\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InfoCD (Seedformer, PCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "\n",
    "# load NPYs from each. (GT, Complete, CD, InfoCD, UniformCD)\n",
    "category_id = \"02933112/\"\n",
    "cd_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/pcn/train_pcn_Log_2024_02_22_14_49_07/outputs_cd/\"\n",
    "info_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/pcn/train_pcn_Log_2024_02_22_14_49_07/outputs_info/\"\n",
    "uniform_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/pcn/train_pcn_Log_2024_02_29_23_43_45/outputs_uniform/\"\n",
    "\n",
    "blueprint = \"data/sample.ifc\"\n",
    "ifc = setup_ifc_file(blueprint)\n",
    "\n",
    "def view_cloud_list_seedformer(category_id, path, prefix, ifc, col=0):\n",
    "    files = os.listdir(os.path.join(path,category_id))\n",
    "    files_filtered = [f for f in files if prefix in f]\n",
    "    files_filtered.sort()\n",
    "    #print(len(files), len(files_filtered))\n",
    "    \n",
    "    limit = 10\n",
    "    vis = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files_filtered:\n",
    "        count +=1\n",
    "        if count == limit:\n",
    "            break\n",
    "        cloud = np.load(os.path.join(path, category_id, f))\n",
    "        cloud_list = [None, None, None]\n",
    "        cloud_list[col] = cloud.astype(\"float64\")\n",
    "        vis.append(vis_ifc_and_cloud(ifc, cloud_list))\n",
    "    return vis\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v_cd = view_cloud_list_seedformer(category_id, cd_path, \"pred\", ifc, 0)\n",
    "v_info = view_cloud_list_seedformer(category_id, info_path, \"pred\", ifc, 1)\n",
    "v_uniform = view_cloud_list_seedformer(category_id, uniform_path, \"pred\", ifc, 2)\n",
    "v_complete = view_cloud_list_seedformer(category_id, cd_path, \"gt\", ifc, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(v_cd)):\n",
    "    print(v_cd[i], v_info[i], v_uniform[i], v_complete[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industrial facility completion visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compatible with VCN, seedformer results\n",
    "cloi = True\n",
    "if not cloi:\n",
    "    data_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/outputs/\"\n",
    "else:\n",
    "    data_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/outputs_cloi_corrected_elbow/\"\n",
    "data = os.listdir(data_path)\n",
    "preds, gts, partials = [], [], []\n",
    "for dt in data:\n",
    "    with open(data_path + dt, \"rb\") as f:\n",
    "        pred, partial, gt = pickle.load(f)\n",
    "        preds.append(pred[0].detach().cpu().numpy().astype(np.double))\n",
    "        gts.append(gt[0].detach().cpu().numpy().astype(np.double))\n",
    "        partials.append(partial[0].cpu().numpy().astype(np.double))\n",
    "print(len(preds), partials[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a second set of results and visualise them together\n",
    "data_path2 = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/outputs_cloi_elbow/\"\n",
    "data2 = os.listdir(data_path2)\n",
    "preds2 = []\n",
    "for dt in data2:\n",
    "    with open(data_path2 + dt, \"rb\") as f:\n",
    "        pred, partial, gt = pickle.load(f)\n",
    "        preds2.append(pred[0].detach().cpu().numpy().astype(np.double))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pcd = o3d.geometry.PointCloud()\n",
    "pred_pcd2 = o3d.geometry.PointCloud()\n",
    "partial_pcd = o3d.geometry.PointCloud()\n",
    "compare_preds = True\n",
    "\n",
    "for ind in range(69,70):\n",
    "    pred_pcd.points = o3d.utility.Vector3dVector(preds[ind])\n",
    "    partial_pcd.points = o3d.utility.Vector3dVector(partials[ind])\n",
    "    if compare_preds:\n",
    "        pred_pcd2.points = o3d.utility.Vector3dVector(preds2[ind])\n",
    "        pred_pcd2.paint_uniform_color([1., 0.5, 0.0])\n",
    "\n",
    "    pred_pcd.paint_uniform_color([0.0, 0.65, 0.65])\n",
    "    partial_pcd.paint_uniform_color([1., 0.25, 0.25])\n",
    "    # gt.paint_uniform_color([1., 0.706, 1])\n",
    "\n",
    "    o3d.visualization.draw_geometries([partial_pcd])\n",
    "    if compare_preds:\n",
    "        o3d.visualization.draw_geometries([pred_pcd])\n",
    "        o3d.visualization.draw_geometries([pred_pcd2])\n",
    "        #o3d.visualization.draw_geometries([pred_pcd, pred_pcd2, partial_pcd])\n",
    "    else:\n",
    "        o3d.visualization.draw_geometries([pred_pcd, partial_pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save normalsied cloi clouds for reference\n",
    "\n",
    "def normalize_point_cloud(points):\n",
    "    # points: Tensor of shape (B, N, 3)\n",
    "\n",
    "    # Calculate the centroids for each point cloud in the batch\n",
    "    centroids = points.mean(dim=1, keepdim=True)\n",
    "    points_centered = points - centroids\n",
    "\n",
    "    # Find the maximum distance from the origin for each cloud\n",
    "    max_distances = points_centered.abs().max(dim=1, keepdim=True).values.max(dim=2, keepdim=True).values\n",
    "\n",
    "    # Normalize each point cloud to be within a unit sphere\n",
    "    points_normalized = points_centered / max_distances\n",
    "\n",
    "    return points_normalized\n",
    "\n",
    "partials = torch.tensor(partials)\n",
    "partials = normalize_point_cloud(partials)\n",
    "\n",
    "partials = partials.detach().cpu().numpy()\n",
    "\n",
    "normalised_save_path = \"cloi/normalised/\"\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "for i, p in enumerate(partials):\n",
    "    pcd.points = o3d.utility.Vector3dVector(p)\n",
    "    o3d.io.write_point_cloud(normalised_save_path + str(i) + \".ply\", pcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale and move obj file\n",
    "\n",
    "def scale_obj(input_file, output_file, scale):\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    scaled_vertices = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('v '):\n",
    "            parts = line.split()\n",
    "            x = float(parts[1]) * scale\n",
    "            y = float(parts[2]) * scale - 0.5\n",
    "            z = float(parts[3]) * scale\n",
    "            scaled_vertices.append((x, y, z))\n",
    "        else:\n",
    "            scaled_vertices.append(line)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in scaled_vertices:\n",
    "            if isinstance(item, tuple):\n",
    "                f.write(\"v {} {} {}\\n\".format(item[0], item[1], item[2]))\n",
    "            else:\n",
    "                f.write(item)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_file = \"scaled_lamp3.obj\"\n",
    "output_file = \"scaled_lamp4.obj\"\n",
    "scale = 3\n",
    "\n",
    "\n",
    "scale_obj(input_file, output_file, scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "cdbb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
