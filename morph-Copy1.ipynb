{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpvL68OfBEQC"
   },
   "source": [
    "# Element Parameter Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z7n_pw4SMWl"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ47VNF7fmTS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import os.path\n",
    "import torch\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import importlib\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import functorch\n",
    "from numpy.random import default_rng\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import interact \n",
    "import gc\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from chamferdist import ChamferDistance\n",
    "from pathlib import Path\n",
    "\n",
    "import ifcopenshell\n",
    "import open3d as o3d\n",
    "\n",
    "from src.elements import *\n",
    "from src.ifc import *\n",
    "from src.preparation import *\n",
    "from src.dataset import *\n",
    "from src.pointnet import *\n",
    "from src.visualisation import *\n",
    "from src.geometry import sq_distance, get_oriented_bbox_from_points\n",
    "from src.icp import icp_finetuning\n",
    "from src.chamfer import *\n",
    "from src.utils import *\n",
    "from src.plots import plot_error_graph, plot_parameter_errors\n",
    "from src.pca import testset_PCA\n",
    "from src.finetune import chamfer_fine_tune, mahalanobis_fine_tune\n",
    "from src.cloud import add_noise\n",
    "\n",
    "\n",
    "random.seed = 42\n",
    "rng = default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sphere morphing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise a list of point clouds as an animation using open3d\n",
    "# use ctrl+c to copy and ctrl+v to set camera and zoom inside visualiser\n",
    "def create_point_cloud_animation(cloud_list, loss_func, save_image=False, colours=None):\n",
    "    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Debug)\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    cloud = cloud_list[0]\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(cloud)\n",
    "    if colours is not None:\n",
    "        point_cloud.colors = o3d.utility.Vector3dVector(colours[0])\n",
    "    vis.add_geometry(point_cloud)\n",
    "    stops = [9,39,99,299,999]\n",
    "\n",
    "    for i in range(len(cloud_list)):\n",
    "        time.sleep(0.01 + 0.05/(i/10+1))\n",
    "        cloud = cloud_list[i]\n",
    "        point_cloud.points = o3d.utility.Vector3dVector(cloud)\n",
    "        if colours is not None:\n",
    "            point_cloud.colors = o3d.utility.Vector3dVector(colours[i])\n",
    "        vis.update_geometry(point_cloud)\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "        if save_image and i in stops:\n",
    "            vis.capture_screen_image(\"sphere/\" + loss_func + str(i) + \".jpg\", do_render=True)\n",
    "    vis.destroy_window()\n",
    "\n",
    "    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_direct(x, y):\n",
    "    return torch.sum(torch.square(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# morph a sphere into the shape of an input point cloud\n",
    "# by optimising chamfer loss iteratively\n",
    "# total points = num_points**2\n",
    "def morph_sphere(src_pcd_tensor, num_points, iterations, learning_rate, stops=[],\n",
    "                 loss_func= \"chamfer\", measure_consistency=True, sphere=True, return_assignment=True):\n",
    "    \n",
    "    cuda = torch.device(\"cuda\")\n",
    "    if sphere:\n",
    "        # gnerate sphere\n",
    "        # Generate spherical coordinates\n",
    "        theta = np.linspace(0, 2 * np.pi, num_points)\n",
    "        phi = np.linspace(0, np.pi, num_points)\n",
    "\n",
    "        # Create a meshgrid from spherical coordinates\n",
    "        theta, phi = np.meshgrid(theta, phi)\n",
    "\n",
    "        # Convert spherical coordinates to Cartesian coordinates\n",
    "        x = np.sin(phi) * np.cos(theta)\n",
    "        y = np.sin(phi) * np.sin(theta)\n",
    "        z = np.cos(phi)\n",
    "\n",
    "        # Stack the coordinates to form a 3D point cloud and reshape to (num_points * num_points, 3)\n",
    "        sphere_points = np.stack([x, y, z], axis=-1).reshape(-1, 3)\n",
    "        sphere_points = np.array([sphere_points for i in range(len(src_pcd_tensor))])\n",
    "        sphere_points = torch.tensor(sphere_points, device=cuda, \n",
    "                                     requires_grad=True)\n",
    "    else:\n",
    "        sphere_points = torch.rand(1, num_points**2, 3, device=cuda, \n",
    "                                   dtype=torch.double, requires_grad=True)\n",
    "    \n",
    "    # optimise\n",
    "    optimizer = torch.optim.Adam([sphere_points], lr=learning_rate)\n",
    "    intermediate, losses, assingments = [], [], []\n",
    "    chamferDist = ChamferDistance()\n",
    "    assignments = []\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if loss_func == \"chamfer\":\n",
    "            nn = chamferDist(\n",
    "                src_pcd_tensor, sphere_points, bidirectional=True, return_nn=True)\n",
    "            loss = torch.sum(nn[1].dists) + torch.sum(nn[0].dists)\n",
    "            assignment = [nn[0].idx[:,:,0].detach().cpu().numpy(), nn[1].idx[:,:,0].detach().cpu().numpy()]\n",
    "        elif loss_func == \"emd\":\n",
    "            loss, assignment = calc_emd(sphere_points, src_pcd_tensor, 0.05, 50)\n",
    "            assignment = assignment.detach().cpu().numpy()\n",
    "        elif loss_func == \"direct\":\n",
    "            loss = calc_direct(sphere_points, src_pcd_tensor)\n",
    "            assignment = None\n",
    "        elif loss_func == \"pair\":\n",
    "            loss, assignment = get_pair_loss_clouds_tensor(src_pcd_tensor, sphere_points, add_pair_loss=True, it=i)\n",
    "        elif loss_func == \"jittery\":\n",
    "            loss = get_jittery_cd_tensor(src_pcd_tensor, sphere_points, k=1, it=i)\n",
    "        elif loss_func == \"self\":\n",
    "            loss = get_self_cd_tensor(src_pcd_tensor, sphere_points)\n",
    "        elif loss_func == \"reverse\":\n",
    "            loss, assignment = calc_reverse_weighted_cd_tensor(src_pcd_tensor, sphere_points, return_assignment=True, k=32)\n",
    "        elif loss_func == \"prob\":\n",
    "            loss, assignment = calc_pairing_probabilty_loss_tensor(src_pcd_tensor, sphere_points, k=64)\n",
    "        elif loss_func == \"balanced\":\n",
    "            loss, assignment = calc_balanced_chamfer_loss_tensor(src_pcd_tensor, sphere_points, return_assignment=True, k=32)\n",
    "        elif loss_func == \"single\":\n",
    "            loss, assignment = calc_balanced_single_chamfer_loss_tensor(src_pcd_tensor, sphere_points, return_assignment=True, k=32)\n",
    "        elif loss_func == \"infocd\":\n",
    "            loss, assignment = calc_cd_like_InfoV2(src_pcd_tensor, sphere_points, return_assignment=True)\n",
    "        else:\n",
    "            print(\"unspecified loss\")\n",
    "            \n",
    "        #print(\"a\", assignment[0].shape)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(\"iteration\", i, \"loss\", loss.item())\n",
    "        \n",
    "        if i in stops:\n",
    "            intermediate.append(sphere_points.clone())\n",
    "            losses.append(loss.item())\n",
    "            if measure_consistency:\n",
    "                assignments.append(assignment)\n",
    "            \n",
    "    # calculate final chamfer loss\n",
    "    dist = chamferDist(\n",
    "                src_pcd_tensor, sphere_points, bidirectional=True)\n",
    "    emd_loss, _ = calc_emd(sphere_points, src_pcd_tensor, 0.05, 50)\n",
    "    print(\"final chamfer dist\", dist.item(), \"emd\", emd_loss.item())\n",
    "    \n",
    "    # save assignments for analysis\n",
    "#     if measure_consistency:\n",
    "#         with open(\"sphere/assignments_\" + loss_func + \".pkl\", \"wb\") as f:\n",
    "#             pickle.dump(assingments, f)\n",
    "            \n",
    "    intermediate = torch.stack(intermediate)\n",
    "    if return_assignment:\n",
    "        assignments = assignments\n",
    "        return intermediate, losses, assignments\n",
    "    return intermediate, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_morph(cld1_name, loss_func):\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    cld1 = np.array(o3d.io.read_point_cloud(cld1_name).points)\n",
    "    src_pcd_tensor = torch.tensor([cld1], device=cuda)\n",
    "\n",
    "    iterations = 1000\n",
    "    #stops = [0, 10, 50, 100, 150, 500, 999]\n",
    "    stops = [i for i in range(0,iterations,2)]\n",
    "\n",
    "    morphed, losses = morph_sphere(src_pcd_tensor, 64, iterations, 0.01, stops, loss_func=loss_func)\n",
    "    morphed = torch.flatten(morphed, start_dim=1, end_dim=2)\n",
    "    morphed = morphed.cpu().detach().numpy()\n",
    "    #print(morphed.shape)\n",
    "\n",
    "    # save frames\n",
    "    with open(\"sphere/\" + loss_func + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(morphed, f)\n",
    "\n",
    "    with open(\"sphere/loss_\" + loss_func + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(losses, f)\n",
    "\n",
    "    # Save the PointCloud to a PCD file\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(morphed[-1])\n",
    "    o3d.io.write_point_cloud(\"sphere/sphere_\" + loss_func + \".pcd\", point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cld1_name = \"sphere/chair.pcd\"\n",
    "# loss_func = \"chamfer\"\n",
    "# run_morph(cld1_name, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# visualise animation\n",
    "cld1_name = \"sphere/plane1.pcd\"\n",
    "visualise = True\n",
    "#loss_funcs = [ \"chamfer\", \"emd\", \"balanced\", \"reverse\", \"single\"]\n",
    "#loss_funcs = [ \"balanced\", \"single\"]\n",
    "loss_funcs = [ \"balanced\"]\n",
    "for loss_func in loss_funcs:\n",
    "    print(loss_func)\n",
    "    run_morph(cld1_name, loss_func)\n",
    "    if visualise:\n",
    "        with open(\"sphere/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "            morphed = pickle.load(f)\n",
    "        colours = visualise_density(morphed, 'plasma_r')\n",
    "        with open(\"sphere/\" + loss_func + \"_dens.pkl\", \"wb\") as f:\n",
    "            pickle.dump(colours, f)\n",
    "        #create_point_cloud_animation(cloud_list, loss_func)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def view_density(loss_func): \n",
    "    with open(\"sphere/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "        morphed = pickle.load(f)\n",
    "    with open(\"sphere/\" + loss_func + \"_dens.pkl\", \"rb\") as f:\n",
    "        colours = pickle.load(f)\n",
    "\n",
    "    create_point_cloud_animation(morphed, loss_func, True, colours[:,:,:3])\n",
    "    \n",
    "interact(view_density, loss_func=[ \"balanced\", \"infocd\", \"single\", \"chamfer\", \"reverse\", \"emd\", \"direct\"]); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# visualise animation\n",
    "loss_func = \"emd\"\n",
    "with open(\"sphere/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "    morphed = pickle.load(f)\n",
    "print(morphed.shape, loss_func)\n",
    "cloud_list = [m for m in morphed]\n",
    "#create_point_cloud_animation(cloud_list, loss_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = visualise_density(morphed, 'plasma_r')\n",
    "with open(\"sphere/\" + loss_func + \"_dens.pkl\", \"wb\") as f:\n",
    "    pickle.dump(colours, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch sphere optimisation (for metrics)\n",
    "\n",
    "def sphere_morph_metrics(loss_func, shapenet_path):\n",
    "    # load shapenet test dataset\n",
    "    folders = os.listdir(shapenet_path)\n",
    "    print(loss_func)\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    chamferDist = ChamferDistance()\n",
    "\n",
    "    iterations = 1001\n",
    "    stops = [i for i in range(0,1001,10)]\n",
    "    chamfer_results = np.zeros(len(stops))\n",
    "    emd_results = np.zeros(len(stops))\n",
    "    count = 0\n",
    "    assignments_folders = []\n",
    "    \n",
    "    for fl in folders:\n",
    "        files = os.listdir(shapenet_path + fl)\n",
    "        clouds = []\n",
    "        for cl in files:\n",
    "            clouds.append(np.array(o3d.io.read_point_cloud(shapenet_path + fl + \"/\" + cl).points))\n",
    "        clouds = np.array(clouds)\n",
    "        clouds = torch.tensor(clouds, device=cuda)\n",
    "        count += len(clouds)\n",
    "        #print(clouds.shape)\n",
    "\n",
    "        # optimise spheres and gather intermediate clouds\n",
    "        \n",
    "        morphed, losses, assignments = morph_sphere(clouds, 64, iterations, 0.01, stops, loss_func=loss_func, \n",
    "                                       return_assignment=True)\n",
    "        assignments_folders.append(assignments)\n",
    "\n",
    "        #calculate chamfer and EMD\n",
    "        print(morphed.shape)\n",
    "\n",
    "        # loop through stops\n",
    "        for i, mr in enumerate(tqdm(morphed)):\n",
    "            nn = chamferDist(clouds, mr, bidirectional=True, return_nn=True)\n",
    "            cd_loss = torch.sum(nn[1].dists) + torch.sum(nn[0].dists)\n",
    "            #cd_assignment = [nn[0].idx[0,:,0].detach().cpu().numpy(), nn[1].idx[0,:,0].detach().cpu().numpy()]\n",
    "            emd_loss, _ = calc_emd(clouds, mr, 0.05, 50)\n",
    "            #emd_assignment = emd_assignment.detach().cpu().numpy()\n",
    "\n",
    "            #print(cd_loss.item(), emd_loss.item())\n",
    "#             cd_assignments_cat.append(cd_assignment)\n",
    "#             emd_assignments_cat.append(emd_assignment)\n",
    "            chamfer_results[i] += cd_loss\n",
    "            emd_results[i] += emd_loss\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(count, chamfer_results[0])\n",
    "    chamfer_results = chamfer_results/count\n",
    "    emd_results = emd_results/count\n",
    "\n",
    "    # save results\n",
    "    with open(\"sphere/\" + loss_func + \"_metrics.pkl\", \"wb\") as f:\n",
    "        pickle.dump([chamfer_results, emd_results, assignments_folders], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample\n",
    "shapenet_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/ShapeNetCompletion/test/complete/\"\n",
    "downsampled_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/ShapeNetCompletion/downsample/\"\n",
    "# folders = os.listdir(shapenet_path)\n",
    "# choices = np.random.choice(len(points), 4096)\n",
    "\n",
    "# for fl in tqdm(folders):\n",
    "#     if not os.path.exists(downsampled_path+fl):\n",
    "#         os.mkdir(downsampled_path+fl)\n",
    "        \n",
    "#     files = os.listdir(shapenet_path + fl)\n",
    "#     cloud =  o3d.geometry.PointCloud()\n",
    "#     for cl in files:\n",
    "#         points = np.array(o3d.io.read_point_cloud(shapenet_path + fl + \"/\" + cl).points)\n",
    "#         points = points[choices]\n",
    "#         cloud.points = o3d.utility.Vector3dVector(points)\n",
    "#         o3d.io.write_point_cloud(downsampled_path+fl + \"/\" + cl, cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = \"balanced\"\n",
    "loss_func = \"balanced\"\n",
    "\n",
    "sphere_morph_metrics(loss_func, downsampled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to recombine batched output from balancedCD\n",
    "file_prefix = \"sphere/balanced_metrics\"\n",
    "icd_chamfer_list, icd_emd_list, icd_assignment_list = [], [], []\n",
    "\n",
    "for i in range(1,9):\n",
    "    with open(file_prefix + str(i) + \".pkl\", \"rb\") as f:\n",
    "        chamfer, emd, ass = pickle.load(f)\n",
    "        print(len(ass), len(ass[0]), len(ass[0][0]), ass[0][0][0].shape)\n",
    "        icd_chamfer_list.append(chamfer)\n",
    "        icd_emd_list.append(emd)\n",
    "        icd_assignment_list.append(ass[0])\n",
    "\n",
    "icd_assignment_list = np.array(icd_assignment_list)        \n",
    "icd_assignment_list = np.transpose(icd_assignment_list, axes=(1,2,0,3,4))\n",
    "icd_assignment_list = np.reshape(icd_assignment_list, (101, 2, 8*150, 4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_cd = np.sum(np.array(icd_chamfer_list), axis=0)\n",
    "balanced_emd = np.sum(np.array(icd_emd_list), axis=0)\n",
    "print(balanced_emd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plots\n",
    "loss_funcs = [\"emd\"]\n",
    "#loss_funcs = [\"infocd\", \"chamfer\", \"emd\", \"balanced\"]\n",
    "chamfer_list, emd_list = [], []\n",
    "for loss_func in loss_funcs:\n",
    "    if loss_func == \"balanced\":\n",
    "        continue\n",
    "    with open(\"sphere/\" + loss_func + \"_metrics.pkl\", \"rb\") as f:\n",
    "        chamfer, emd, ass = pickle.load(f)\n",
    "        chamfer_list.append(chamfer)\n",
    "        emd_list.append(emd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"balanced\" in loss_funcs:\n",
    "    chamfer_list.append(balanced_cd)\n",
    "    emd_list.append(balanced_emd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rc('xtick', labelsize=14) \n",
    "plt.rc('ytick', labelsize=14) \n",
    "loss_funcs = [\"infoCD\", \"CD\", \"EMD\", \"UniformCD\"]\n",
    "\n",
    "plot_dists(axes[1], chamfer_list, loss_funcs, \"Chamfer Distance\", \"iterations\", log=True)\n",
    "plot_dists(axes[0], emd_list, loss_funcs, \"Earth Mover's Distance\", \"iterations\", log=True)\n",
    "print(\"EMD infocd\", emd_list[0][-1], \"chamfer\", emd_list[1][-1],  \"emd\", emd_list[2][-1], \"BALANCED\", emd_list[3][-1])\n",
    "print(\"CD infocd\", chamfer_list[0][-1], \"chamfer\", chamfer_list[1][-1], \"emd\", chamfer_list[2][-1], \"balanced\", chamfer_list[3][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emd_list[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ass), len(ass[0]), ass[0][0].shape, len(ass[0][0]), ass[0][0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ass2), len(ass2[0]), len(ass2[0][0]), ass2[0][0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses on same axis\n",
    "def plot_dists(ax, losses, labels, title, xlabel=\"point cloud index\", ylabel=\"distance (log)\", \n",
    "               log=True, limit=None, legend=True):\n",
    "    if limit == None:\n",
    "        limit = len(losses[0])\n",
    "    x = np.arange(0, limit*10, 10)\n",
    "\n",
    "#     # scale chamfer distance to be comparable with mahalanobis\n",
    "#     max = []\n",
    "\n",
    "#     mahal_dist_max = np.max(mahal_dist_sk)\n",
    "#     chamfer_dist_max = np.max(chamfer_dist)\n",
    "#     chamfer_dist = chamfer_dist / chamfer_dist_max * mahal_dist_max\n",
    "\n",
    "    for i, loss in enumerate(losses):\n",
    "        if log:\n",
    "            ax.plot(x, np.log(loss[:limit]), label=labels[i])\n",
    "        else:\n",
    "            ax.plot(x, loss[:limit], label=labels[i])\n",
    "\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    if legend:\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load losses\n",
    "# single, not batch\n",
    "loss_types = [\"reverse\", \"chamfer\", \"emd\", \"pair\"]\n",
    "losses = []\n",
    "\n",
    "for loss_func in loss_types:\n",
    "    with open(\"sphere/loss_\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "        losses.append(pickle.load(f))\n",
    "        \n",
    "plot_dists(losses, loss_types, \"loss function comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure conssistency between forward and backward correspondences for chamfer distance\n",
    "# optionally compare against the ideal assignment, as measured by EMD\n",
    "def measure_assignment_consistency(assignment, emd=None):\n",
    "    reverse_assignment = torch.gather(assignment[0], 0, assignment[1])\n",
    "    expected = torch.arange(assignment[0].shape[0], device=torch.device(\"cuda\"))\n",
    "    consistency = torch.sum(torch.eq(expected, reverse_assignment).long())\n",
    "    print(\"consistency\", consistency.item(), len(torch.unique(assignment[0])), len(torch.unique(assignment[1])))\n",
    "    \n",
    "    if emd is not None:\n",
    "        #print(emd[:5], assignment[0][:5], assignment[1][:5])\n",
    "        emd_consistency = torch.sum(torch.eq(emd, assignment[0]).long())\n",
    "        #print(\"emd_consistency\", emd_consistency.item(), len(torch.unique(emd)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure conssistency for EMD approx correspondences\n",
    "# this is different from other distances as only a one way assignment is returned\n",
    "def measure_batch_assignment_consistency_emd(assignment):\n",
    "    consistencies = np.array([4096 for i in range(assignment.shape[-1])])\n",
    "    cuda = torch.device(\"cuda\")\n",
    "\n",
    "    uniques = []\n",
    "    changes = []\n",
    "    for i in tqdm(range(len(assignment))):\n",
    "        class_assignment = torch.tensor(assignment[i], device=cuda)\n",
    "        \n",
    "        unique = 0\n",
    "        for j in range(len(class_assignment)):\n",
    "            unique += len(torch.unique(class_assignment[j]))\n",
    "        unique = (unique/len(class_assignment))\n",
    "        uniques.append(unique)\n",
    "        \n",
    "        # check rate of change of matches\n",
    "        if i==0:\n",
    "            change = 0\n",
    "        else:\n",
    "            change = (assignment[i] == assignment[i-1]).sum()\n",
    "            change = (change/len(class_assignment[0]))\n",
    "        changes.append(change)\n",
    "        \n",
    "        print(\"un\", unique, \"change\", change)\n",
    "        \n",
    "    # save results\n",
    "    with open(\"sphere/\" + \"emd\" + \"_consistency.pkl\", \"wb\") as f:\n",
    "        pickle.dump([consistencies, uniques, changes], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "measure_batch_assignment_consistency_emd(ass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure conssistency between forward and backward correspondences\n",
    "# measure number of unique assignments\n",
    "# measure rate of change of assignments\n",
    "def measure_batch_assignment_consistency(assignment, loss):\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    # loop through iterations\n",
    "    consistencies = []\n",
    "    uniques = []\n",
    "    changes = []\n",
    "    for i in tqdm(range(len(assignment))):\n",
    "        \n",
    "        # check reverse consistency\n",
    "        class_assignment = torch.tensor(assignment[i], device=cuda)\n",
    "        reverse_assignment = torch.gather(class_assignment[0], 1, class_assignment[1])\n",
    "        #print(\"r\", reverse_assignment.shape)\n",
    "        expected = torch.arange(class_assignment[0].shape[1], device=cuda)\n",
    "        expected = expected.repeat(class_assignment[0].shape[0], 1)\n",
    "        #print(\"e\", expected.shape)\n",
    "        consistency = torch.sum(torch.eq(expected, reverse_assignment).long())/len(class_assignment[0])\n",
    "        consistencies.append(consistency.item())\n",
    "        \n",
    "        # check uniqueness of matches\n",
    "        unique = 0\n",
    "        for j in range(len(class_assignment[0])):\n",
    "            unique += (len(torch.unique(class_assignment[0][j])) +\n",
    "                       len(torch.unique(class_assignment[1][j])))\n",
    "        unique = (unique/len(class_assignment[0]))/2\n",
    "        uniques.append(unique)\n",
    "        \n",
    "        # check rate of change of matches\n",
    "        if i==0:\n",
    "            change = 0\n",
    "        else:\n",
    "            change = (assignment[i] == assignment[i-1]).sum()\n",
    "            change = (change/len(class_assignment[0]))/2\n",
    "        changes.append(change)\n",
    "        \n",
    "        print(\"un\", unique, \"consistency\", consistency.item(), \"change\", change)\n",
    "\n",
    "    \n",
    "    # save results\n",
    "    with open(\"sphere/\" + loss + \"_consistency.pkl\", \"wb\") as f:\n",
    "        pickle.dump([consistencies, uniques, changes], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "loss = \"emd\"\n",
    "#ass = np.array(ass)\n",
    "\n",
    "if loss == \"infocd\":\n",
    "    ass = np.transpose(ass, axes=(1,2,0,3,4,5))\n",
    "    ass = np.reshape(ass, (101, 2, 8*150, 4096))\n",
    "    \n",
    "if loss == \"chamfer\":\n",
    "    ass = np.transpose(ass, axes=(1,2,0,3,4))\n",
    "    ass = np.reshape(ass, (101, 2, 8*150, 4096))\n",
    "    \n",
    "if loss == \"emd\":\n",
    "    ass = np.array(ass)\n",
    "    ass = np.transpose(ass, axes=(1,0,2,3))\n",
    "    ass = np.reshape(ass, (101, 8*150, 4096))\n",
    "print(ass.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "measure_batch_assignment_consistency(icd_assignment_list, loss=\"UniformCD\")\n",
    "#measure_batch_assignment_consistency(ass, loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot consistency\n",
    "\n",
    "loss_funcs = [\"infocd\", \"chamfer\", \"emd\", \"UniformCD\"]\n",
    "unique_list, consistency_list, change_list = [], [], []\n",
    "for loss_func in loss_funcs:\n",
    "    with open(\"sphere/\" + loss_func + \"_consistency.pkl\", \"rb\") as f:\n",
    "        consistency, unique, change = pickle.load(f)\n",
    "        consistency_list.append(consistency)\n",
    "        unique_list.append(unique)\n",
    "        change_list.append(change)\n",
    "print(change_list[0][-1], change_list[1][-1])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 5))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rc('xtick', labelsize=14) \n",
    "plt.rc('ytick', labelsize=14) \n",
    "\n",
    "limit= 50\n",
    "loss_funcs = [\"InfoCD\", \"CD\", \"EMD\", \"UniformCD\"]\n",
    "\n",
    "plot_dists(axes[0], consistency_list, loss_funcs, title=\"Backward consistency\", \n",
    "           ylabel=\"no. of points\", xlabel=\"Iterations\", log=False, limit=limit, legend=False)\n",
    "plot_dists(axes[1], unique_list, loss_funcs, title=\"Point coverage\", \n",
    "           ylabel=\"\", xlabel=\"Iterations\", log=False, limit=limit, legend=False)\n",
    "plot_dists(axes[2], change_list, loss_funcs, title=\"Correspondence variation\", \n",
    "           ylabel=\"\", xlabel=\"Iterations\", log=False, limit=limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare correspondences\n",
    "#TODO: include consistency in top5 matches?\n",
    "loss_func = \"emd\"\n",
    "with open(\"sphere/assignments_\" + \"emd\" + \".pkl\", \"rb\") as f:\n",
    "    emd_assignment = pickle.load(f)\n",
    "\n",
    "with open(\"sphere/assignments_\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "    assignment = pickle.load(f)\n",
    "    \n",
    "for i, ass in enumerate(assignment):\n",
    "    #measure_assignment_consistency(ass, emd_assignment[i][0])\n",
    "    measure_assignment_consistency(ass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clouds\n",
    "tgt_path = \"sphere/target.pcd\"\n",
    "l1_path = \"sphere/l1d.pcd\"\n",
    "l2_path = \"sphere/l2d.pcd\"\n",
    "l3_path = \"sphere/l3d.pcd\"\n",
    "l4_path = \"sphere/l4d.pcd\"\n",
    "\n",
    "tgt = np.array(o3d.io.read_point_cloud(tgt_path).points)\n",
    "l1 = np.array(o3d.io.read_point_cloud(l1_path).points)\n",
    "l2 = np.array(o3d.io.read_point_cloud(l2_path).points)\n",
    "l3 = np.array(o3d.io.read_point_cloud(l3_path).points)\n",
    "l4 = np.array(o3d.io.read_point_cloud(l4_path).points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure point-wise distance between two clouds\n",
    "def get_point_distance(src, tgt, loss=\"chamfer\"):\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    src_tensor = torch.tensor([src], device=cuda)\n",
    "    tgt_tensor = torch.tensor([tgt], device=cuda)\n",
    "    chamferDist = ChamferDistance()\n",
    "\n",
    "    if loss == \"chamfer\":\n",
    "        nn = chamferDist(\n",
    "            src_tensor, tgt_tensor, bidirectional=True, return_nn=True)\n",
    "        weights = nn[1].dists[0,:,0]\n",
    "        #print( torch.sum(nn[1].dists))\n",
    "        loss = torch.sum(nn[1].dists)\n",
    "        \n",
    "    elif loss == \"balanced\":\n",
    "        dist_0, dist_1 = calc_balanced_chamfer_loss_tensor(src_tensor, tgt_tensor, return_dists=True, k=32)\n",
    "        loss = torch.sum(dist_1[0])\n",
    "        #print( torch.sum(dist_1[0]))\n",
    "        weights = dist_1[0]\n",
    "    \n",
    "    return weights.detach().cpu().numpy(), loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a colour map based on the weights of a point cloud\n",
    "def visualise_point_loss(weights, high, low, colormap_name='plasma'):\n",
    "    # normalise weights\n",
    "    diff = high - low\n",
    "    weights = (weights - low) / diff\n",
    "    \n",
    "    # map colour\n",
    "    colours = np.zeros((len(weights), 4))\n",
    "    colormap = plt.get_cmap(colormap_name)\n",
    "    for j, pt in enumerate(weights):\n",
    "        colours[j] = colormap(pt)\n",
    "\n",
    "    return colours[:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def get_point_weights(tgt, l1, l2, l3, l4, loss_func=\"chamfer\"):\n",
    "    w1, loss1 = get_point_distance(tgt, l1, loss_func)\n",
    "    w2, loss2 = get_point_distance(tgt, l2, loss_func)\n",
    "    w3, loss3 = get_point_distance(tgt, l3, loss_func)\n",
    "    w4, loss4 = get_point_distance(tgt, l4, loss_func)\n",
    "\n",
    "    high1, low1 = np.max(w1), np.min(w1)\n",
    "    high2, low2 = np.max(w2), np.min(w2)\n",
    "    high3, low3 = np.max(w3), np.min(w3)\n",
    "    high4, low4 = np.max(w4), np.min(w4)\n",
    "\n",
    "    high = max(high1, high2, high3, high4)\n",
    "    low = min(low1, low2, low3, low4)\n",
    "    \n",
    "    losses = [loss1, loss2, loss3, loss4]\n",
    "    print(\"loss1\", loss1, \"loss2\", loss2, \"loss3\", loss3, \"loss4\", loss4)\n",
    "    \n",
    "    return high, low, w1, w2, w3, w4, losses\n",
    "\n",
    "\n",
    "cd_high, cd_low, cd_w1, cd_w2, cd_w3, cd_w4, cd_losses = get_point_weights(tgt, l1, l2, l3, l4, loss_func=\"chamfer\")\n",
    "balanced_high, balanced_low, balanced_w1, balanced_w2, balanced_w3, balanced_w4, balanced_losses = get_point_weights(tgt, l1, l2, l3, l4, loss_func=\"balanced\")\n",
    "\n",
    "high, low = max(cd_high, balanced_high), min(cd_low, balanced_low)\n",
    "\n",
    "#colours = visualise_point_loss(balanced_w1, high, low, 'plasma_r')\n",
    "#colours = visualise_point_loss(balanced_w4, balanced_high, balanced_low, 'plasma_r')\n",
    "colours = visualise_point_loss(cd_w2, cd_high, cd_low, 'plasma_r')\n",
    "\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "#print(colours, cloud.shape)\n",
    "point_cloud.points = o3d.utility.Vector3dVector(l2)\n",
    "point_cloud.colors = o3d.utility.Vector3dVector(colours)\n",
    "o3d.visualization.draw_geometries([point_cloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud = o3d.geometry.PointCloud()\n",
    "#print(colours, cloud.shape)\n",
    "point_cloud.points = o3d.utility.Vector3dVector(tgt)\n",
    "point_cloud.paint_uniform_color([0.1, 0.8, 0.6])\n",
    "\n",
    "o3d.visualization.draw_geometries([point_cloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "print(normalize([balanced_losses]))\n",
    "X = np.arange(4)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "#ax.bar(X + 0.00, normalize([balanced_losses])[0], color = 'b', width = 0.25)\n",
    "ax.bar(X + 0.25, normalize([cd_losses])[0], color = 'mediumslateblue', width = 0.8)\n",
    "ax.set(ylabel=\"Loss (normalised)\")\n",
    "ax.set(title=\"Chamfer distance\")\n",
    "\n",
    "x1,x2,y1,y2 = plt.axis()  \n",
    "plt.axis((x1,x2,0,0.7))\n",
    "ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(4)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.rc('xtick', labelsize=10) \n",
    "#ax.bar(X + 0.00, normalize([balanced_losses])[0], color = 'b', width = 0.25)\n",
    "ax.bar(X + 0.25, normalize([balanced_losses])[0], color = 'indianred', width = 0.8)\n",
    "ax.set(ylabel=\"Loss (normalised)\")\n",
    "ax.set(title=\"UniformCD distance\")\n",
    "x1,x2,y1,y2 = plt.axis()  \n",
    "plt.axis((x1,x2,0,0.7))\n",
    "ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CD EMD scatterplot on PCN results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"sphere/out_metrics_cd.pkl\", \"rb\") as f:\n",
    "    chamfer_cd, chamfer_emd = pickle.load(f)\n",
    "    \n",
    "chamfer_cd = np.array(chamfer_cd)\n",
    "chamfer_emd = np.array(chamfer_emd)*100\n",
    "\n",
    "with open (\"sphere/out_metrics.pkl\", \"rb\") as f:\n",
    "    balanced_cd, balanced_emd = pickle.load(f)\n",
    "    \n",
    "balanced_cd = np.array(balanced_cd)\n",
    "balanced_emd = np.array(balanced_emd)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize']=(10,6)\n",
    "plt.xlabel('CD', fontsize=20)\n",
    "plt.ylabel('EMD', fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.title('Testset Correlation between CD and EMD', fontsize=20)\n",
    "\n",
    "plt.scatter(balanced_cd, balanced_emd, s=3, color=\"lightseagreen\")\n",
    "plt.scatter(chamfer_cd, chamfer_emd, s=3, color=\"palevioletred\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find correlation between chamfer and EMD\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "print(\"balanced r\", scipy.stats.pearsonr(balanced_cd, balanced_emd))\n",
    "print(\"chamfer r\", scipy.stats.pearsonr(chamfer_cd, chamfer_emd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completion visual"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
