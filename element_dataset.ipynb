{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c41916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import ifcopenshell\n",
    "from ifcopenshell import template\n",
    "import ifcopenshell.geom\n",
    "\n",
    "from src.visualisation import *\n",
    "from src.ifc import *\n",
    "from src.elements import create_pipe, create_elbow\n",
    "from src.dataset import *\n",
    "from src.preparation import *\n",
    "from src.cloud import add_noise\n",
    "\n",
    "# create_guid = lambda: ifcopenshell.guid.compress(uuid.uuid1().hex)\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b951d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "rng = default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254db9ec",
   "metadata": {},
   "source": [
    "### CLOI Dataset Creation\n",
    "\n",
    "The following section converts CLOI scans into a pcd dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c829d9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine clouds\n",
    "data_path = \"/mnt/f/datasets/export/export/\"\n",
    "max_points = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed33839",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(data_path)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d33efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = []\n",
    "element_count = 0\n",
    "error_count = 0\n",
    "for i, cl in enumerate(classes):\n",
    "    all_elements = []\n",
    "    class_path = data_path + cl\n",
    "    elements = os.listdir(class_path)\n",
    "    for j, el in tqdm(enumerate(elements)):\n",
    "        try:\n",
    "            element = np.loadtxt(class_path + \"/\" + el)\n",
    "\n",
    "            # downsample\n",
    "            if len(element) > 0 and element.ndim == 2 and element.shape[1] == 4:\n",
    "                if len(element) > max_points:\n",
    "                    # idx = np.random.randint(element.shape[0], size=max_points)\n",
    "                    # element = element[idx :]\n",
    "                    element = np.random.permutation(element)[:max_points]\n",
    "\n",
    "                element = np.delete(element, 3, axis=1)  # remove point index\n",
    "                element = np.insert(\n",
    "                    element, 3, values=[element_count], axis=1\n",
    "                )  # add element index\n",
    "                # print(element.shape)\n",
    "                element_count += 1\n",
    "                all_elements.append(element)\n",
    "        except Exception as E:\n",
    "            error_count += 1\n",
    "\n",
    "    all_elements = np.vstack(all_elements)\n",
    "    all_elements = np.insert(all_elements, 4, values=[i], axis=1)  # add class index\n",
    "    all_classes.append(all_elements)\n",
    "\n",
    "all_classes = np.concatenate(all_classes)\n",
    "print(all_classes.shape)\n",
    "print(\"errors: \", error_count)\n",
    "\n",
    "# print(points[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10faefd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.t.geometry.PointCloud()\n",
    "points = all_classes[:, 0:3]\n",
    "el_index = [[i] for i in all_classes[:, 3]]\n",
    "cl_index = [[i] for i in all_classes[:, 4]]\n",
    "\n",
    "\n",
    "pcd.point[\"positions\"] = o3d.core.Tensor(points)\n",
    "pcd.point[\"elements\"] = o3d.core.Tensor(el_index)\n",
    "pcd.point[\"classes\"] = o3d.core.Tensor(cl_index)\n",
    "\n",
    "o3d.t.io.write_point_cloud(\"water2.pcd\", pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36616a89",
   "metadata": {},
   "source": [
    "## Pipe parameter detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f2162",
   "metadata": {},
   "source": [
    "### Generation of synethetic IFC element dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15067b95",
   "metadata": {},
   "source": [
    "#### Dataset creation process\n",
    "1. Generate params for element model\n",
    "2. Generate ifc models\n",
    "3. Convert to obj models using ifcConvert (ifc2obj.py script (synthetic) OR element_to_obj function (BP)) \n",
    "4. Convert to partially occluded EXR images using render_depth.py script *./blender -b -P render_depth.py ../industrial-facility-relationships/output/obj/ output/*\n",
    "5. Convert to point clouds using process_exr.py script *python process_exr.py output/exr/ output/intrinsics.txt output/*\n",
    "6. Combine multiple views of object to create training and testing datasets (for this, the metadata generated in step 3 must be pasted to individual metadata files for each class. This steps outputs a metadata_new file. set multiple=False for BP datasets)\n",
    "\n",
    "BP datasets are created by following steps 3->6 above after splitting extracted IFC into multiple IFCs (to fix overlapping pieces)\n",
    "\n",
    "##### Step 1 & 2. IFC model generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b37d51aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "density = 2048\n",
    "sample_size = 4096\n",
    "config_path = \"config/pipeline.json\"\n",
    "pcd_path = \"/home/haritha/documents/blender-2.79-linux-glibc219-x86_64/output_elbow/pcd/\"\n",
    "blueprint = \"data/sample.ifc\"\n",
    "num_scans = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d769f40",
   "metadata": {},
   "source": [
    "**Elbow - modelled as an IfcRevolvedAreaSolid model**\n",
    "\n",
    "*params:*\n",
    "\n",
    "- position - 3D coordinate\n",
    "- direction - 3D vector, axis of extrusion (normal to axis of revolution) (z>=0)\n",
    "- axis_position - 2D coordinate, relative to position\n",
    "- angle - angle of revolution (0 -> pi)\n",
    "- radius\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2340ef",
   "metadata": {},
   "source": [
    "**Pipe - modelled as an IfcExtrudedAreaSolid model**\n",
    "\n",
    "*params:*\n",
    "    \n",
    "- position - 3D coordinate\n",
    "- extrusion_direction - 3D vector (z>=0)\n",
    "- length\n",
    "- radius\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c89590",
   "metadata": {},
   "source": [
    "**Tee - modelled as a combination of 2 IfcRevolvedAreaSolid models**\n",
    "\n",
    "The two pipes are each susbtracted from the other to create an IfcCsgSolid using an IfcBooleanResult.\n",
    "\n",
    "*params:*\n",
    "    \n",
    "- position - 3D coordinate\n",
    "- extrusion_direction1 - 3D vector (z>=0)\n",
    "- extrusion_direction2 - 3D vector (z>=0)\n",
    "- length1\n",
    "- length2 - percentage of length1\n",
    "- tee angle - 90 degrees or within an angle range\n",
    "- radius1\n",
    "- radius2 - same as radius1 or percentage of radius1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b080190c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# synthetic_dataset(config_path, sample_size, \"pipe\", 'occluded', blueprint, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3985b78",
   "metadata": {},
   "source": [
    "*Use external scripts to convert above IFC dataset into ocluded PCD dataset. (step 3, 4 & 5)*\n",
    "\n",
    "##### Step 6. Test / train dataset creation\n",
    "\n",
    "1. merge multiple views\n",
    "2. sample to standard density\n",
    "3. generate training and testing dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e176e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_merged_dataset(\n",
    "    pcd_path, \"completion/\", \"elbow\", num_scans, density, 3, 0.0, False, multiple=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create occluded dataset with non occluded ground truth for point cloud completion\n",
    "create_completion_dataset(\n",
    "    pcd_path, \"completion_noisy/\", \"elbow\", num_scans, density, 1, 0.1, False, multiple=False, noise=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(7 * sum([i for i in range(1, 5 + 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise  to existing dataset\n",
    "input_dir = \"output/tee/train/\"\n",
    "output_dir = \"output/noisy/tee/train/\"\n",
    "noise_size = 128\n",
    "cloud = o3d.geometry.PointCloud()\n",
    "\n",
    "files = os.listdir(input_dir)\n",
    "\n",
    "for f in tqdm(files):\n",
    "    points = np.array(o3d.io.read_point_cloud(input_dir + f).points)\n",
    "    noisy_points = add_noise(points, noise_size, rng)\n",
    "    noisy_points = o3d.utility.Vector3dVector(noisy_points)\n",
    "    cloud.points = noisy_points\n",
    "    o3d.io.write_point_cloud(output_dir + f, cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to recover pipe metadata from metadata_new since the metadata file has magically gotten corrupted\n",
    "\n",
    "new_m = \"output/pipe/metadata_new.json\"\n",
    "f = open(new_m, \"r\")\n",
    "metadata = json.load(f)\n",
    "meta_dict = {}\n",
    "\n",
    "for m in metadata:\n",
    "    meta_dict[metadata[m][\"initial_ifc\"]] = {\n",
    "        \"radius\": metadata[m][\"radius\"],\n",
    "        \"direction\": metadata[m][\"direction\"],\n",
    "        \"length\": metadata[m][\"length\"],\n",
    "        \"position\": metadata[m][\"position\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17db78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb33353",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"responses.csv\"\n",
    "data = pd.read_csv(file_path, sep=\",\")\n",
    "# Display the first few rows of the dataframe\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c79020",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_terms = data[\"Number of terms in postgrad accommodation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(category_based_on_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the variable 'data' contains the DataFrame\n",
    "data[\"category\"] = pd.cut(\n",
    "    data[\"Number of terms in postgrad accommodation\"],\n",
    "    bins=[-1, 0, 3, 6, float(\"inf\")],\n",
    "    labels=[1, 2, 3, 4],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462cd6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffbfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae4354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the DataFrame is read from a CSV\n",
    "file_path = \"data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Select columns \"1st choice\" to \"10th choice\"\n",
    "columns_of_interest = [f\"{i}th choice\" for i in range(1, 11)]\n",
    "columns_of_interest[0] = \"1st choice\"  # Correct the first item\n",
    "columns_of_interest[1] = \"2nd choice\"  # Correct the second item\n",
    "columns_of_interest[2] = \"3rd choice\"  # Correct the third item\n",
    "\n",
    "# Extract the specific columns\n",
    "choices_data = data[columns_of_interest]\n",
    "\n",
    "# Define the prefixes to check\n",
    "prefixes = (\"OX\", \"OR\", \"PS\", \"HR\")\n",
    "\n",
    "\n",
    "# Function to check if the value starts with any of the specified prefixes\n",
    "def starts_with_prefix(value):\n",
    "    if pd.isna(value):\n",
    "        return False\n",
    "    return value.startswith(prefixes)\n",
    "\n",
    "\n",
    "# Apply the function to each cell in the DataFrame\n",
    "matches_prefix = choices_data.applymap(starts_with_prefix)\n",
    "\n",
    "# Sum the True values across the row to check if there are at least four\n",
    "data[\"meets_criteria\"] = matches_prefix.sum(axis=1) >= 4\n",
    "\n",
    "# Check if all 10 values in the row are unique\n",
    "data[\"meets_uniqueness_criteria\"] = choices_data.apply(\n",
    "    lambda row: len(set(row)) == 10, axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Display the updated DataFrame with the new 'meets_criteria' column\n",
    "print(data[[\"meets_criteria\"] + columns_of_interest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data_hostels_unique.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d5d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
