{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpvL68OfBEQC"
   },
   "source": [
    "# Element Parameter Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z7n_pw4SMWl"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ47VNF7fmTS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import os.path\n",
    "import torch\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import importlib\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import functorch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "#from chamferdist import ChamferDistance\n",
    "from pathlib import Path\n",
    "\n",
    "import ifcopenshell\n",
    "import open3d as o3d\n",
    "\n",
    "from src.elements import *\n",
    "from src.ifc import *\n",
    "from src. preparation import *\n",
    "from src.dataset import *\n",
    "from src.pointnet import *\n",
    "from src.visualisation import *\n",
    "from src.geometry import sq_distance\n",
    "from src.icp import icp_finetuning\n",
    "from src.chamfer import *\n",
    "from src.utils import *\n",
    "from src.plots import plot_error_graph, plot_parameter_errors\n",
    "from src.pca import testset_PCA\n",
    "from src.finetune import chamfer_fine_tune\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chamfer_distance(points1_t, points2_t))\n",
    "\n",
    "chamferDist = ChamferDistance()\n",
    "print(chamferDist(points1_t, points2_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpzTlKjmlr2q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xyu78RWIQEQJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#path = Path(\"ModelNet10\")\n",
    "#path = Path('/content/drive/MyDrive/ElementNet/')\n",
    "path = Path('output/')\n",
    "#savepath = '/content/drive/MyDrive/ElementNet/'\n",
    "savepath = 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Isb_97zOA8Tl"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8W4gOI_P9a9"
   },
   "source": [
    "## Test\n",
    "\n",
    "Analyze results statistically\n",
    "\n",
    "POINTNET++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pOl95glmphX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                    Normalize(),\n",
    "#                    RandomNoise(),\n",
    "                    ToTensor()\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data and model\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('industrial-facility-relationships/'))\n",
    "BASE_DIR = os.path.join(BASE_DIR, 'pointnet2')\n",
    "ROOT_DIR = BASE_DIR\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
    "inference = False\n",
    "if inference:\n",
    "    path = Path('output/bp_data/')\n",
    "    ext = \".ply\"\n",
    "#     path = Path('tee_fix/')\n",
    "#     ext = \".pcd\"\n",
    "else:\n",
    "    path = Path('output/')\n",
    "    ext = \".pcd\"\n",
    "\n",
    "cat= 'tee'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_normals = False\n",
    "cat_targets = {\"elbow\":14, \"tee\":19, \"pipe\":11}\n",
    "\n",
    "if inference:\n",
    "    test_ds = PointCloudData(path, valid=True, folder='test', category=cat, transform=train_transforms, inference=True)\n",
    "    targets = cat_targets[cat]\n",
    "else:\n",
    "    test_ds = PointCloudData(path, valid=True, folder='test', category=cat, transform=train_transforms)\n",
    "    targets = test_ds.targets\n",
    "\n",
    "testDataLoader = torch.utils.data.DataLoader(dataset=test_ds, batch_size=32)\n",
    "test_criterion = nn.MSELoss()\n",
    "\n",
    "model_name = \"pointnet2_cls_ssg\"\n",
    "model_path = Path(\"pointnet2/log/classification/pointnet2_cls_ssg/\")\n",
    "model = importlib.import_module(model_name)\n",
    "\n",
    "\n",
    "predictor = model.get_model(targets, normal_channel=use_normals)\n",
    "if device != \"cpu\":\n",
    "    predictor = predictor.cuda()\n",
    "\n",
    "checkpoint = torch.load(model_path/'checkpoints/best_model.pth')\n",
    "#checkpoint = torch.load(model_path/'checkpoints/models/best_model_t_chamfer_0005.pth')\n",
    "predictor.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blueprint = 'data/sample.ifc'\n",
    "temp_dir = \"output/temp/\"\n",
    "target_dir = \"output/tee/test/\"\n",
    "\n",
    "ifcConvert_executable = \"scripts/./IfcConvert\"\n",
    "cloudCompare_executable = \"cloudcompare.CloudCompare\"\n",
    "sample_size = 2048\n",
    "threshold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_inference(model, loader, device, calculate_score=False):\n",
    "    predictor = model.eval()\n",
    "    predictions_list, pcd_list, id_list = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for j, data  in tqdm(enumerate(loader), total=len(loader)):\n",
    "            points, ids = data['pointcloud'].to(device).float(), data['id'].to(device)\n",
    "            points = points.transpose(2, 1)\n",
    "            preds, _ = predictor(points)\n",
    "            preds, points, ids = preds.to(torch.device('cpu')), points.to(torch.device('cpu')), data['id'].to(torch.device('cpu'))\n",
    "            for i, pr in enumerate(preds):\n",
    "                predictions_list.append(pr.numpy())\n",
    "                pcd_list.append(points[i].numpy())\n",
    "                id_list.append(ids[i].numpy())\n",
    "\n",
    "        return (predictions_list, pcd_list, id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if inference:\n",
    "    predictions_list, cloud_list, id_list = model_inference(predictor.eval(), testDataLoader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(model, loader, device, criterion):\n",
    "    losses = []\n",
    "    predictor = model.eval()\n",
    "    cloud_list = []\n",
    "    label_list = []\n",
    "    output_list = []\n",
    "    predictions_list = []\n",
    "    inputs_list = []\n",
    "    id_list = []\n",
    "    parameter_id = 0\n",
    "    tot = 0\n",
    "    count = 0\n",
    "    \n",
    "    for j, data  in tqdm(enumerate(loader), total=len(loader)):\n",
    "        inputs, labels, ids = data['pointcloud'].to(device).float(), data['properties'].to(device), data['id'].to(device)\n",
    "        points, target, ids = data['pointcloud'].to(device).float(), data['properties'].to(device), data['id'].to(device)\n",
    "        points = points.transpose(2, 1)\n",
    "        outputs, _ = predictor(points)\n",
    "        outputs = outputs.to(torch.device('cpu'))\n",
    "        inputs = points.to(torch.device('cpu'))\n",
    "        labels = target.to(torch.device('cpu'))\n",
    "        ids = ids.to(torch.device('cpu'))\n",
    "        #print(data['pointcloud'].size(), labels.size(), outputs.size())\n",
    "\n",
    "        for i in range(outputs.size(0)):\n",
    "            label_list.append(labels[i][parameter_id].item())\n",
    "            id_list.append(ids[i].item())\n",
    "            output_list.append(outputs[i][parameter_id].item())\n",
    "            predictions_list.append(outputs[i].numpy())\n",
    "            inputs_list.append(labels[i].numpy())\n",
    "            cloud_list.append(inputs[i].numpy())\n",
    "            ratio = ((labels[i][parameter_id]-outputs[i][parameter_id])/labels[i][parameter_id]).item()\n",
    "            #print('r', i+count, ids[i].item(), labels[i][parameter_id].item(), outputs[i][parameter_id].item(), ratio)\n",
    "            tot += np.absolute(ratio)\n",
    "            #print('l', labels[i][1].item(), outputs[i][1].item(), ((labels[i][1]-outputs[i][1])/labels[i][1]).item())\n",
    "        \n",
    "        count += outputs.size(0)\n",
    "    print(tot/count)\n",
    "\n",
    "    return predictions_list, inputs_list, label_list, output_list, id_list, cloud_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not inference:\n",
    "    with torch.no_grad():\n",
    "        predictions_list, inputs_list, label_list, output_list, id_list, cloud_list = test(predictor.eval(), testDataLoader, device, test_criterion)\n",
    "\n",
    "    print(len(predictions_list), len(inputs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 5\n",
    "# j = 4\n",
    "# direction = False\n",
    "# use_direction = True\n",
    "# square_error = True\n",
    "# error_calc(predictions_list, inputs_list, k, j, direction, use_direction, square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not inference:\n",
    "    label_list, output_list, id_list = np.array(label_list), np.array(output_list), np.array(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ratio = np.absolute((label_list - output_list)/label_list)\n",
    "# ratio_ind = ratio.argsort()\n",
    "# id_list = id_list[ratio_ind]\n",
    "# print(id_list[-10:-1])\n",
    "\n",
    "# error_threshold = 0.1\n",
    "# correct = ratio[np.where(ratio < error_threshold)]\n",
    "# print(len(ratio), len(correct), len(correct)/len(ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud_id = 2\n",
    "# pcd_id = 24229  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visually analyse predictions and Fine tune with ICP, calculate chamfer distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scaling up and down is required for icp calculations\n",
    "def chamfer_evaluate(predictions_list, cloud_list, id_list, cat, blueprint,  ifcConvert_executable,\n",
    "                     cloudCompare_executable, temp_dir, target_dir, sample_size,\n",
    "                     threshold, icp_correction = False):\n",
    "\n",
    "    preds_list, pcd_list = [], []\n",
    "    error_count = 0\n",
    "\n",
    "    # get predictions and pcds\n",
    "    for i in tqdm(range(len(predictions_list))):\n",
    "    #for i in tqdm(range(50)):\n",
    "        pcd_id = id_list[i].item()\n",
    "        pcd, preds = cloud_list[i].transpose(1, 0), copy.deepcopy(predictions_list[i])\n",
    "        #print(preds, inputs_list[i])\n",
    "\n",
    "        preds = scale_preds(preds.tolist(), cat)\n",
    "        #pcd, preds = prepare_visualisation(pcd_id, cat, i, cloud_list, predictions_list, path, ext)\n",
    "\n",
    "        try:\n",
    "            if  icp_correction:\n",
    "                # note: preds are updated in place during ICP\n",
    "                _, _ = icp_finetuning(o3d.utility.Vector3dVector(pcd), pcd_id, cat, preds, blueprint, temp_dir, target_dir, \n",
    "                                     ifcConvert_executable, cloudCompare_executable, sample_size, threshold, False)\n",
    "\n",
    "            preds_list.append(preds)\n",
    "            pcd_list.append(pcd)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"ICP error\", pcd_id, e)\n",
    "            error_count += 1\n",
    "\n",
    "    # calculate chamfer distances\n",
    "    cuda = torch.device('cuda')    \n",
    "    rescaled_preds = [scale_preds(preds, cat, up=0) for preds in preds_list]\n",
    "    preds_t = torch.tensor(rescaled_preds, requires_grad=True, device=cuda)\n",
    "    cloud_t = torch.tensor(cloud_list, device=cuda)\n",
    "    \n",
    "    chamfer_dists = get_chamfer_loss_tensor(preds_t, cloud_t, cat, reduce=False, alpha=3.)\n",
    "    chamfer_dists = chamfer_dists.detach().cpu().numpy()\n",
    "    \n",
    "#     for i, preds in enumerate(tqdm(preds_list)):\n",
    "#         preds = scale_preds(preds, cat, up=0)\n",
    "#         chamfer_distance, _ = get_chamfer_dist_single(pcd_list[i], preds, cat)\n",
    "#         chamfer_dists.append(chamfer_distance)\n",
    "        \n",
    "\n",
    "    \n",
    "    print(\"error_count\", error_count)\n",
    "    return chamfer_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dists = chamfer_evaluate(predictions_list, cloud_list, id_list, cat, blueprint,  ifcConvert_executable,\n",
    "                     cloudCompare_executable, temp_dir, target_dir, sample_size, threshold, icp_correction = False)\n",
    "# if inference:\n",
    "#     with open(model_path + 'preds_' + cat + '.pkl', 'wb') as f:\n",
    "#         pickle.dump([predictions_list, id_list, dists], f)\n",
    "    \n",
    "plot_error_graph(dists, \"Binned chamfer loss\", max_val=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scaling up and down is required for icp calculations\n",
    "def _visualise_predictions(predictions_list, cloud_list, id_list, cat, blueprint,  ifcConvert_executable,\n",
    "                     cloudCompare_executable, temp_dir, target_dir, sample_size,\n",
    "                     threshold, icp_correction = False):\n",
    "\n",
    "    preds_list, pcd_list = [], []\n",
    "    viewer_list, ifc_list = [], []\n",
    "    error_count = 0\n",
    "\n",
    "    # get predictions and pcds\n",
    "    #for i in tqdm(range(len(predictions_list))):\n",
    "    for i in tqdm(range(20)):\n",
    "        pcd_id = id_list[i].item()\n",
    "        pcd, preds = cloud_list[i].transpose(1, 0).tolist(), copy.deepcopy(predictions_list[i])\n",
    "        #print(preds, inputs_list[i])\n",
    "\n",
    "        preds = scale_preds(preds.tolist(), cat)\n",
    "        print(preds)\n",
    "        #pcd, preds = prepare_visualisation(pcd_id, cat, i, cloud_list, inputs_list, ext)\n",
    "\n",
    "#         try:\n",
    "        if  icp_correction:\n",
    "            # note: preds are updated in place during ICP\n",
    "            viewer, ifc = icp_finetuning(o3d.utility.Vector3dVector(pcd), pcd_id, cat, preds, blueprint, temp_dir, target_dir, \n",
    "                                 ifcConvert_executable, cloudCompare_executable, sample_size, threshold, True)\n",
    "        else:\n",
    "            #print(preds)\n",
    "            viewer, ifc = visualize_predictions([pcd], cat, [preds], blueprint, visualize=True)\n",
    "\n",
    "\n",
    "        preds_list.append(preds)\n",
    "        pcd_list.append(pcd)\n",
    "        viewer_list.append(viewer)\n",
    "        ifc_list.append(ifc)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(\"ICP error\", pcd_id, e)\n",
    "#             error_count += 1\n",
    "\n",
    "\n",
    "    print(\"error_count\", error_count)    \n",
    "    return viewer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viewers = _visualise_predictions(predictions_list, cloud_list, id_list, cat, blueprint,  ifcConvert_executable,\n",
    "                     cloudCompare_executable, temp_dir, target_dir, sample_size, threshold, icp_correction = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in viewers:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# icp_correction = False\n",
    "# viewers, ifcs = [], []\n",
    "# preds_list, pcd_list = [], []\n",
    "\n",
    "# for i in range(1,4):\n",
    "#     pcd_id = id_list[i].item()\n",
    "#     #print(pcd_id) \n",
    "        \n",
    "#     #print(\"p\", predictions_list[i], \"in\", inputs_list[i])\n",
    "\n",
    "# #     #pcd, preds = prepare_visualisation(pcd_id, cat, i, cloud_list, predictions_list, path, ext)\n",
    "# #     pcd, preds = cloud_list[i].transpose(1, 0), copy.deepcopy(predictions_list[i])\n",
    "# #     print(pcd.shape)\n",
    "# #     preds = scale_preds(preds.tolist(), cat)\n",
    "# # #     pcd2, input_preds = prepare_visualisation(pcd_id, cat, i, cloud_list, inputs_list, path, ext)\n",
    "# # #     indices_to_replace = [5, 6, 7]\n",
    "# # #     for i in indices_to_replace:\n",
    "# # #         preds[i] = input_preds[i]\n",
    "    \n",
    "# #     #try:\n",
    "# #     if not icp_correction:\n",
    "# #         viewer, ifc = visualize_predictions([pcd], cat, [preds], blueprint, visualize=True)\n",
    "\n",
    "# #     else:\n",
    "# #         viewer, ifc = icp_finetuning(pcd, pcd_id, cat, preds, blueprint, temp_dir, target_dir, \n",
    "# #                              ifcConvert_executable, cloudCompare_executable, sample_size, threshold)\n",
    "\n",
    "# #     viewers.append(viewer) \n",
    "# #     ifcs.append(ifc)\n",
    "# #     preds_list.append(preds)\n",
    "# #     pcd_list.append(pcd)\n",
    "       \n",
    "# # #     except Exception as e:\n",
    "# # #         print(\"ICP error\", pcd_id, e)\n",
    "        \n",
    "#     preds_list.append(inputs_list[i])\n",
    "#     pcd_list.append(cloud_list[i].transpose(1, 0))\n",
    "\n",
    "# cloud = o3d.geometry.PointCloud()\n",
    "# chamfer_dists = []\n",
    "# for i, preds in enumerate(preds_list):\n",
    "#     pcd = pcd_list[i]\n",
    "#     chamfer_distance, cl = get_chamfer_dist_single(pcd, preds, cat)\n",
    "#     chamfer_dists.append(chamfer_distance)\n",
    "    \n",
    "# print(\"chamfer_distance\", sum(chamfer_dists)/len(chamfer_dists))\n",
    "# print(\"chamfer_distance\", chamfer_dists)\n",
    "# cloud.points = o3d.utility.Vector3dVector(np.array(cl))\n",
    "# o3d.io.write_point_cloud(\"generated.pcd\", cloud)\n",
    "# cloud.points = o3d.utility.Vector3dVector(pcd)\n",
    "# o3d.io.write_point_cloud(\"pcd.pcd\", cloud)\n",
    "\n",
    "\n",
    "# #tensor based chamfer dist\n",
    "# preds_tensor = torch.tensor(preds_list).cuda()\n",
    "# pcd_tensor = torch.transpose(torch.tensor(pcd_list).cuda().float(), 1,2)\n",
    "# #print(pcd_tensor.shape)\n",
    "# chamfer_distance = get_chamfer_loss_tensor(preds_tensor, pcd_tensor)\n",
    "# print(\"chamfer\", chamfer_distance)\n",
    "\n",
    "\n",
    "# # for i, cl in enumerate(clouds_tensor.detach().cpu().numpy()):\n",
    "# #     cloud.points = o3d.utility.Vector3dVector(cl)\n",
    "# #     o3d.io.write_point_cloud(\"axis_\"+str(i)+\".pcd\", cloud)\n",
    "# #     cloud.points = pcd_list[i]\n",
    "# #     o3d.io.write_point_cloud(\"pcd_\"+str(i)+\".pcd\", cloud)\n",
    "    \n",
    "# # cloud = generate_elbow_cloud(preds)\n",
    "    \n",
    "# #chamfer_distance = get_chamfer_dist_single(cloud, pcd)\n",
    "# #print(\"chamfer_distance\", chamfer_distance)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_errors(inputs_list, predictions_list, cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BP data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_visualise(model_path, blueprint, path, ext, device, ifc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merge_clouds(path, 'pipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "#testset_PCA(cloud_list, inputs_list, testDataLoader, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = inputs_list[:512]\n",
    "# #preds = scale_preds(preds, cat, up=0)\n",
    "# pcd = cloud_list[1].transpose(1, 0)\n",
    "# print(preds)\n",
    "\n",
    "# preds_tensor = torch.tensor(np.array(preds)).cuda().float()\n",
    "# gen_cloud = generate_tee_cloud_tensor(preds_tensor)\n",
    "# print(gen_cloud.shape)\n",
    "# #gen_cloud = generate_pipe_cloud(preds)\n",
    "\n",
    "# points = gen_cloud.detach().cpu().numpy()\n",
    "# #points = np.array(gen_cloud)\n",
    "# cloud = o3d.geometry.PointCloud()\n",
    "# print(type(points), points.shape)\n",
    "# cloud.points = o3d.utility.Vector3dVector(points[0])\n",
    "# #cloud.points = o3d.utility.Vector3dVector(points)\n",
    "# o3d.io.write_point_cloud(\"pipe_gen.pcd\", cloud)\n",
    "\n",
    "# cloud.points = o3d.utility.Vector3dVector(pcd)\n",
    "# o3d.io.write_point_cloud(\"pipe_input.pcd\", cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# chamfer loss\n",
    "#v, loss = chamfer_fine_tune(50, 0.0001, predictions_list[0], cloud_list[0], cat, blueprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi element Adam with single loss\n",
    "def chamfer_fine_tune(n_iter, step_size, preds, cloud, cat, blueprint, alpha=1.0, visualise=True):\n",
    "    # prepare data on gpu and setup optimiser\n",
    "    cuda = torch.device('cuda')\n",
    "    preds_copy = copy.deepcopy(preds)\n",
    "    scaled_original_preds = [scale_preds(pc.tolist(), cat) for pc in preds_copy]\n",
    "    preds_t = torch.tensor(preds, requires_grad=True, device=cuda)\n",
    "    cloud_t = torch.tensor(cloud, device=cuda)\n",
    "    optimiser = torch.optim.Adam([preds_t], lr=step_size*3, betas=(0.7, 0.9) ,foreach=True)\n",
    "\n",
    "    # check initial loss\n",
    "    chamfer_loss = get_chamfer_loss_tensor(preds_t, cloud_t, cat, reduce=False, alpha=alpha)\n",
    "    print(\"intial loss\", chamfer_loss)   \n",
    "\n",
    "    # iterative refinement with adam\n",
    "    for i in tqdm(range (n_iter)):\n",
    "        optimiser.zero_grad()\n",
    "        chamfer_loss = get_chamfer_loss_tensor(preds_t, cloud_t, cat, alpha=alpha)\n",
    "        chamfer_loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        print(i, \"loss\", chamfer_loss.detach().cpu().numpy())#, \"preds\", preds_t)\n",
    "        \n",
    "    # check final loss\n",
    "    chamfer_loss = get_chamfer_loss_tensor(preds_t, cloud_t, cat, reduce=False, alpha=alpha)\n",
    "    print(\"final loss\", chamfer_loss)\n",
    "    modified_preds = preds_t.detach().cpu().numpy()\n",
    "    \n",
    "    # visualise\n",
    "    if visualise:\n",
    "        error_count = 0\n",
    "        scaled_preds = [scale_preds(p.tolist(), cat) for p in modified_preds]\n",
    "        visualisers = []\n",
    "        \n",
    "        for i, p in enumerate(scaled_preds):\n",
    "            try:\n",
    "                v_orignal, _ = visualize_predictions([cloud[i].transpose(1,0).tolist()], cat, [scaled_original_preds[i]], blueprint, visualize=True)\n",
    "                v_modified, _ = visualize_predictions([None, None, cloud[i].transpose(1,0).tolist()], cat, [scaled_preds[i]], blueprint, visualize=True)\n",
    "                visualisers.append(v_orignal)\n",
    "                visualisers.append(v_modified)\n",
    "            except:\n",
    "                error_count += 1\n",
    "    \n",
    "#         return v_orignal,v_modified, modified_preds\n",
    "        print(\"errors \", error_count)\n",
    "        return visualisers, modified_preds\n",
    "    else:\n",
    "        return modified_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictions_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#limit = 20\n",
    "limit = len(predictions_list)\n",
    "\n",
    "#v, modified_preds = chamfer_fine_tune(50, 0.01, predictions_list[:limit], cloud_list[:limit], cat, blueprint, alpha=3., visualise=True)\n",
    "modified_preds = chamfer_fine_tune(50, 0.01, predictions_list[:limit], cloud_list[:limit],\n",
    "                                   cat, blueprint, alpha=3., visualise=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_parameter_errors(inputs_list, modified_preds, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dists = chamfer_evaluate(modified_preds, cloud_list[:limit], id_list, cat, blueprint,  ifcConvert_executable,\n",
    "                     cloudCompare_executable, temp_dir, target_dir, sample_size, threshold, icp_correction = False)\n",
    "\n",
    "if inference:\n",
    "    with open(model_path/('preds_basin_finetuned_' + cat + '.pkl'), 'wb') as f:\n",
    "        pickle.dump([modified_preds, id_list, dists], f)\n",
    "\n",
    "plot_error_graph(dists, \"Binned chamfer loss\", max_val=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "warn: hacky fix\n",
    "the termination condition of most methods like newton-CG is based on no. of iterations.\n",
    "But there can be an uncapped number of gradient computations inside a single iteration.\n",
    "This can make the optimisation stuck. \n",
    "This method counts function evaluations within the loss function call itself using a \n",
    "global variable, and terminates when a threshold of gradient evaluations are reached.\n",
    "the callback is unsuitable for this since its only called after each iteration.\n",
    "\"\"\"\n",
    "\n",
    "# simulated annealing optimisation\n",
    "from scipy.optimize import basinhopping, minimize\n",
    "def c_loss(preds):\n",
    "    #print(i)\n",
    "    global eval_count\n",
    "    global results\n",
    "    eval_count += 1\n",
    "    #print(eval_count)\n",
    "    \n",
    "    # hacky method to stop optimizer iterations based on grad evaluations\n",
    "    if eval_count > 50:\n",
    "        results.append(preds)\n",
    "        raise StopIteration(\"gradient evals exceeded\")\n",
    "    \n",
    "    cloud = cloud_list[i]\n",
    "    #cloud = cloud_list2[4]\n",
    "    cuda = torch.device('cuda')\n",
    "    #cloud = cloud.transpose(1,0)\n",
    "    preds_copy = copy.deepcopy(preds)\n",
    "    scaled_original_preds = scale_preds(preds_copy.tolist(), cat)\n",
    "    preds_t = torch.tensor([preds], requires_grad=True, device=cuda, dtype=torch.float)\n",
    "    #print(preds_t.grad)\n",
    "    cloud_t = torch.tensor([cloud], device=cuda)\n",
    "    chamfer_loss = get_chamfer_loss_tensor(preds_t, cloud_t, cat, alpha=3., reduce=True)\n",
    "    chamfer_loss.backward()\n",
    "    #print(\"ch\", chamfer_loss.item())\n",
    "    #print(preds_t.grad.detach().cpu().numpy()[0])\n",
    "    return chamfer_loss.item(), preds_t.grad.detach().cpu().numpy()[0]\n",
    "    #return chamfer_loss.item()\n",
    "   \n",
    "    \n",
    "def basinhopping_single(n_iter, step_size, preds, cloud, cat, blueprint):\n",
    "    minimizer_kwargs = {\"method\": \"CG\", \"jac\":True}\n",
    "    #minimizer_kwargs = {\"method\": \"BFGS\"}\n",
    "    ret = basinhopping(c_loss, preds, stepsize=step_size, minimizer_kwargs=minimizer_kwargs, \n",
    "                       niter=n_iter)\n",
    "    print(\"global minimum: \", ret)\n",
    "    return ret\n",
    "    \n",
    "    # visualise\n",
    "    # modified_preds = preds_t.detach().cpu().numpy()[0].tolist()\n",
    "    # scaled_preds = scale_preds(modified_preds, cat)\n",
    "    # v_orignal, _ = visualize_predictions([cloud.transpose(1,0).tolist()], cat, [scaled_original_preds], blueprint, visualize=True)\n",
    "    # v_modified, _ = visualize_predictions([cloud.transpose(1,0).tolist()], cat, [scaled_preds], blueprint, visualize=True)\n",
    "    \n",
    "    #return [v_orignal,v_modified], chamfer_loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_fn(intermediate_result):\n",
    "    #print(intermediate_result[\"x\"], \"ch\", intermediate_result[\"fun\"])\n",
    "    print(intermediate_result)\n",
    "    raise StopIteration(\"gradient evals exceeded\")\n",
    "    \n",
    "    \n",
    "def minimize_single(n_iter, preds, step_size):\n",
    "    ret = minimize(c_loss, preds, method = \"Newton-CG\", jac = True, \n",
    "                   options = {\"maxiter\":n_iter, \"eps\":step_size, \"disp\":False})\n",
    "    print(\"min reached\")\n",
    "    results.append(ret[\"x\"])\n",
    "#     ret = minimize(c_loss, preds, method = \"TNC\", jac = True, \n",
    "#                    options = {\"maxfun\":n_iter, \"eps\":step_size})\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictions_list))\n",
    "evaluation_limit = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#c_loss(2, 0.01, predictions_list[0], cloud_list[0], cat, blueprint)\n",
    "results = []\n",
    "for i in tqdm(range(evaluation_limit)):\n",
    "    eval_count = 0\n",
    "    try:\n",
    "        minimize_single(25, predictions_list[i], 0.0001)\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modified_preds = chamfer_fine_tune(10, 0.01, predictions_list[:evaluation_limit], \n",
    "                                   cloud_list[:evaluation_limit], cat, blueprint, \n",
    "                                   alpha=3., visualise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified_preds = []\n",
    "# for i in range(evaluation_limit):\n",
    "#     modified_pred = chamfer_fine_tune(10, 0.01, [predictions_list[i]], \n",
    "#                                    [cloud_list[i]], cat, blueprint, \n",
    "#                                    alpha=3., visualise=False)\n",
    "#     modified_preds.append(modified_pred[0])\n",
    "# modified_preds = np.array(modified_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = chamfer_evaluate(predictions_list[:evaluation_limit], cloud_list[:evaluation_limit], id_list, cat, \n",
    "                         blueprint,  ifcConvert_executable, cloudCompare_executable, \n",
    "                         temp_dir, target_dir, sample_size, threshold, icp_correction = False)\n",
    "\n",
    "plot_error_graph(dists, \"Binned chamfer loss\", max_val=300)\n",
    "print(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = chamfer_evaluate(modified_preds, cloud_list[:evaluation_limit], id_list, cat, \n",
    "                         blueprint,  ifcConvert_executable, cloudCompare_executable, \n",
    "                         temp_dir, target_dir, sample_size, threshold, icp_correction = False)\n",
    "\n",
    "plot_error_graph(dists, \"Binned chamfer loss\", max_val=300)\n",
    "print(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = chamfer_evaluate(results, cloud_list[:evaluation_limit], id_list, cat, \n",
    "                         blueprint,  ifcConvert_executable, cloudCompare_executable, \n",
    "                         temp_dir, target_dir, sample_size, threshold, icp_correction = False)\n",
    "\n",
    "plot_error_graph(dists, \"Binned chamfer loss\", max_val=300)\n",
    "print(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_errors(inputs_list[:evaluation_limit], results, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# second round of fine tuning\n",
    "print(sum(dists)/len(dists))\n",
    "threshold = 0.02\n",
    "start_point = int(threshold*len(dists))\n",
    "print(start_point)\n",
    "\n",
    "indices = dists.argsort()\n",
    "#print(dists[indices][-1*start_point:])\n",
    "mod_pred2 = modified_preds[indices][-1*start_point:]\n",
    "cloud_list2 = np.array(cloud_list)[indices][-1*start_point:]\n",
    "id_list2 = np.array(id_list)[indices][-1*start_point:]\n",
    "print(len(mod_pred2), max(dists))\n",
    "# modified_preds2 = chamfer_fine_tune(50, 0.001, mod_pred2, cloud_list2, cat, blueprint, alpha=3, visualise=False)\n",
    "# dists2 = chamfer_evaluate(modified_preds2, cloud_list2, id_list2, cat, blueprint,  ifcConvert_executable,\n",
    "#                      cloudCompare_executable, temp_dir, target_dir, sample_size, threshold, icp_correction = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists[indices][-1*start_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ret = basinhopping_single(1, 0.01, mod_pred2[4], cloud_list2[7], cat, blueprint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modified_preds = []\n",
    "funs = []\n",
    "for i in range(len(mod_pred2)):\n",
    "    ret = basinhopping_single(1, 0.01, mod_pred2[i], cloud_list2[i], cat, blueprint)\n",
    "    modified_preds.append(ret.x)\n",
    "    funs.append(ret.fun)\n",
    "print(len(modified_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('basinhopped_res.pkl', 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump([modified_preds, funs], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists2 = chamfer_evaluate(modified_preds, cloud_list2, id_list2, cat, blueprint,  ifcConvert_executable,\n",
    "                     cloudCompare_executable, temp_dir, target_dir, sample_size, threshold, icp_correction = False)\n",
    "dists2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = torch.tensor([predictions_list[1]]).cuda()\n",
    "print(predictions_list[1])\n",
    "pcd = generate_tee_cloud_tensor(preds)\n",
    "#tee = generate_tee_cloud(predictions_list[0])\n",
    "tee = pcd[0].cpu().numpy()\n",
    "tee = o3d.utility.Vector3dVector(tee)\n",
    "tee_cloud = o3d.geometry.PointCloud()\n",
    "tee_cloud.points = tee\n",
    "o3d.io.write_point_cloud(\"tee_cl.pcd\", tee_cloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original = cloud_list[2]\n",
    "points= o3d.utility.Vector3dVector(original.transpose(1,0))\n",
    "tee_cloud.points = points\n",
    "o3d.io.write_point_cloud(\"tee_cl_inp.pcd\", tee_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# undo normalisation opf the bp tee dataset created for inference, only for comparison with the inferred tee results\n",
    "tee_path = 'tee_fix/tee/test/'\n",
    "metadata_file = open(\"tee_fix/tee/metadata.json\", 'r')\n",
    "metadata = json.load(metadata_file)\n",
    "output_path = 'tee_fix/tee/unnormalised/'\n",
    "\n",
    "files = os.listdir(tee_path)\n",
    "new_points = []\n",
    "for f in tqdm(files):\n",
    "    cloud_data = metadata[f.split(\".\")[0]]\n",
    "    points = np.array(o3d.io.read_point_cloud(tee_path + f).points)\n",
    "    print(\"a\", points[0])\n",
    "    print(cloud_data[\"norm_factor\"], cloud_data[\"mean\"])\n",
    "    points *= cloud_data[\"norm_factor\"]\n",
    "    print(\"b\", points[0])\n",
    "\n",
    "    for i, pnt in enumerate(points):\n",
    "        pnt += cloud_data[\"mean\"]\n",
    "    print(\"c\", points[10])\n",
    "    new_points.append(points)\n",
    "        \n",
    "new_points = o3d.utility.Vector3dVector(np.concatenate(new_points))\n",
    "new_cloud = o3d.geometry.PointCloud()\n",
    "new_cloud.points = new_points\n",
    "o3d.io.write_point_cloud(output_path+\"tee_bp_unnormalised.pcd\", new_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesh deformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.finetune import chamfer_fine_tune_single\n",
    "modified_preds = []\n",
    "for i in range(len(mod_pred2)):\n",
    "    ft = chamfer_fine_tune(20, 0.01, [mod_pred2[i]], [cloud_list2[i]], cat, blueprint, alpha=3., visualise=False)\n",
    "    modified_preds.append(ft[0])\n",
    "print(len(modified_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modified_preds2 = chamfer_fine_tune(50, 0.02, modified_preds, cloud_list, cat, blueprint, alpha=3, visualise=False)\n",
    "dists2 = chamfer_evaluate(modified_preds2, cloud_list2, id_list2, cat, blueprint,  ifcConvert_executable,\n",
    "                     cloudCompare_executable, temp_dir, target_dir, sample_size, threshold, icp_correction = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret.x, ret.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
