{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpvL68OfBEQC"
   },
   "source": [
    "# Loss Function Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z7n_pw4SMWl"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ47VNF7fmTS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import os.path\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import interact \n",
    "import gc\n",
    "import open3d as o3d\n",
    "\n",
    "from src.elements import *\n",
    "from src.ifc import *\n",
    "from src.preparation import *\n",
    "from src.visualisation import *\n",
    "from src.chamfer import *\n",
    "from src.utils import *\n",
    "\n",
    "from src.morph import *\n",
    "\n",
    "random.seed = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains experiments on loss functions.\n",
    "\n",
    "Specifically it contains;\n",
    "\n",
    "1. Analysis of results from sphere morphing, including consistency metrics\n",
    "2. Visualising point-wise distances from a loss function\n",
    "3. Measuring loss correlation in results from completion models\n",
    "4. Visualising loss curves for model training\n",
    "\n",
    "#### data loading and pre-processing\n",
    "\n",
    "All data from sphere morphing must be loaded for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper code to recombine batched output from balancedCD\n",
    "file_prefix = \"sphere/balanced_metrics\"\n",
    "icd_chamfer_list, icd_emd_list, icd_assignment_list = [], [], []\n",
    "\n",
    "for i in range(1,9):\n",
    "    with open(file_prefix + str(i) + \".pkl\", \"rb\") as f:\n",
    "        chamfer, emd, ass = pickle.load(f)\n",
    "        print(len(ass), len(ass[0]), len(ass[0][0]), ass[0][0][0].shape)\n",
    "        icd_chamfer_list.append(chamfer)\n",
    "        icd_emd_list.append(emd)\n",
    "        icd_assignment_list.append(ass[0])\n",
    "\n",
    "icd_assignment_list = np.array(icd_assignment_list)        \n",
    "icd_assignment_list = np.transpose(icd_assignment_list, axes=(1,2,0,3,4))\n",
    "icd_assignment_list = np.reshape(icd_assignment_list, (101, 2, 8*150, 4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_cd = np.sum(np.array(icd_chamfer_list), axis=0)\n",
    "balanced_emd = np.sum(np.array(icd_emd_list), axis=0)\n",
    "print(balanced_emd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metrics\n",
    "loss_funcs = [\"emd\"]\n",
    "loss_funcs = [\"emd\",  \"cyclic\"]\n",
    "chamfer_list, emd_list = [], []\n",
    "for loss_func in loss_funcs:\n",
    "    if loss_func == \"balanced\":\n",
    "        continue\n",
    "    with open(\"sphere/\" + loss_func + \"_metrics.pkl\", \"rb\") as f:\n",
    "        chamfer, emd, ass = pickle.load(f)\n",
    "        chamfer_list.append(chamfer)\n",
    "        emd_list.append(emd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"balanced\" in loss_funcs:\n",
    "    chamfer_list.append(balanced_cd)\n",
    "    emd_list.append(balanced_emd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rc('xtick', labelsize=14) \n",
    "plt.rc('ytick', labelsize=14) \n",
    "#loss_funcs = [\"infoCD\", \"CD\", \"EMD\", \"UniformCD\"]\n",
    "\n",
    "plot_dists(axes[1], chamfer_list, loss_funcs, \"Chamfer Distance\", \"iterations\", log=True)\n",
    "plot_dists(axes[0], emd_list, loss_funcs, \"Earth Mover's Distance\", \"iterations\", log=True)\n",
    "#print(\"EMD infocd\", emd_list[0][-1], \"chamfer\", emd_list[1][-1],  \"emd\", emd_list[2][-1], \"BALANCED\", emd_list[3][-1])\n",
    "#print(\"CD infocd\", chamfer_list[0][-1], \"chamfer\", chamfer_list[1][-1], \"emd\", chamfer_list[2][-1], \"balanced\", chamfer_list[3][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ass), len(ass[0]), len(ass[0][0]), len(ass[0][0][0]), ass[0][0][0][0].shape)\n",
    "#print(len(ass), len(ass[0]), ass[0][0].shape, len(ass[0][0]), ass[0][0][0].shape)\n",
    "print(emd_list[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load losses\n",
    "# single, not batch\n",
    "loss_types = [\"reverse\", \"chamfer\", \"emd\", \"pair\"]\n",
    "losses = []\n",
    "\n",
    "for loss_func in loss_types:\n",
    "    with open(\"sphere/loss_\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "        losses.append(pickle.load(f))\n",
    "        \n",
    "plot_dists(losses, loss_types, \"loss function comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### consistency metrics\n",
    "\n",
    "We measure;\n",
    "\n",
    "1. backward assignment consistency\n",
    "2. coorrespondence coverage\n",
    "3. correspondence stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure conssistency between forward and backward correspondences for chamfer distance\n",
    "# optionally compare against the ideal assignment, as measured by EMD\n",
    "# def measure_assignment_consistency(assignment, emd=None):\n",
    "#     reverse_assignment = torch.gather(assignment[0], 0, assignment[1])\n",
    "#     expected = torch.arange(assignment[0].shape[0], device=torch.device(\"cuda\"))\n",
    "#     consistency = torch.sum(torch.eq(expected, reverse_assignment).long())\n",
    "#     print(\"consistency\", consistency.item(), len(torch.unique(assignment[0])), len(torch.unique(assignment[1])))\n",
    "    \n",
    "#     if emd is not None:\n",
    "#         #print(emd[:5], assignment[0][:5], assignment[1][:5])\n",
    "#         emd_consistency = torch.sum(torch.eq(emd, assignment[0]).long())\n",
    "#         #print(\"emd_consistency\", emd_consistency.item(), len(torch.unique(emd)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure conssistency for EMD approx correspondences\n",
    "# this is different from other distances as only a one way assignment is returned\n",
    "def measure_batch_assignment_consistency_emd(assignment):\n",
    "    consistencies = np.array([4096 for i in range(assignment.shape[-1])])\n",
    "    cuda = torch.device(\"cuda\")\n",
    "\n",
    "    uniques = []\n",
    "    changes = []\n",
    "    for i in tqdm(range(len(assignment))):\n",
    "        class_assignment = torch.tensor(assignment[i], device=cuda)\n",
    "        \n",
    "        unique = 0\n",
    "        for j in range(len(class_assignment)):\n",
    "            unique += len(torch.unique(class_assignment[j]))\n",
    "        unique = (unique/len(class_assignment))\n",
    "        uniques.append(unique)\n",
    "        \n",
    "        # check rate of change of matches\n",
    "        if i==0:\n",
    "            change = 0\n",
    "        else:\n",
    "            change = (assignment[i] == assignment[i-1]).sum()\n",
    "            change = (change/len(class_assignment[0]))\n",
    "        changes.append(change)\n",
    "        \n",
    "        print(\"un\", unique, \"change\", change)\n",
    "        \n",
    "    # save results\n",
    "    with open(\"sphere/\" + \"emd\" + \"_consistency.pkl\", \"wb\") as f:\n",
    "        pickle.dump([consistencies, uniques, changes], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "measure_batch_assignment_consistency_emd(ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure consistency between forward and backward correspondences\n",
    "# measure number of unique assignments\n",
    "# measure rate of change of assignments\n",
    "def measure_batch_assignment_consistency(assignment, loss):\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    # loop through iterations\n",
    "    consistencies = []\n",
    "    uniques = []\n",
    "    changes = []\n",
    "    for i in tqdm(range(len(assignment))):\n",
    "        \n",
    "        # check reverse consistency\n",
    "        class_assignment = torch.tensor(assignment[i], device=cuda)\n",
    "        reverse_assignment = torch.gather(class_assignment[0], 1, class_assignment[1])\n",
    "        #print(\"r\", reverse_assignment.shape)\n",
    "        expected = torch.arange(class_assignment[0].shape[1], device=cuda)\n",
    "        expected = expected.repeat(class_assignment[0].shape[0], 1)\n",
    "        #print(\"e\", expected.shape)\n",
    "        consistency = torch.sum(torch.eq(expected, reverse_assignment).long())/len(class_assignment[0])\n",
    "        consistencies.append(consistency.item())\n",
    "        \n",
    "        # check uniqueness of matches\n",
    "        unique = 0\n",
    "        for j in range(len(class_assignment[0])):\n",
    "            unique += (len(torch.unique(class_assignment[0][j])) +\n",
    "                       len(torch.unique(class_assignment[1][j])))\n",
    "        unique = (unique/len(class_assignment[0]))/2\n",
    "        uniques.append(unique)\n",
    "        \n",
    "        # check rate of change of matches\n",
    "        if i==0:\n",
    "            change = 0\n",
    "        else:\n",
    "            change = (assignment[i] == assignment[i-1]).sum()\n",
    "            change = (change/len(class_assignment[0]))/2\n",
    "        changes.append(change)\n",
    "        \n",
    "        print(\"un\", unique, \"consistency\", consistency.item(), \"change\", change)\n",
    "\n",
    "    \n",
    "    # save results\n",
    "    with open(\"sphere/\" + loss + \"_consistency.pkl\", \"wb\") as f:\n",
    "        pickle.dump([consistencies, uniques, changes], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "loss = \"cyclic\"\n",
    "#ass = np.array(ass)\n",
    "\n",
    "if loss == \"infocd\":\n",
    "    ass = np.transpose(ass, axes=(1,2,0,3,4,5))\n",
    "    ass = np.reshape(ass, (101, 2, 8*150, 4096))\n",
    "    \n",
    "\n",
    "if loss == \"cyclic\":\n",
    "    ass = np.transpose(ass, axes=(1,2,0,3,4,))\n",
    "    ass = np.reshape(ass, (101, 2, 8*150, 4096))\n",
    "    \n",
    "if loss == \"chamfer\":\n",
    "    print(ass.shape)\n",
    "    ass = np.transpose(ass, axes=(1,2,0,3,4))\n",
    "    ass = np.reshape(ass, (101, 2, 8*150, 4096))\n",
    "    print(ass.shape)\n",
    "\n",
    "if loss == \"emd\":\n",
    "    ass = np.array(ass)\n",
    "    ass = np.transpose(ass, axes=(1,0,2,3))\n",
    "    ass = np.reshape(ass, (101, 8*150, 4096))\n",
    "print(ass.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#measure_batch_assignment_consistency(icd_assignment_list, loss=\"cyclic\")\n",
    "measure_batch_assignment_consistency(ass, loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot consistency metrics\n",
    "\n",
    "loss_funcs = [\"infocd\", \"chamfer\", \"emd\", \"UniformCD\", \"cyclic\"]\n",
    "unique_list, consistency_list, change_list = [], [], []\n",
    "for loss_func in loss_funcs:\n",
    "    with open(\"sphere/\" + loss_func + \"_consistency.pkl\", \"rb\") as f:\n",
    "        consistency, unique, change = pickle.load(f)\n",
    "        consistency_list.append(consistency)\n",
    "        unique_list.append(unique)\n",
    "        change_list.append(change)\n",
    "print(change_list[0][-1], change_list[1][-1])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 5))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rc('xtick', labelsize=14) \n",
    "plt.rc('ytick', labelsize=14) \n",
    "\n",
    "limit= 50\n",
    "loss_funcs = [\"InfoCD\", \"CD\", \"EMD\", \"UniformCD\", \"SCD\"]\n",
    "\n",
    "plot_dists(axes[0], consistency_list, loss_funcs, title=\"Backward consistency\", \n",
    "           ylabel=\"no. of points\", xlabel=\"Iterations\", log=False, limit=limit, legend=False)\n",
    "plot_dists(axes[1], unique_list, loss_funcs, title=\"Point coverage\", \n",
    "           ylabel=\"\", xlabel=\"Iterations\", log=False, limit=limit, legend=False)\n",
    "plot_dists(axes[2], change_list, loss_funcs, title=\"Correspondence variation\", \n",
    "           ylabel=\"\", xlabel=\"Iterations\", log=False, limit=limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare correspondences\n",
    "#TODO: include consistency in top5 matches?\n",
    "loss_func = \"emd\"\n",
    "with open(\"sphere/assignments_\" + \"emd\" + \".pkl\", \"rb\") as f:\n",
    "    emd_assignment = pickle.load(f)\n",
    "\n",
    "with open(\"sphere/assignments_\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "    assignment = pickle.load(f)\n",
    "    \n",
    "for i, ass in enumerate(assignment):\n",
    "    #measure_assignment_consistency(ass, emd_assignment[i][0])\n",
    "    measure_assignment_consistency(ass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point-wise correspondence distance visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clouds\n",
    "tgt_path = \"sphere/scaled_lamp4_su.pcd\"\n",
    "l1_path = \"sphere/scaled_lamp4_su2.pcd\"\n",
    "l2_path = \"sphere/scaled_lamp4_sr3.pcd\"\n",
    "l3_path = \"sphere/scaled_lamp4_sl2.pcd\"\n",
    "\n",
    "tgt = np.array(o3d.io.read_point_cloud(tgt_path).points)\n",
    "l1 = np.array(o3d.io.read_point_cloud(l1_path).points)\n",
    "l2 = np.array(o3d.io.read_point_cloud(l2_path).points)\n",
    "l3 = np.array(o3d.io.read_point_cloud(l3_path).points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure point-wise distance between two clouds\n",
    "def get_point_distance(src, tgt, loss=\"chamfer\"):\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    src_tensor = torch.tensor([src], device=cuda)\n",
    "    tgt_tensor = torch.tensor([tgt], device=cuda)\n",
    "    chamferDist = ChamferDistance()\n",
    "\n",
    "    if loss == \"chamfer\":\n",
    "        nn = chamferDist(\n",
    "            src_tensor, tgt_tensor, bidirectional=True, return_nn=True)\n",
    "        weights = nn[1].dists[0,:,0]\n",
    "        #print( torch.sum(nn[1].dists))\n",
    "        print(\"w\", weights.shape)\n",
    "        loss = torch.mean(nn[1].dists)\n",
    "        \n",
    "    elif loss == \"balanced\":\n",
    "        dist_0, dist_1 = calc_balanced_chamfer_loss_tensor(src_tensor, tgt_tensor, return_dists=True, k=32)\n",
    "        loss = torch.mean(dist_1[0])\n",
    "        #print( torch.sum(dist_1[0]))\n",
    "        weights = dist_1[0]\n",
    "    \n",
    "    return weights.detach().cpu().numpy(), loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a colour map based on the weights of a point cloud\n",
    "def visualise_point_loss(weights, high, low, colormap_name='plasma'):\n",
    "    # normalise weights\n",
    "    # weights = np.log(weights)\n",
    "    # high = np.log(high)\n",
    "    # low = np.log(low)\n",
    "    diff = high - low\n",
    "    weights = (weights - low) / diff\n",
    "    \n",
    "    # map colour\n",
    "    colours = np.zeros((len(weights), 4))\n",
    "    colormap = plt.get_cmap(colormap_name)\n",
    "    for j, pt in enumerate(weights):\n",
    "        colours[j] = colormap(pt)\n",
    "\n",
    "    return colours[:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def get_point_weights(tgt, l1, l2, l3, loss_func=\"chamfer\"):\n",
    "    w1, loss1 = get_point_distance(tgt, l1, loss_func)\n",
    "    w2, loss2 = get_point_distance(tgt, l2, loss_func)\n",
    "    w3, loss3 = get_point_distance(tgt, l3, loss_func)\n",
    "    \n",
    "\n",
    "    high1, low1 = np.max(w1), np.min(w1)\n",
    "    high2, low2 = np.max(w2), np.min(w2)\n",
    "    high3, low3 = np.max(w3), np.min(w3)\n",
    "\n",
    "    high = max(high1, high2, high3)\n",
    "    low = min(low1, low2, low3)\n",
    "    \n",
    "    losses = [loss1, loss2, loss3]\n",
    "    print(loss_func, \"loss1\", loss1, \"loss2\", loss2, \"loss3\", loss3)\n",
    "    \n",
    "    return high, low, w1, w2, w3, losses\n",
    "\n",
    "\n",
    "\n",
    "cd_high, cd_low, cd_w1, cd_w2, cd_w3, cd_losses = get_point_weights(tgt, l1, l2, l3, loss_func=\"chamfer\")\n",
    "balanced_high, balanced_low, balanced_w1, balanced_w2, balanced_w3, balanced_losses = get_point_weights(tgt, l1, l2, l3, loss_func=\"balanced\")\n",
    "\n",
    "high, low = max(cd_high, balanced_high), min(cd_low, balanced_low)\n",
    "\n",
    "#colours = visualise_point_loss(balanced_w1, high, low, 'plasma_r')\n",
    "#colours = visualise_point_loss(balanced_w4, balanced_high, balanced_low, 'plasma_r')\n",
    "colours = visualise_point_loss(cd_w3, high, low, 'plasma_r')\n",
    "print(\"colours\", (colours!=0).sum(), colours.shape)\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "#print(colours, cloud.shape)\n",
    "point_cloud.points = o3d.utility.Vector3dVector(l3)\n",
    "point_cloud.colors = o3d.utility.Vector3dVector(colours)\n",
    "o3d.visualization.draw_geometries([point_cloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud = o3d.geometry.PointCloud()\n",
    "#print(colours, cloud.shape)\n",
    "point_cloud.points = o3d.utility.Vector3dVector(tgt)\n",
    "point_cloud.paint_uniform_color([0.1, 0.8, 0.6])\n",
    "\n",
    "o3d.visualization.draw_geometries([point_cloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(normalize([balanced_losses]))\n",
    "X = np.arange(3)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "#ax.bar(X + 0.00, normalize([balanced_losses])[0], color = 'b', width = 0.25)\n",
    "ax.bar(X + 0.25, normalize([cd_losses])[0], color = 'mediumslateblue', width = 0.8)\n",
    "ax.set(ylabel=\"Loss (normalised)\")\n",
    "ax.set(title=\"Chamfer distance\")\n",
    "\n",
    "x1,x2,y1,y2 = plt.axis()  \n",
    "plt.axis((x1,x2,0,0.7))\n",
    "ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(3)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.rc('xtick', labelsize=10) \n",
    "#ax.bar(X + 0.00, normalize([balanced_losses])[0], color = 'b', width = 0.25)\n",
    "ax.bar(X + 0.25, normalize([balanced_losses])[0], color = 'indianred', width = 0.8)\n",
    "ax.set(ylabel=\"Loss (normalised)\")\n",
    "ax.set(title=\"UniformCD distance\")\n",
    "x1,x2,y1,y2 = plt.axis()  \n",
    "plt.axis((x1,x2,0,0.7))\n",
    "ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CD EMD scatterplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on PCN results\n",
    "dataset = \"mvp\"\n",
    "\n",
    "if dataset == \"mvp\":\n",
    "    chamfer_path = \"../experiments/Density_aware_Chamfer_Distance/out_metrics_cd.pkl\" #MVP \n",
    "    balanced_path = \"../experiments/Density_aware_Chamfer_Distance/out_metrics_balanced.pkl\" #MVP \n",
    "else:\n",
    "    chamfer_path = \"sphere/out_metrics_cd.pkl\" #PCN\n",
    "    balanced_path = \"sphere/out_metrics.pkl\" #PCN\n",
    "\n",
    "with open (chamfer_path, \"rb\") as f:\n",
    "    chamfer_cd, chamfer_emd = pickle.load(f)\n",
    "    \n",
    "if dataset == \"mvp\":\n",
    "    \n",
    "    chamfer_cd, chamfer_emd = [x.detach().cpu().numpy() for x in chamfer_cd], [x.detach().cpu().numpy() for x in chamfer_emd]\n",
    "    chamfer_cd, chamfer_emd = np.array(chamfer_cd).flatten(), np.array(chamfer_emd).flatten()\n",
    "    chamfer_cd = chamfer_cd*1000\n",
    "    chamfer_emd = chamfer_emd*100\n",
    "else:\n",
    "    chamfer_cd = np.array(chamfer_cd)\n",
    "    chamfer_emd = np.array(chamfer_emd)*100\n",
    "\n",
    "with open (balanced_path, \"rb\") as f:\n",
    "    balanced_cd, balanced_emd = pickle.load(f)\n",
    "    \n",
    "if dataset == \"mvp\":\n",
    "    balanced_cd, balanced_emd = [x.detach().cpu().numpy() for x in balanced_cd], [x.detach().cpu().numpy() for x in balanced_emd]\n",
    "    balanced_cd, balanced_emd = np.array(balanced_cd).flatten(), np.array(balanced_emd).flatten()\n",
    "    balanced_cd = balanced_cd*1000\n",
    "    balanced_emd = balanced_emd*100\n",
    "else:\n",
    "    balanced_cd = np.array(balanced_cd)\n",
    "    balanced_emd = np.array(balanced_emd)*100\n",
    "    \n",
    "    \n",
    "print(np.average(balanced_cd), np.average(chamfer_cd), np.average(balanced_emd), np.average(chamfer_emd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize']=(10,6)\n",
    "plt.xlabel('CD (x$10^4$)', fontsize=20)\n",
    "plt.ylabel('EMD (x$10^2$)', fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.title('MVP Testset Correlation between CD and EMD, VRC model', fontsize=20)\n",
    "\n",
    "plt.scatter(balanced_cd, balanced_emd, s=3, color=\"lightseagreen\", label=\"UniformCD loss\")\n",
    "plt.scatter(chamfer_cd, chamfer_emd, s=3, color=\"palevioletred\", label=\"CD loss\")\n",
    "plt.legend(prop = { \"size\": 18 })\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find correlation between chamfer and EMD\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "print(\"balanced r\", scipy.stats.pearsonr(balanced_cd, balanced_emd))\n",
    "print(\"chamfer r\", scipy.stats.pearsonr(chamfer_cd, chamfer_emd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise loss curves for training VCN models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open train log and get eval records\n",
    "train_log_file = \"../experiments/Density_aware_Chamfer_Distance/log/vrcnet_plus_cd_debug_2024-03-11T17:00:03/train - Copy.log\"\n",
    "def get_metrics_from_log(train_log_file, balanced=False):\n",
    "    with open(train_log_file, \"rb\") as f:\n",
    "        train_log = f.readlines()\n",
    "        \n",
    "    eval_lines = []\n",
    "    for line in train_log:\n",
    "        if \"curr\" in str(line):\n",
    "            eval_lines.append(line)\n",
    "            \n",
    "            \n",
    "    # get results\n",
    "    dcd, cd, emd, bcd = [], [] ,[], []\n",
    "\n",
    "    for line in eval_lines:\n",
    "        line = str(line)\n",
    "        dcd.append(float(line.split(\"dcd: \")[1].split(\";\")[0]))\n",
    "        cd.append(float(line.split(\"cd_t: \")[1].split(\";\")[0]))\n",
    "        emd.append(float(line.split(\"emd: \")[1].split(\";\")[0]))\n",
    "        if balanced:\n",
    "            bcd.append(float(line.split(\"bcd_t: \")[1].split(\";\")[0]))\n",
    "\n",
    "    if balanced:\n",
    "        return dcd, cd, emd, bcd\n",
    "    return dcd, cd, emd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_file = \"../experiments/Density_aware_Chamfer_Distance/log/vrcnet_plus_cd_debug_2024-03-11T17:00:03/train - Copy.log\"\n",
    "dcd_cd, cd_cd, emd_cd  = get_metrics_from_log(train_log_file)\n",
    "\n",
    "train_log_file = \"../experiments/Density_aware_Chamfer_Distance/log/vrcnet_plus_balanced_debug_2024-03-12T23:40:55/train - Copy.log\"\n",
    "dcd_u, cd_u, emd_u, bcd_u = get_metrics_from_log(train_log_file, balanced=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise losses\n",
    "iterations = list(range(0, len(cd_cd)*2, 2))\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "plt.rc('xtick', labelsize=16) \n",
    "plt.rc('ytick', labelsize=16) \n",
    "\n",
    "# Plotting the line graph\n",
    "#plt.plot(iterations, dcd_cd, label='DCD', color=\"lightseagreen\")\n",
    "\n",
    "# emd\n",
    "# plt.plot(iterations, emd_cd, label='CD', color=\"palevioletred\")\n",
    "# plt.plot(iterations, emd_u, label='UniformCD', color=\"lightseagreen\")\n",
    "# plt.title('Completion training performance (EMD)')\n",
    "\n",
    "# cd\n",
    "# plt.plot(iterations, cd_cd, label='CD', color=\"palevioletred\")\n",
    "# plt.plot(iterations, cd_u, label='UniformCD', color=\"lightseagreen\")\n",
    "# plt.title('Completion training performance (CD)')\n",
    "\n",
    "# dcd\n",
    "# plt.plot(iterations, dcd_cd, label='CD', color=\"palevioletred\")\n",
    "# plt.plot(iterations, dcd_u, label='UniformCD', color=\"lightseagreen\")\n",
    "# plt.title('Completion training performance (DCD)')\n",
    "\n",
    "# train loss\n",
    "#plt.plot(iterations, cd_cd, label='CD', color=\"palevioletred\")\n",
    "plt.plot(iterations, bcd_u, label='UniformCD', color=\"lightseagreen\")\n",
    "plt.title('Completion training loss')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Iterations',  fontsize=16)\n",
    "plt.ylabel('Loss',  fontsize=16)\n",
    "\n",
    "# Displaying the graph\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "cdbb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
