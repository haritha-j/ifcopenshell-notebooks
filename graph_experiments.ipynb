{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIM relationship detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for performing relationship detection using design files.\n",
    "\n",
    "#### Input:\n",
    "1. Design file structure extracted from navisworks NWD file.\n",
    "2. IFC file generated from NWD file\n",
    "3. graph predictions from GNN\n",
    "\n",
    "#### Sections:\n",
    "1. Relationship identification: Identify, extract and (visualize) \n",
    "aggregation and connectivity relationships.\n",
    "2. Visalization: Deprecated\n",
    "3. IFC to cloud: Create point clouds from each element in IFC file\n",
    "4. Graph dataset: Create a graph dataset from relationship information\n",
    " \n",
    " (GNN training available in link_prediction.ipynb notebook)\n",
    "5. Evaluate GNN: Evaluate predictions from GNN\n",
    "6. Visalize predictions: Draw IFC element to visualize FP,TP,FNs\n",
    "7. Analyze dataset and results: Repetition removal, element category analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import json\n",
    "import collections\n",
    "import math\n",
    "import uuid\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ifc and pointcloud\n",
    "import ifcopenshell\n",
    "import open3d as o3d\n",
    "#import pymeshlab as ml\n",
    "from compas.geometry import oriented_bounding_box_numpy\n",
    "from scipy.spatial import distance\n",
    "from ifcopenshell.util.selector import Selector\n",
    "\n",
    "# graph \n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "\n",
    "from src.structure import get_systems, get_branches\n",
    "from src.ifc import draw_relationship, setup_ifc_file\n",
    "from src.geometry import element_distance\n",
    "from src.cloud import element_to_cloud\n",
    "from src.graph import process_nodes, process_edges, IndustrialFacilityDataset, get_node_features, get_edges_from_node_info\n",
    "from src.icp import run_command\n",
    "from src.geometry import sq_dist_vect, vector_mag\n",
    "from src.centerline import get_centerline_deviation, get_centerline_distance\n",
    "from src.evaluation import *\n",
    "# vis\n",
    "# from ipywidgets import interact\n",
    "# from OCC.Core.Bnd import Bnd_Box, Bnd_OBB\n",
    "# from OCC.Core.BRepBndLib import brepbndlib_AddOBB\n",
    "# from OCC.Core.BRepPrimAPI import (BRepPrimAPI_MakeBox, \n",
    "# BRepPrimAPI_MakeSphere, BRepPrimAPI_MakeCylinder)\n",
    "# from OCC.Core.gp import gp_Pnt, gp_XYZ, gp_Ax2, gp_Dir\n",
    "\n",
    "# from utils.JupyterIFCRenderer import JupyterIFCRenderer\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load design file structure\n",
    "data_path = \"/mnt/c/data/3D_CAD/\"\n",
    "system_dict_file = data_path + \"WestDeckBox.nwd_aggregation.json\"\n",
    "#system = 'West-DeckBox-Piping.rvm'\n",
    "system = 'West-DeckBox-Pipe.rvm'\n",
    "pipe_path = \"pipe_graph/\"\n",
    "sitename = 'east'\n",
    "\n",
    "blueprint = 'data/sample.ifc'\n",
    "temp_dir = \"output/east2/\"\n",
    "ifcConvert_executable = \"scripts/./IfcConvert\"\n",
    "\n",
    "pipe_graph = False\n",
    "cloi = False\n",
    "predict_classes = False\n",
    "\n",
    "if pipe_graph:\n",
    "    model_path = 'pipe_graph/'\n",
    "    dist_thresh=0.025\n",
    "    rough_dist_thresh=0.1\n",
    "elif cloi:\n",
    "    model_path = 'cloi/gnn/'\n",
    "    data_path = 'cloi/'\n",
    "    dist_thresh=0.02\n",
    "    rough_dist_thresh=2.0  \n",
    "else:\n",
    "#     model_path = 'gnn_params_bbox_only/'\n",
    "    model_path = 'gnn_params_bmvc/'\n",
    "    dist_thresh=0.0002\n",
    "    rough_dist_thresh=0.2\n",
    "\n",
    "#m = ifcopenshell.open(\"data/231110AC-11-Smiley-West-04-07-2007.ifc\")k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  aggrgegation relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_systems(system_dict_file)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topological relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches = get_branches(system_dict_file)[system]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requisties for IFC file creation\n",
    "\n",
    "create_guid = lambda: ifcopenshell.guid.compress(uuid.uuid1().hex)\n",
    "m = ifcopenshell.open(\"../merged.ifc\")\n",
    "owner_history = m.by_type(\"IfcOwnerHistory\")[0]\n",
    "project = m.by_type(\"IfcProject\")[0]\n",
    "context = m.by_type(\"IfcGeometricRepresentationContext\")[0]\n",
    "floor = m.by_type(\"IfcBuildingStorey\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw links between connected elements, return connections\n",
    "def visualize_branches(branches, ifc, floor=None, owner_history=None, context=None, draw=False, contiguous=True, dist_thresh=0.002):\n",
    "    pipe_type = 'IFCPIPESEGMENT'\n",
    "    fitting_type = 'IFCPIPEFITTING'\n",
    "\n",
    "    pipe_selector = Selector()\n",
    "    fitting_selector = Selector()\n",
    "    pipes = pipe_selector.parse(ifc, '.' + pipe_type)\n",
    "    fittings = fitting_selector.parse(ifc, '.' + fitting_type)\n",
    "    fitting_names = [f.Name for f in fittings]\n",
    "    pipe_names = [p.Name for p in pipes]\n",
    "    print(pipes[0].Name)\n",
    "\n",
    "    vis_dict = {}\n",
    "    for k, val in branches.items():\n",
    "        vis_elements = []\n",
    "        connect = True\n",
    "        for element in val:\n",
    "            if element in pipe_names:\n",
    "                vis_elements.append((element, pipe_type, connect))\n",
    "                connect = True\n",
    "            elif element in fitting_names:\n",
    "                vis_elements.append((element, fitting_type, connect))\n",
    "                connect = True\n",
    "            else:\n",
    "                connect = False\n",
    "        vis_dict[k] = vis_elements\n",
    "\n",
    "    error_count = 0\n",
    "    count = 0\n",
    "    rels = []\n",
    "    selector = Selector()\n",
    "    \n",
    "    # enumerate through branches\n",
    "    for k, val in tqdm(vis_dict.items()):\n",
    "#         if count == 10:\n",
    "#             break\n",
    "        branch_size = len(val)\n",
    "        for i, element in enumerate(val):\n",
    "            #check if element is not the last element\n",
    "            if (i+1) < branch_size:\n",
    "                try:\n",
    "                    \n",
    "                    if val[i+1][2] or not contiguous:\n",
    "                        rels.append([(element[0], element[1]), \n",
    "                                  (val[i+1][0], val[i+1][1])])\n",
    "                        if draw:\n",
    "                            element1 = selector.parse(\n",
    "                                m, '.' + element[1] + '[Name *= \"' + element[0] + '\"]')[0]\n",
    "                            element2 = selector.parse(\n",
    "                                m, '.' + val[i+1][1] + '[Name *= \"' + val[i+1][0] + '\"]')[0]\n",
    "                            draw_relationship(element[0], element1, \n",
    "                                  val[i+1][0], element2, ifc, floor, owner_history, context)\n",
    "                \n",
    "                    else:\n",
    "                        element1 = selector.parse(\n",
    "                            m, '.' + element[1] + '[Name *= \"' + element[0] + '\"]')[0]\n",
    "                        element2 = selector.parse(\n",
    "                            m, '.' + val[i+1][1] + '[Name *= \"' + val[i+1][0] + '\"]')[0]\n",
    "                        \n",
    "                        if element_distance(element1, element2, ifc) < dist_thresh:\n",
    "                            rels.append([(element[0], element[1]), \n",
    "                                  (val[i+1][0], val[i+1][1])])\n",
    "                            if draw:\n",
    "                                draw_relationship(element[0], element1, \n",
    "                                  val[i+1][0], element2, ifc, floor, owner_history, context)\n",
    "                except Exception as e:\n",
    "                    #print (e)\n",
    "                    error_count +=1\n",
    "        count +=1\n",
    "\n",
    "    print(error_count)\n",
    "    return rels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = visualize_branches(branches, m, floor, owner_history, context, True)\n",
    "# rels = visualize_branches(branches, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rels), rels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../top_rels_eastdeckbox_test.pkl', 'wb') as f:\n",
    "    pickle.dump(rels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.write('../east_vis_test.ifc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# viewer = JupyterIFCRenderer(m, size=(400,300))\n",
    "# viewer.setAllTransparent()\n",
    "# viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregation relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picker = viewer.colorPicker()\n",
    "# picker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dict = {'HVAC':['Rohrtypen:Kupfer - Hartgelötet:7718868', 'Rohrtypen:Kupfer - Hartgelötet:7718886', ],\n",
    "#            'electrical':[ 'Rohrtypen:Kupfer - Hartgelötet:7718872', 'Rohrtypen:Kupfer - Hartgelötet:7718880']}\n",
    "\n",
    "# # PAINT A SET OF ELEMENTS IN ONE COLOUR\n",
    "# def systemSelect(system):\n",
    "#     selector = Selector()\n",
    "#     for e in out_dict[system]:\n",
    "#         element = selector.parse(m, '.IfcProduct[Name *= \"' + e + '\"]')[0]\n",
    "#         viewer.setColor(element, picker.value)\n",
    "#     return system\n",
    "\n",
    "# interact(systemSelect, system=['HVAC', 'electrical'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances of building elements with represenations can be selected interactivly. Information such as the attributes `GUID`, `Name` etc. are displayed to the left of the 3D viewport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset colours\n",
    "# viewer.setDefaultColors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topological relationships\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### replace jupyter renderer\n",
    "\n",
    "1. Compute the bounding box of ifc product directly from points\n",
    "2. generate ifc elements to indicate relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "# element_name = \"TUBE 1 of BRANCH /AM-8120227-WD-MDA-01/B1\"\n",
    "# element_type = \"IFCPIPESEGMENT\"\n",
    "\n",
    "# selector = Selector()\n",
    "# element = selector.parse(\n",
    "#     m, '.' + element_type + '[Name *= \"' + element_name + '\"]')[0]\n",
    "\n",
    "# shape = element.Representation.Representations[0].Items[0]\n",
    "# element_coords = np.array(shape.Coordinates.CoordList)\n",
    "# #print(element_coords)\n",
    "# bbox = oriented_bounding_box_numpy(element_coords)\n",
    "# print(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "# element1_name = \"TUBE 1 of BRANCH /AM-8120227-WD-MDA-01/B1\"\n",
    "# element1_type = \"IFCPIPESEGMENT\"\n",
    "# element2_name = \"ELBOW 1 of BRANCH /AM-8120227-WD-MDA-01/B1\"\n",
    "# element2_type = \"IFCPIPEFITTING\"\n",
    "\n",
    "# element3_name = \"TUBE 2 of BRANCH /AM-8120227-WD-MDA-01/B1\"\n",
    "# element3_type = \"IFCPIPESEGMENT\"\n",
    "# element4_name = \"ELBOW 2 of BRANCH /AM-8120227-WD-MDA-01/B1\"\n",
    "# element4_type = \"IFCPIPEFITTING\"\n",
    "# element5_name = \"TUBE 3 of BRANCH /AM-8120227-WD-MDA-01/B1\"\n",
    "# element5_type = \"IFCPIPESEGMENT\"\n",
    "\n",
    "# draw_relationship(element1_name, element1_type, \n",
    "#                   element2_name, element2_type, m)\n",
    "# draw_relationship(element2_name, element2_type, \n",
    "#                   element3_name, element3_type, m)\n",
    "# draw_relationship(element3_name, element3_type, \n",
    "#                   element4_name, element4_type, m)\n",
    "# draw_relationship(element4_name, element4_type, \n",
    "#                   element5_name, element5_type, m)\n",
    "#element1_center, element1_coords = get_element_deets()\n",
    "\n",
    "\n",
    "\n",
    "# centerpoint =gp_Pnt(element1_center)\n",
    "# ball = BRepPrimAPI_MakeSphere(centerpoint, 0.02).Shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element.Representation.Representations[0].Items[0].CoordIndex = element.Representation.Representations[0].Items[0].CoordIndex[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IFC to cloud\n",
    "\n",
    "sample points from ifc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse through individual ifc files for each model type and sample point clouds\n",
    "#ifc = ifcopenshell.open(data_path +\"deckboxelbow_ref.ifc\")\n",
    "ifc = ifcopenshell.open(temp_dir +\"TEE.ifc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_type = 'IFCPIPEFITTING'\n",
    "#element_type = 'IFCPIPESEGMENT'\n",
    "selector = Selector()\n",
    "tees = selector.parse(ifc, '.' + element_type)\n",
    "print(len(tees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise an element and convert it into an obj\n",
    "def element_to_obj(element, save_path, blueprint, temp_dir, ifcConvert_executable):\n",
    "    # setup new ifc\n",
    "    ifc = setup_ifc_file(blueprint)\n",
    "    owner_history = ifc.by_type(\"IfcOwnerHistory\")[0]\n",
    "    project = ifc.by_type(\"IfcProject\")[0]\n",
    "    context = ifc.by_type(\"IfcGeometricRepresentationContext\")[0]\n",
    "    floor = ifc.by_type(\"IfcBuildingStorey\")[0]\n",
    "\n",
    "    ifc_info = {\"owner_history\": owner_history,\n",
    "        \"project\": project,\n",
    "       \"context\": context, \n",
    "       \"floor\": floor}\n",
    "    \n",
    "    # normalise points\n",
    "    points = np.array([e for e in ifc.traverse(element) if e.is_a(\"IfcCartesianPointList3D\")][0][0])\n",
    "    mean = np.mean(points, axis=0)\n",
    "    norm_points = points - mean\n",
    "    norm_factor = np.max(np.linalg.norm(norm_points, axis=1))*3/1000\n",
    "    norm_points /= norm_factor\n",
    "    #print(norm_factor, mean, element.id())\n",
    "    [e for e in ifc.traverse(element) if e.is_a(\"IfcCartesianPointList3D\")][0][0] = norm_points.tolist()\n",
    "\n",
    "    # add element to new ifc\n",
    "    ifc.add(element)\n",
    "    tmp_ifc = os.path.join(temp_dir, 'tmp.ifc')\n",
    "    ifc.write(tmp_ifc)\n",
    "    \n",
    "    # convert ifc to obj\n",
    "    cmds = (ifcConvert_executable, tmp_ifc, save_path)\n",
    "    result = run_command(cmds)\n",
    "    \n",
    "    return mean.tolist(), norm_factor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an element to an obj\n",
    "metadata_path = data_path + \"bp_east_metadata.json\"\n",
    "category = {}\n",
    "with open(metadata_path, \"r\") as jsonFile:\n",
    "    metadata = json.load(jsonFile)\n",
    "\n",
    "for i, element in enumerate(tqdm(tees)):\n",
    "    save_path = data_path +\"east_tee_obj/\" + str(i) + \".obj\"\n",
    "    mean, norm_factor = element_to_obj(element, save_path, blueprint, temp_dir, ifcConvert_executable)\n",
    "    category[str(i)] = {\"mean\":mean, \"norm_factor\":norm_factor, \"id\":element.id()}\n",
    "    \n",
    "metadata[\"tee\"] = category\n",
    "with open(metadata_path, \"w\") as jsonFile:\n",
    "    json.dump(metadata, jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an element to a point cloud directly\n",
    "metadata_path = data_path + \"bp_east_metadata.json\"\n",
    "category = {}\n",
    "with open(metadata_path, \"r\") as jsonFile:\n",
    "    metadata = json.load(jsonFile)\n",
    "\n",
    "for i, element in enumerate(tqdm(tees)):\n",
    "    save_path = data_path +\"east_tee_cloud/\" + str(i) + \".pcd\"\n",
    "    cloud = element_to_cloud(element, save_path, 2048)\n",
    "    category[str(i)] = {\"id\":element.id()}\n",
    "    \n",
    "#print(category)\n",
    "metadata[\"tee\"] = category\n",
    "with open(metadata_path, \"w\") as jsonFile:\n",
    "    json.dump(metadata, jsonFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get nodes & edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['FLANGE', 'ELBOW', 'TEE', 'TUBE', 'BEND']\n",
    "node_info = process_nodes(ifc, types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(node_info), len(node_info[0]), len(node_info[1]))\n",
    "\n",
    "with open('../nodes_eastdeckbox_test.pkl', 'wb') as f:\n",
    "    pickle.dump(node_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../top_rels_eastdeckbox.pkl', 'rb') as f:\n",
    "    rels = pickle.load(f)\n",
    "with open('../nodes_eastdeckbox.pkl', 'rb') as f:\n",
    "    node_info = pickle.load(f)\n",
    "    nodes = node_info[0]\n",
    "    \n",
    "edges = process_edges(ifc, nodes, rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../edges_eastdeckbox_test.pkl', 'wb') as f:\n",
    "    pickle.dump(edges, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "\n",
    "# print(points.shape, labels.shape, centers.shape, lengths.shape, directions.shape)\n",
    "# print(points[0][0], labels[0], centers[0], lengths[0], directions[0])\n",
    "# print(5 in labels)\n",
    "\n",
    "# edges_src = edges[:,0]\n",
    "# edges_dst = edges[:,1]\n",
    "# print(edges_src)\n",
    "# print(np.max(edges_dst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = IndustrialFacilityDataset()\n",
    "graph = dataset[0]\n",
    "\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load edges\n",
    "\n",
    "with open(model_path + '/eval/pos_edges_test.pkl', 'rb') as f:\n",
    "            u, v = pickle.load(f)\n",
    "with open(model_path + '/eval/neg_edges_test.pkl', 'rb') as f:\n",
    "            neg_u_full,neg_v_full = pickle.load(f)\n",
    "print(len(neg_u_full), len(u))\n",
    "\n",
    "u = u.cpu().numpy()\n",
    "v = v.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predicted scores\n",
    "with open(model_path + '/eval/pos_score_test.pkl', 'rb') as f:\n",
    "            pos_score = pickle.load(f)\n",
    "with open(model_path + '/eval/neg_score_test.pkl', 'rb') as f:\n",
    "            neg_score= pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate confusion matrix coefficiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "\n",
    "# TPs / FNs\n",
    "sig = 1/(1 + np.exp(-pos_score))\n",
    "ind = np.arange(len(pos_score))\n",
    "tp = ind[sig > threshold]\n",
    "TPs = [(u[i], v[i]) for i in tp]\n",
    "fn = ind[sig < threshold]\n",
    "FNs = [(u[i], v[i]) for i in fn]\n",
    "\n",
    "# FPs\n",
    "sig = 1/(1 + np.exp(-neg_score))\n",
    "ind = np.arange(len(neg_score))\n",
    "fp = ind[sig > threshold]\n",
    "FPs = [(neg_u_full[i], neg_v_full[i]) for i in fp]\n",
    "#FPs = fp\n",
    "print(len(TPs), len(FNs), len(FPs), FPs[20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(1/(1 + np.exp(-neg_score[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(model_path + '/eval/metrics_test.pkl', 'wb') as f:\n",
    "#     pickle.dump([TPs, FPs, FNs], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(model_path + '/eval/metrics_test.pkl', 'rb') as f:\n",
    "#     TPs, FPs, FNs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate raw metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = len(TPs)/(len(TPs)+len(FPs))\n",
    "recall = len(TPs)/(len(TPs)+len(FNs))\n",
    "accuracy = (len(TPs)-len(FPs) +len(neg_score))/(len(neg_score)+len(pos_score))\n",
    "print(precision, recall, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refinement based on element distances (eliminate predictions beyond a distance threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cloi:\n",
    "    site = 'cloi'\n",
    "else:\n",
    "    site = 'eastdeckbox'\n",
    "\n",
    "node_file = \"nodes_\" + site + \".pkl\"\n",
    "with open(data_path + node_file, 'rb') as f:\n",
    "    node_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipe_graph:\n",
    "    # filter out non-pipe nodes and recode edges with new edge indices\n",
    "    node_info0 = [ni for i, ni in enumerate(node_info[0]) if ni[0]==3]\n",
    "    node_info1 = [node_info[1][i] for i, ni in enumerate(node_info[0]) if ni[0]==3]\n",
    "    node_info = [node_info0, (node_info1)]\n",
    "    \n",
    "    print(len(node_info0), len(node_info1),  len(TPs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refined_TPs = check_predictions(TPs, node_info[1])\n",
    "# refined_FPs = check_predictions(FPs, node_info[1])\n",
    "\n",
    "refined_TPs, new_FNs = check_predictions_fast(TPs, node_info[1], node_info[0], \n",
    "                                              dist_thresh, rough_dist_thresh)\n",
    "refined_FPs, _ = check_predictions_fast(FPs, node_info[1], node_info[0],\n",
    "                                        dist_thresh, rough_dist_thresh)\n",
    "\n",
    "print(len(refined_TPs), len(refined_FPs), len(new_FNs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path + '/eval/refined_test.pkl', 'wb') as f:\n",
    "    pickle.dump([refined_TPs, refined_FPs, new_FNs], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path + '/eval/refined_test.pkl', 'rb') as f:\n",
    "    refined_TPs, refined_FPs, new_FNs = pickle.load(f)\n",
    "print(len(refined_TPs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNs = FNs + new_FNs\n",
    "precision = len(refined_TPs)/(len(refined_TPs)+len(refined_FPs))\n",
    "recall = len(refined_TPs)/(len(refined_TPs)+len(FNs))\n",
    "accuracy = (len(refined_TPs)-len(refined_FPs) +len(neg_score))/(len(neg_score)+len(pos_score))\n",
    "f1 = (2 * precision * recall) / (precision + recall)\n",
    "print(\"pr\", precision, \"re\", recall, \"ac\", accuracy, \"f1\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = (2 * 83.6 * 85.4) / (83.6 + 85.4)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "radius_threshold = 0.1\n",
    "centerline_angle_threshold = math.radians(90) # angle threshold for two pipes to be considered parallel\n",
    "centerline_dist_threshold = 0.001 # centerline distance threshold pipes to be to considered connected\n",
    "params_path = Path('output/east_ref/')\n",
    "dataset = \"east\"\n",
    "\n",
    "refined_refined_TPs, new_new_FNs = connectivity_refinements(refined_TPs, node_info, params_path, dataset, radius_threshold,\n",
    "                                              centerline_angle_threshold, centerline_dist_threshold)\n",
    "\n",
    "refined_refined_FPs, _ = connectivity_refinements(refined_FPs, node_info, params_path, dataset, radius_threshold,\n",
    "                                              centerline_angle_threshold, centerline_dist_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(refined_refined_TPs)/len(refined_TPs), len(refined_refined_FPs)/len(refined_FPs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refined metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = len(refined_refined_TPs)/(len(refined_refined_TPs)+len(refined_refined_FPs))\n",
    "recall = len(refined_refined_TPs)/(len(refined_refined_TPs)+len(FNs)+len(new_new_FNs))\n",
    "accuracy = (len(refined_refined_TPs)-len(refined_refined_FPs) +len(neg_score))/(len(neg_score)+len(pos_score))\n",
    "print(precision, recall, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_score= pos_score[:int(len(pos_score)/10)]\n",
    "neg_score= pos_score[:int(len(neg_score)/10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.concatenate([pos_score, neg_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = 1/(1 + np.exp(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "RocCurveDisplay.from_predictions(labels, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifc = ifcopenshell.open(data_path+\"/east_merged.ifc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_guid = lambda: ifcopenshell.guid.compress(uuid.uuid1().hex)\n",
    "owner_history = ifc.by_type(\"IfcOwnerHistory\")[0]\n",
    "project = ifc.by_type(\"IfcProject\")[0]\n",
    "context = ifc.by_type(\"IfcGeometricRepresentationContext\")[0]\n",
    "floor = ifc.by_type(\"IfcBuildingStorey\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = ifc.createIfcColourRgb('red', Red=0.9, Green=0.0, Blue=0.0)\n",
    "green = ifc.createIfcColourRgb('green', Red=0.0, Green=0.9, Blue=0.0)\n",
    "yellow = ifc.createIfcColourRgb('yellow', Red=0.9, Green=0.9, Blue=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(refined_TPs), len(refined_FPs), len(FNs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize results on ifc file\n",
    "def draw_predictions(preds, nodes, ifc, colour):\n",
    "    for pair in tqdm(preds):\n",
    "        element1 = ifc.by_id(nodes[pair[0]][4])\n",
    "        element1_name = element1.Name\n",
    "        element2 = ifc.by_id(nodes[pair[1]][4])\n",
    "        element2_name = element2.Name\n",
    "        \n",
    "        draw_relationship(element1_name, element1, element2_name, \n",
    "                          element2, ifc, floor, owner_history, context, colour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize pipe dataset\n",
    "if pipe_graph:\n",
    "    with open(pipe_path + 'pipe_edges2_' + sitename + '.pkl', 'rb') as f:\n",
    "        pipe_edges = pickle.load(f)\n",
    "    pipe_edges = [e[0] for e in pipe_edges]\n",
    "    print(len(pipe_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipe_graph:\n",
    "    draw_predictions(pipe_edges, node_info[0], ifc, green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_predictions(refined_TPs, node_info[0], ifc, green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_predictions(refined_FPs, node_info[0], ifc, yellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_predictions(FNs, node_info[0], ifc, red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifc.write(model_path + '/pipe2_vis_test_ref.ifc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional refinements\n",
    "\n",
    "### remove repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(refined_TPs))\n",
    "non_rep_TPs = remove_repetitions(refined_TPs)\n",
    "print(len(non_rep_TPs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(refined_FPs))\n",
    "non_rep_FPs = remove_repetitions(refined_FPs)\n",
    "print(len(non_rep_FPs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(FNs))\n",
    "non_rep_FNs = remove_repetitions(FNs)\n",
    "print(len(non_rep_FNs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_FNs = compare_preds(non_rep_FNs, non_rep_TPs)\n",
    "print(len(non_rep_FNs), len(refined_FNs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(non_rep_FNs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refined metrics after removing repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = len(non_rep_TPs)/(len(non_rep_TPs)+len(non_rep_FPs))\n",
    "recall = len(non_rep_TPs)/(len(non_rep_TPs)+len(non_rep_FNs))\n",
    "accuracy = (len(non_rep_TPs)-len(refined_FPs) +len(neg_score))/(len(neg_score)+len(pos_score))\n",
    "print(precision, recall, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path + '/eval/non_rep_test.pkl', 'wb') as f:\n",
    "    pickle.dump([non_rep_TPs, non_rep_FPs, non_rep_FNs], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate metrics for each element category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse dataset and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_bins = sort_type(non_rep_TPs,node_info[0] )\n",
    "fp_bins = sort_type(non_rep_FPs,node_info[0] )\n",
    "fn_bins = sort_type(non_rep_FNs,node_info[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tp_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp_bins/(tp_bins+fn_bins)\n",
    "precision = tp_bins/(tp_bins+fp_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1:\", (2*precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate element types in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse dataset\n",
    "\n",
    "# load data\n",
    "site = 'east'\n",
    "\n",
    "c_east = analyse_dataset('east')\n",
    "c_west = analyse_dataset('west')\n",
    "count = c_east + c_west\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation v2\n",
    "\n",
    "The dataset must have shared element ids across both the pointclouds used for parameter inference and the graph node dataset. The steps for creating the new dataset are;\n",
    "1. create subsets of merged.ifc for the 4 classes (include bend in elbow) preserving element id\n",
    "2. run refinement to get refined_ifcs (manual deletion)\n",
    "3. generate pointcloud dataset from ifcs\n",
    "4. get the element ids of refined ifcs, compare with ids in the graph and delete extra nodes / edges to get refined graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create subsets of merged.ifc for the 4 classes (include bend in elbow) preserving element id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elements_from_ifc(ifc):\n",
    "    ifc_full = ifcopenshell.open(ifc)\n",
    "    selector = Selector()\n",
    "    element_type = 'IFCPIPEFITTING'\n",
    "    elements_fitting = selector.parse(ifc_full, '.' + element_type)\n",
    "    element_type = 'IFCPIPESEGMENT'\n",
    "    elements_segments = selector.parse(ifc_full, '.' + element_type)\n",
    "    elements = elements_segments + elements_fitting\n",
    "    print(len(elements))\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ifc\n",
    "elements = get_elements_from_ifc(data_path +\"east_merged.ifc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nodes\n",
    "with open(data_path + 'nodes_eastdeckbox.pkl', 'rb') as f:\n",
    "    node_info = pickle.load(f)\n",
    "    nodes = node_info[0]\n",
    "    \n",
    "print(len(nodes))\n",
    "\n",
    "# load edges\n",
    "with open(data_path + 'edges_eastdeckbox.pkl', 'rb') as f:\n",
    "    edges = pickle.load(f)\n",
    "    \n",
    "print(edges[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get matching elements between nodes and elements\n",
    "matching_elements = {}\n",
    "\n",
    "for i, el in enumerate(tqdm(elements)):\n",
    "    for j, node in enumerate(nodes):\n",
    "        if el.id() == node[4]:\n",
    "            matching_elements[str(j)] = i\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [b for b in nodes if b[0]]\n",
    "print(len(matching_elements), nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter by class\n",
    "#types = ['FLANGE', 'ELBOW', 'TEE', 'TUBE', 'BEND']\n",
    "types = ['BEND']\n",
    "combined_metadata = {}\n",
    "for k, tp in enumerate(types):\n",
    "    class_metadata = {}\n",
    "    \n",
    "    # setup new ifc\n",
    "    ifc = setup_ifc_file(blueprint)\n",
    "    owner_history = ifc.by_type(\"IfcOwnerHistory\")[0]\n",
    "    project = ifc.by_type(\"IfcProject\")[0]\n",
    "    context = ifc.by_type(\"IfcGeometricRepresentationContext\")[0]\n",
    "    floor = ifc.by_type(\"IfcBuildingStorey\")[0]\n",
    "    k = 4\n",
    "    # add element to new ifc\n",
    "    for i, nd in enumerate(tqdm(nodes)):\n",
    "        # check if class matches\n",
    "        #print(matching_elements[str(i)])\n",
    "        count = 0\n",
    "        if nd[0] == k:\n",
    "            el = elements[matching_elements[str(i)]]\n",
    "            new_id = ifc.add(el).id()\n",
    "            class_metadata[str(new_id)] = el.id()\n",
    "            #print(el.id(), new_id)\n",
    "            \n",
    "    combined_metadata[tp] = class_metadata\n",
    "    \n",
    "    # write ifc\n",
    "    tmp_ifc = os.path.join(temp_dir, tp+'.ifc')\n",
    "    ifc.write(tmp_ifc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(combined_metadata[\"BEND\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(temp_dir, 'id_metadata.json'), 'w')\n",
    "json.dump(combined_metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2 and 3 must be carried out using IFCPARSE script (or manually) and using element_to_cloud function (or using blender for tees)\n",
    "\n",
    "4. get the element ids of refined ifcs, compare with ids in the graph and delete extra nodes / edges to get refined graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify nodes which have been deleted\n",
    "total_ids = []\n",
    "for k, tp in enumerate(types):\n",
    "    elements = get_elements_from_ifc(os.path.join(temp_dir, tp+'_ref.ifc'))\n",
    "    element_ids = [combined_metadata[tp][str(el.id())] for el in elements]\n",
    "    total_ids += element_ids\n",
    "\n",
    "print(len(total_ids))\n",
    "missing_count = 0\n",
    "\n",
    "for nd in nodes:\n",
    "    if nd[4] not in total_ids:\n",
    "        missing_count += 1\n",
    "print(\"missing\", missing_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(total_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create modified graph with refined nodes\n",
    "new_nodes, new_node_points, new_edges = [], [], []\n",
    "id_dict = {}\n",
    "for i in range(len(nodes)):\n",
    "    id_dict[str(i)] = None\n",
    "    \n",
    "for i, nd in enumerate(nodes):\n",
    "    if nd[4] in total_ids:\n",
    "        new_nodes.append(nd)\n",
    "        new_node_points.append(node_info[1][i])\n",
    "        id_dict[str(i)] = len(new_nodes)-1\n",
    "        \n",
    "print(len(new_nodes), new_nodes[0])\n",
    "\n",
    "for e in edges:\n",
    "    if id_dict[str(e[0])] is not None and id_dict[str(e[1])] is not None:\n",
    "        new_edges.append((id_dict[str(e[0])], id_dict[str(e[1])]))\n",
    "        \n",
    "print(len(new_edges), new_edges[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(temp_dir, 'nodes_westdeckbox_ref.pkl'), 'wb') as f:\n",
    "    pickle.dump([new_nodes, new_node_points], f)\n",
    "    \n",
    "with open(os.path.join(temp_dir, 'edges_westdeckbox_ref.pkl'), 'wb') as f:\n",
    "    pickle.dump(new_edges, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
