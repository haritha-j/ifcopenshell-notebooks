{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIM relationship detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for performing relationship detection using design files.\n",
    "\n",
    "#### Input:\n",
    "1. Design file structure extracted from navisworks NWD file.\n",
    "2. IFC file generated from NWD file\n",
    "3. graph predictions from GNN\n",
    "\n",
    "#### Sections:\n",
    "1. Relationship identification: Identify, extract and (visualize) \n",
    "aggregation and connectivity relationships.\n",
    "2. Visalization: Deprecated\n",
    "3. IFC to cloud: Create point clouds from each element in IFC file\n",
    "4. Graph dataset: Create a graph dataset from relationship information\n",
    " \n",
    " (GNN training available in link_prediction.ipynb notebook)\n",
    "5. Evaluate GNN: Evaluate predictions from GNN\n",
    "6. Visalize predictions: Draw IFC element to visualize FP,TP,FNs\n",
    "7. Analyze dataset and results: Repetition removal, element category analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import json\n",
    "import collections\n",
    "import math\n",
    "import uuid\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ifc and pointcloud\n",
    "import ifcopenshell\n",
    "import open3d as o3d\n",
    "#import pymeshlab as ml\n",
    "from compas.geometry import oriented_bounding_box_numpy\n",
    "from scipy.spatial import distance\n",
    "from ifcopenshell.util.selector import Selector\n",
    "\n",
    "# graph \n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "\n",
    "from src.structure import get_systems, get_branches\n",
    "from src.ifc import draw_relationship, setup_ifc_file\n",
    "from src.geometry import element_distance\n",
    "from src.cloud import element_to_cloud\n",
    "from src.graph import process_nodes, process_edges, IndustrialFacilityDataset, get_node_features\n",
    "from src.icp import run_command\n",
    "from src.geometry import sq_dist_vect, vector_mag\n",
    "from src.centerline import get_centerline_deviation, get_centerline_distance\n",
    "# vis\n",
    "# from ipywidgets import interact\n",
    "# from OCC.Core.Bnd import Bnd_Box, Bnd_OBB\n",
    "# from OCC.Core.BRepBndLib import brepbndlib_AddOBB\n",
    "# from OCC.Core.BRepPrimAPI import (BRepPrimAPI_MakeBox, \n",
    "# BRepPrimAPI_MakeSphere, BRepPrimAPI_MakeCylinder)\n",
    "# from OCC.Core.gp import gp_Pnt, gp_XYZ, gp_Ax2, gp_Dir\n",
    "\n",
    "# from utils.JupyterIFCRenderer import JupyterIFCRenderer\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load design file structure\n",
    "data_path = \"/mnt/c/data/3D_CAD/\"\n",
    "system_dict_file = data_path + \"WestDeckBox.nwd_aggregation.json\"\n",
    "#system = 'West-DeckBox-Piping.rvm'\n",
    "system = 'West-DeckBox-Pipe.rvm'\n",
    "\n",
    "blueprint = 'data/sample.ifc'\n",
    "temp_dir = \"output/east2/\"\n",
    "ifcConvert_executable = \"scripts/./IfcConvert\"\n",
    "\n",
    "\n",
    "#m = ifcopenshell.open(\"data/231110AC-11-Smiley-West-04-07-2007.ifc\")k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  aggrgegation relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_systems(system_dict_file)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topological relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches = get_branches(system_dict_file)[system]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requisties for IFC file creation\n",
    "\n",
    "create_guid = lambda: ifcopenshell.guid.compress(uuid.uuid1().hex)\n",
    "m = ifcopenshell.open(\"../merged.ifc\")\n",
    "owner_history = m.by_type(\"IfcOwnerHistory\")[0]\n",
    "project = m.by_type(\"IfcProject\")[0]\n",
    "context = m.by_type(\"IfcGeometricRepresentationContext\")[0]\n",
    "floor = m.by_type(\"IfcBuildingStorey\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw links between connected elements, return connections\n",
    "def visualize_branches(branches, ifc, floor=None, owner_history=None, context=None, draw=False, contiguous=True, dist_thresh=0.002):\n",
    "    pipe_type = 'IFCPIPESEGMENT'\n",
    "    fitting_type = 'IFCPIPEFITTING'\n",
    "\n",
    "    pipe_selector = Selector()\n",
    "    fitting_selector = Selector()\n",
    "    pipes = pipe_selector.parse(ifc, '.' + pipe_type)\n",
    "    fittings = fitting_selector.parse(ifc, '.' + fitting_type)\n",
    "    fitting_names = [f.Name for f in fittings]\n",
    "    pipe_names = [p.Name for p in pipes]\n",
    "    print(pipes[0].Name)\n",
    "\n",
    "    vis_dict = {}\n",
    "    for k, val in branches.items():\n",
    "        vis_elements = []\n",
    "        connect = True\n",
    "        for element in val:\n",
    "            if element in pipe_names:\n",
    "                vis_elements.append((element, pipe_type, connect))\n",
    "                connect = True\n",
    "            elif element in fitting_names:\n",
    "                vis_elements.append((element, fitting_type, connect))\n",
    "                connect = True\n",
    "            else:\n",
    "                connect = False\n",
    "        vis_dict[k] = vis_elements\n",
    "\n",
    "    error_count = 0\n",
    "    count = 0\n",
    "    rels = []\n",
    "    selector = Selector()\n",
    "    \n",
    "    # enumerate through branches\n",
    "    for k, val in tqdm(vis_dict.items()):\n",
    "#         if count == 10:\n",
    "#             break\n",
    "        branch_size = len(val)\n",
    "        for i, element in enumerate(val):\n",
    "            #check if element is not the last element\n",
    "            if (i+1) < branch_size:\n",
    "                try:\n",
    "                    \n",
    "                    if val[i+1][2] or not contiguous:\n",
    "                        rels.append([(element[0], element[1]), \n",
    "                                  (val[i+1][0], val[i+1][1])])\n",
    "                        if draw:\n",
    "                            element1 = selector.parse(\n",
    "                                m, '.' + element[1] + '[Name *= \"' + element[0] + '\"]')[0]\n",
    "                            element2 = selector.parse(\n",
    "                                m, '.' + val[i+1][1] + '[Name *= \"' + val[i+1][0] + '\"]')[0]\n",
    "                            draw_relationship(element[0], element1, \n",
    "                                  val[i+1][0], element2, ifc, floor, owner_history, context)\n",
    "                \n",
    "                    else:\n",
    "                        element1 = selector.parse(\n",
    "                            m, '.' + element[1] + '[Name *= \"' + element[0] + '\"]')[0]\n",
    "                        element2 = selector.parse(\n",
    "                            m, '.' + val[i+1][1] + '[Name *= \"' + val[i+1][0] + '\"]')[0]\n",
    "                        \n",
    "                        if element_distance(element1, element2, ifc) < dist_thresh:\n",
    "                            rels.append([(element[0], element[1]), \n",
    "                                  (val[i+1][0], val[i+1][1])])\n",
    "                            if draw:\n",
    "                                draw_relationship(element[0], element1, \n",
    "                                  val[i+1][0], element2, ifc, floor, owner_history, context)\n",
    "                except Exception as e:\n",
    "                    #print (e)\n",
    "                    error_count +=1\n",
    "        count +=1\n",
    "\n",
    "    print(error_count)\n",
    "    return rels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = visualize_branches(branches, m, floor, owner_history, context, True)\n",
    "# rels = visualize_branches(branches, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rels), rels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../top_rels_eastdeckbox_test.pkl', 'wb') as f:\n",
    "    pickle.dump(rels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.write('../east_vis_test.ifc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# viewer = JupyterIFCRenderer(m, size=(400,300))\n",
    "# viewer.setAllTransparent()\n",
    "# viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregation relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picker = viewer.colorPicker()\n",
    "# picker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dict = {'HVAC':['Rohrtypen:Kupfer - Hartgelötet:7718868', 'Rohrtypen:Kupfer - Hartgelötet:7718886', ],\n",
    "#            'electrical':[ 'Rohrtypen:Kupfer - Hartgelötet:7718872', 'Rohrtypen:Kupfer - Hartgelötet:7718880']}\n",
    "\n",
    "# # PAINT A SET OF ELEMENTS IN ONE COLOUR\n",
    "# def systemSelect(system):\n",
    "#     selector = Selector()\n",
    "#     for e in out_dict[system]:\n",
    "#         element = selector.parse(m, '.IfcProduct[Name *= \"' + e + '\"]')[0]\n",
    "#         viewer.setColor(element, picker.value)\n",
    "#     return system\n",
    "\n",
    "# interact(systemSelect, system=['HVAC', 'electrical'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances of building elements with represenations can be selected interactivly. Information such as the attributes `GUID`, `Name` etc. are displayed to the left of the 3D viewport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset colours\n",
    "# viewer.setDefaultColors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topological relationships\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### replace jupyter renderer\n",
    "\n",
    "1. Compute the bounding box of ifc product directly from points\n",
    "2. generate ifc elements to indicate relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "# element_name = \"TUBE 1 of BRANCH /AM-8120227-WD-MDA-01/B1\"\n",
    "# element_type = \"IFCPIPESEGMENT\"\n",
    "\n",
    "# selector = Selector()\n",
    "# element = selector.parse(\n",
    "#     m, '.' + element_type + '[Name *= \"' + element_name + '\"]')[0]\n",
    "\n",
    "# shape = element.Representation.Representations[0].Items[0]\n",
    "# element_coords = np.array(shape.Coordinates.CoordList)\n",
    "# #print(element_coords)\n",
    "# bbox = oriented_bounding_box_numpy(element_coords)\n",
    "# print(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "# element1_name = \"TUBE 1 of BRANCH /AM-8120227-WD-MDA-01/B1\"\n",
    "# element1_type = \"IFCPIPESEGMENT\"\n",
    "# element2_name = \"ELBOW 1 of BRANCH /AM-8120227-WD-MDA-01/B1\"\n",
    "# element2_type = \"IFCPIPEFITTING\"\n",
    "\n",
    "# element3_name = \"TUBE 2 of BRANCH /AM-8120227-WD-MDA-01/B1\"\n",
    "# element3_type = \"IFCPIPESEGMENT\"\n",
    "# element4_name = \"ELBOW 2 of BRANCH /AM-8120227-WD-MDA-01/B1\"\n",
    "# element4_type = \"IFCPIPEFITTING\"\n",
    "# element5_name = \"TUBE 3 of BRANCH /AM-8120227-WD-MDA-01/B1\"\n",
    "# element5_type = \"IFCPIPESEGMENT\"\n",
    "\n",
    "# draw_relationship(element1_name, element1_type, \n",
    "#                   element2_name, element2_type, m)\n",
    "# draw_relationship(element2_name, element2_type, \n",
    "#                   element3_name, element3_type, m)\n",
    "# draw_relationship(element3_name, element3_type, \n",
    "#                   element4_name, element4_type, m)\n",
    "# draw_relationship(element4_name, element4_type, \n",
    "#                   element5_name, element5_type, m)\n",
    "#element1_center, element1_coords = get_element_deets()\n",
    "\n",
    "\n",
    "\n",
    "# centerpoint =gp_Pnt(element1_center)\n",
    "# ball = BRepPrimAPI_MakeSphere(centerpoint, 0.02).Shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element.Representation.Representations[0].Items[0].CoordIndex = element.Representation.Representations[0].Items[0].CoordIndex[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IFC to cloud\n",
    "\n",
    "sample points from ifc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse through individual ifc files for each model type and sample point clouds\n",
    "#ifc = ifcopenshell.open(data_path +\"deckboxelbow_ref.ifc\")\n",
    "ifc = ifcopenshell.open(temp_dir +\"TEE.ifc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_type = 'IFCPIPEFITTING'\n",
    "#element_type = 'IFCPIPESEGMENT'\n",
    "selector = Selector()\n",
    "tees = selector.parse(ifc, '.' + element_type)\n",
    "print(len(tees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise an element and convert it into an obj\n",
    "def element_to_obj(element, save_path, blueprint, temp_dir, ifcConvert_executable):\n",
    "    # setup new ifc\n",
    "    ifc = setup_ifc_file(blueprint)\n",
    "    owner_history = ifc.by_type(\"IfcOwnerHistory\")[0]\n",
    "    project = ifc.by_type(\"IfcProject\")[0]\n",
    "    context = ifc.by_type(\"IfcGeometricRepresentationContext\")[0]\n",
    "    floor = ifc.by_type(\"IfcBuildingStorey\")[0]\n",
    "\n",
    "    ifc_info = {\"owner_history\": owner_history,\n",
    "        \"project\": project,\n",
    "       \"context\": context, \n",
    "       \"floor\": floor}\n",
    "    \n",
    "    # normalise points\n",
    "    points = np.array([e for e in ifc.traverse(element) if e.is_a(\"IfcCartesianPointList3D\")][0][0])\n",
    "    mean = np.mean(points, axis=0)\n",
    "    norm_points = points - mean\n",
    "    norm_factor = np.max(np.linalg.norm(norm_points, axis=1))*3/1000\n",
    "    norm_points /= norm_factor\n",
    "    #print(norm_factor, mean, element.id())\n",
    "    [e for e in ifc.traverse(element) if e.is_a(\"IfcCartesianPointList3D\")][0][0] = norm_points.tolist()\n",
    "\n",
    "    # add element to new ifc\n",
    "    ifc.add(element)\n",
    "    tmp_ifc = os.path.join(temp_dir, 'tmp.ifc')\n",
    "    ifc.write(tmp_ifc)\n",
    "    \n",
    "    # convert ifc to obj\n",
    "    cmds = (ifcConvert_executable, tmp_ifc, save_path)\n",
    "    result = run_command(cmds)\n",
    "    \n",
    "    return mean.tolist(), norm_factor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an element to an obj\n",
    "metadata_path = data_path + \"bp_east_metadata.json\"\n",
    "category = {}\n",
    "with open(metadata_path, \"r\") as jsonFile:\n",
    "    metadata = json.load(jsonFile)\n",
    "\n",
    "for i, element in enumerate(tqdm(tees)):\n",
    "    save_path = data_path +\"east_tee_obj/\" + str(i) + \".obj\"\n",
    "    mean, norm_factor = element_to_obj(element, save_path, blueprint, temp_dir, ifcConvert_executable)\n",
    "    category[str(i)] = {\"mean\":mean, \"norm_factor\":norm_factor, \"id\":element.id()}\n",
    "    \n",
    "metadata[\"tee\"] = category\n",
    "with open(metadata_path, \"w\") as jsonFile:\n",
    "    json.dump(metadata, jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an element to a point cloud directly\n",
    "metadata_path = data_path + \"bp_east_metadata.json\"\n",
    "category = {}\n",
    "with open(metadata_path, \"r\") as jsonFile:\n",
    "    metadata = json.load(jsonFile)\n",
    "\n",
    "for i, element in enumerate(tqdm(tees)):\n",
    "    save_path = data_path +\"east_tee_cloud/\" + str(i) + \".pcd\"\n",
    "    cloud = element_to_cloud(element, save_path, 2048)\n",
    "    category[str(i)] = {\"id\":element.id()}\n",
    "    \n",
    "#print(category)\n",
    "metadata[\"tee\"] = category\n",
    "with open(metadata_path, \"w\") as jsonFile:\n",
    "    json.dump(metadata, jsonFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get nodes & edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['FLANGE', 'ELBOW', 'TEE', 'TUBE', 'BEND']\n",
    "node_info = process_nodes(ifc, types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(node_info), len(node_info[0]), len(node_info[1]))\n",
    "\n",
    "with open('../nodes_eastdeckbox_test.pkl', 'wb') as f:\n",
    "    pickle.dump(node_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../top_rels_eastdeckbox.pkl', 'rb') as f:\n",
    "    rels = pickle.load(f)\n",
    "with open('../nodes_eastdeckbox.pkl', 'rb') as f:\n",
    "    node_info = pickle.load(f)\n",
    "    nodes = node_info[0]\n",
    "    \n",
    "edges = process_edges(ifc, nodes, rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../edges_eastdeckbox_test.pkl', 'wb') as f:\n",
    "    pickle.dump(edges, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "\n",
    "# print(points.shape, labels.shape, centers.shape, lengths.shape, directions.shape)\n",
    "# print(points[0][0], labels[0], centers[0], lengths[0], directions[0])\n",
    "# print(5 in labels)\n",
    "\n",
    "# edges_src = edges[:,0]\n",
    "# edges_dst = edges[:,1]\n",
    "# print(edges_src)\n",
    "# print(np.max(edges_dst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = IndustrialFacilityDataset()\n",
    "graph = dataset[0]\n",
    "\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load edges\n",
    "model_path = 'gnn_params/'\n",
    "\n",
    "with open(model_path + '/eval/pos_edges_test.pkl', 'rb') as f:\n",
    "            u, v = pickle.load(f)\n",
    "with open(model_path + '/eval/neg_edges_test.pkl', 'rb') as f:\n",
    "            neg_u_full,neg_v_full = pickle.load(f)\n",
    "print(len(neg_u_full), len(u))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predicted scores\n",
    "with open(model_path + '/eval/pos_score_test.pkl', 'rb') as f:\n",
    "            pos_score = pickle.load(f)\n",
    "with open(model_path + '/eval/neg_score_test.pkl', 'rb') as f:\n",
    "            neg_score= pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(neg_score), len(pos_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate confusion matrix coefficiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "FNs, TPs, FPs, TNs = [], [], [], []\n",
    "\n",
    "# TPs / FNs\n",
    "for i, score in enumerate(pos_score):\n",
    "    sig = 1/(1 + np.exp(-score))\n",
    "    if sig > threshold:\n",
    "        TPs.append([u[i].item(), v[i].item()])\n",
    "    else:\n",
    "        FNs.append([u[i].item(), v[i].item()])\n",
    "\n",
    "# FPs\n",
    "for i, score in enumerate(tqdm(neg_score)):\n",
    "    sig = 1/(1 + np.exp(-score))\n",
    "    if sig > threshold:\n",
    "        FPs.append([neg_u_full[i], neg_v_full[i]])\n",
    "\n",
    "print(len(TPs), len(FPs), len(FNs) )\n",
    "TPs = np.array(TPs)\n",
    "FPs = np.array(FPs)\n",
    "FNs = np.array(FNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1/(1 + np.exp(-neg_score[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path + '/eval/metrics_test.pkl', 'wb') as f:\n",
    "    pickle.dump([TPs, FPs, FNs], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path + '/eval/metrics_test.pkl', 'rb') as f:\n",
    "    TPs, FPs, FNs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate raw metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = len(TPs)/(len(TPs)+len(FPs))\n",
    "recall = len(TPs)/(len(TPs)+len(FNs))\n",
    "accuracy = (len(TPs)-len(FPs) +len(neg_score))/(len(neg_score)+len(pos_score))\n",
    "print(precision, recall, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refinement based on element distances (eliminate predictions beyond a distance threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges_from_node_info(node):\n",
    "    c = np.array(node[1])\n",
    "    l = max(node[2])/2\n",
    "    d = np.array(node[3])\n",
    "\n",
    "    return ((c+l*d), (c-l*d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_predictions_fast(preds, point_info, node_info, dist_thresh=0.002, rough_dist_thresh=0.1):\n",
    "    refined_preds = []\n",
    "    discarded_preds = []\n",
    "    \n",
    "    for pair in tqdm(preds):\n",
    "        #rough distance check using two edges\n",
    "        bb0 = get_edges_from_node_info(node_info[pair[0]])\n",
    "        bb1 = get_edges_from_node_info(node_info[pair[1]])\n",
    "        if ((sq_dist_vect(bb0[0], bb1[0]) < rough_dist_thresh) or\n",
    "            (sq_dist_vect(bb0[0], bb1[1]) < rough_dist_thresh) or\n",
    "            (sq_dist_vect(bb0[1], bb1[0]) < rough_dist_thresh) or\n",
    "            (sq_dist_vect(bb0[1], bb1[1]) < rough_dist_thresh)):\n",
    "            \n",
    "            # slower, precise check using points\n",
    "            dist = np.min(distance.cdist(\n",
    "                point_info[pair[0]], point_info[pair[1]], 'sqeuclidean'))\n",
    "            if (dist < dist_thresh):\n",
    "                refined_preds.append(pair)\n",
    "            else:\n",
    "                discarded_preds.append(pair)\n",
    "                \n",
    "        else:\n",
    "            discarded_preds.append(pair)\n",
    "    return refined_preds, discarded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'eastdeckbox'\n",
    "#data_path = '../'\n",
    "node_file = \"nodes_\" + site + \".pkl\"\n",
    "with open(data_path + node_file, 'rb') as f:\n",
    "    node_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if positive predictions fall within distance threshold\n",
    "def check_predictions(preds, point_info, dist_thresh=0.002):\n",
    "    refined_preds = []\n",
    "    \n",
    "    for pair in tqdm(preds):\n",
    "        #print(len(point_info), pair[0], pair[1])        \n",
    "        dist = np.min(distance.cdist(\n",
    "            point_info[pair[0]], point_info[pair[1]], 'sqeuclidean'))\n",
    "        if (dist < dist_thresh):\n",
    "            refined_preds.append(pair)\n",
    "    return refined_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refined_TPs = check_predictions(TPs, node_info[1])\n",
    "# refined_FPs = check_predictions(FPs, node_info[1])\n",
    "\n",
    "refined_TPs, new_FNs = check_predictions_fast(TPs, node_info[1], node_info[0])\n",
    "refined_FPs, _ = check_predictions_fast(FPs, node_info[1], node_info[0])\n",
    "print(len(refined_TPs), len(refined_FPs), len(new_FNs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path + '/eval/refined_test.pkl', 'wb') as f:\n",
    "    pickle.dump([refined_TPs, refined_FPs], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path + '/eval/refined_test.pkl', 'rb') as f:\n",
    "    refined_TPs, refined_FPs = pickle.load(f)\n",
    "print(len(refined_TPs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = len(refined_TPs)/(len(refined_TPs)+len(refined_FPs))\n",
    "recall = len(refined_TPs)/(len(refined_TPs)+len(FNs)+len(new_FNs))\n",
    "accuracy = (len(refined_TPs)-len(refined_FPs) +len(neg_score))/(len(neg_score)+len(pos_score))\n",
    "print(precision, recall, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centerline_deviation(ad, bd):\n",
    "    centerline_deviation = np.arccos( np.dot(ad, bd))\n",
    "    #print(math.degrees(centerline_deviation), ad, bd)\n",
    "    if centerline_deviation > np.pi/2:\n",
    "        centerline_deviation = np.pi - centerline_deviation\n",
    "    return centerline_deviation\n",
    "\n",
    "\n",
    "def get_distance_to_intersection(a, b):\n",
    "    centerline_connecting_line = np.cross(a[3], b[3])\n",
    "    center_connecting_line = b[1] - a[1]\n",
    "    centerline_distance = (abs(np.dot(centerline_connecting_line, \n",
    "                                          center_connecting_line)) / \n",
    "                           vector_mag(centerline_connecting_line))\n",
    "    sq_mag_ccl = np.dot(centerline_connecting_line, \n",
    "                        centerline_connecting_line)\n",
    "    t1 = np.dot(np.cross(b[3], centerline_connecting_line),\n",
    "                center_connecting_line) / sq_mag_ccl\n",
    "    t2 = np.dot(np.cross(a[3], centerline_connecting_line),\n",
    "                center_connecting_line) / sq_mag_ccl\n",
    "    \n",
    "    return t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rdp(feat, e):\n",
    "    r = feat[1+e]\n",
    "    p = [feat[4+e*3], feat[5+e*3], feat[6+e*3]]\n",
    "    d = [feat[13+e*3], feat[14+e*3], feat[15+e*3]]\n",
    "    return r, d, p\n",
    "\n",
    "\n",
    "\n",
    "# TODO: scale thresholds with radius\n",
    "def connectivity_refinements(preds, node_info, params_path, dataset, radius_threshold, \n",
    "                             centerline_angle_threshold, centerline_dist_threshold):\n",
    "    # load predicted node params\n",
    "    features = get_node_features(node_info, params_path, dataset, [])\n",
    "    features = features.cpu().detach().numpy()\n",
    "    #rint(features.shape, max(np.array(preds)[:,0]), max(np.array(preds)[:,1]))\n",
    "    refined_preds = []\n",
    "    discarded_preds = []\n",
    "    r_count, dev_count, dist_count = 0, 0, 0\n",
    "    \n",
    "    for pair in tqdm(preds):\n",
    "        accepted = False\n",
    "        feat1 = features[pair[0]]\n",
    "        feat2 = features[pair[1]]\n",
    "        \n",
    "        # iterate through edges of first element\n",
    "        for e1 in range(3):\n",
    "            if accepted:\n",
    "                break\n",
    "            r1, d1, p1 = get_rdp(feat1, e1)\n",
    "            if r1 == 0.:\n",
    "                continue\n",
    "            \n",
    "            # iterate through edges of second element\n",
    "            for e2 in range(3):\n",
    "                if accepted:\n",
    "                    break\n",
    "                r2, d2, p2= get_rdp(feat2, e2)\n",
    "                if r2 == 0.:\n",
    "                    continue\n",
    "\n",
    "                # radius check\n",
    "                #print(r1, r2, r1/r2 ,r2/r1 )\n",
    "                if (r1/r2 > radius_threshold) and (r2/r1 > radius_threshold):\n",
    "                    r_count += 1\n",
    "\n",
    "                    # centerline direction check\n",
    "                    centerline_deviation = get_centerline_deviation(d1, d2)\n",
    "                    #print(centerline_deviation, centerline_angle_threshold)\n",
    "                    if centerline_deviation < centerline_angle_threshold:\n",
    "                        dev_count +=1\n",
    "\n",
    "                        #print (\"dists\", math.sqrt(sq_dist_vect(p1, p2)), (r1+r2))\n",
    "                        if math.sqrt(sq_dist_vect(p1, p2)) < (r1+r2):\n",
    "                            \n",
    "                        # centerline proximity check\n",
    "#                         tn1 = [0, np.array(p1), 0, np.array(d1)] # temp node feature\n",
    "#                         tn2 = [0, np.array(p2), 0, np.array(d2)] # temp node feature\n",
    "#                         centerline_distance = get_centerline_distance(tn1, tn2)\n",
    "#                         if centerline_distance < centerline_dist_threshold:\n",
    "                            \n",
    "                            # centerline co-planar check\n",
    "                            #print(\"dist\", get_distance_to_intersection(tn1, tn2))\n",
    "                            dist_count += 1\n",
    "                            refined_preds.append(pair)\n",
    "                            accepted = True\n",
    "        if not accepted:\n",
    "            discarded_preds.append(pair)\n",
    "    \n",
    "    print(r_count, dev_count, dist_count)\n",
    "    return refined_preds, discarded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "radius_threshold = 0.1\n",
    "centerline_angle_threshold = math.radians(90) # angle threshold for two pipes to be considered parallel\n",
    "centerline_dist_threshold = 0.01 # centerline distance threshold pipes to be to considered connected\n",
    "params_path = Path('output/east_ref/')\n",
    "dataset = \"east\"\n",
    "\n",
    "refined_refined_TPs, new_new_FNs = connectivity_refinements(refined_TPs, node_info, params_path, dataset, radius_threshold,\n",
    "                                              centerline_angle_threshold, centerline_dist_threshold)\n",
    "\n",
    "refined_refined_FPs, _ = connectivity_refinements(refined_FPs, node_info, params_path, dataset, radius_threshold,\n",
    "                                              centerline_angle_threshold, centerline_dist_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(refined_refined_TPs)/len(refined_TPs), len(refined_refined_FPs)/len(refined_FPs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refined metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = len(refined_refined_TPs)/(len(refined_refined_TPs)+len(refined_refined_FPs))\n",
    "recall = len(refined_refined_TPs)/(len(refined_refined_TPs)+len(FNs)+len(new_FNs)+len(new_new_FNs))\n",
    "accuracy = (len(refined_refined_TPs)-len(refined_refined_FPs) +len(neg_score))/(len(neg_score)+len(pos_score))\n",
    "print(precision, recall, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_score= pos_score[:int(len(pos_score)/10)]\n",
    "neg_score= pos_score[:int(len(neg_score)/10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.concatenate([pos_score, neg_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = 1/(1 + np.exp(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "RocCurveDisplay.from_predictions(labels, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifc = ifcopenshell.open(data_path+\"/east_merged.ifc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_guid = lambda: ifcopenshell.guid.compress(uuid.uuid1().hex)\n",
    "owner_history = ifc.by_type(\"IfcOwnerHistory\")[0]\n",
    "project = ifc.by_type(\"IfcProject\")[0]\n",
    "context = ifc.by_type(\"IfcGeometricRepresentationContext\")[0]\n",
    "floor = ifc.by_type(\"IfcBuildingStorey\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = ifc.createIfcColourRgb('red', Red=0.9, Green=0.0, Blue=0.0)\n",
    "green = ifc.createIfcColourRgb('green', Red=0.0, Green=0.9, Blue=0.0)\n",
    "yellow = ifc.createIfcColourRgb('yellow', Red=0.9, Green=0.9, Blue=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize results on ifc file\n",
    "def draw_predictions(preds, nodes, ifc, colour):\n",
    "    for pair in tqdm(preds):\n",
    "        element1 = ifc.by_id(nodes[pair[0]][4])\n",
    "        element1_name = element1.Name\n",
    "        element2 = ifc.by_id(nodes[pair[1]][4])\n",
    "        element2_name = element2.Name\n",
    "        \n",
    "        draw_relationship(element1_name, element1, element2_name, \n",
    "                          element2, ifc, floor, owner_history, context, colour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(refined_TPs), len(refined_FPs), len(FNs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_predictions(refined_TPs, node_info[0], ifc, green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_predictions(refined_FPs, node_info[0], ifc, yellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_predictions(FNs, node_info[0], ifc, red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifc.write(model_path + '/eval/pred_vis_test_ref.ifc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional refinements\n",
    "\n",
    "### remove repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the graph is bidirected, a single edge has two predictions. \n",
    "# This function removes repetitions in a single set of predictions.\n",
    "def remove_repetitions(preds):\n",
    "    non_rep = []\n",
    "    for i, pair in enumerate(tqdm(preds)):\n",
    "        found = False\n",
    "        for j, pair2 in enumerate(preds[i:]):\n",
    "            if pair[0] == pair2[1] and pair[1] == pair2[0]:\n",
    "                found = True\n",
    "                #break\n",
    "        if not found:\n",
    "            non_rep.append(pair)\n",
    "    \n",
    "    return non_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above, except for removing repetitions across two sets of predictions\n",
    "# ex. between true positives and false negatives\n",
    "def compare_preds(preds1, preds2):\n",
    "    for i in range(len(preds1)):\n",
    "        preds1[i].sort()\n",
    "    for i in range(len(preds2)):\n",
    "        preds2[i].sort()\n",
    "    non_rep = []\n",
    "    \n",
    "    for i, pair in enumerate(preds1):\n",
    "        found = False\n",
    "        for j, pair2 in enumerate(preds2):\n",
    "            if pair[0] == pair2[0] and pair[1] == pair2[1]:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            non_rep.append(pair)\n",
    "    \n",
    "    return non_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(refined_TPs))\n",
    "non_rep_TPs = remove_repetitions(refined_TPs)\n",
    "print(len(non_rep_TPs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(refined_FPs))\n",
    "non_rep_FPs = remove_repetitions(refined_FPs)\n",
    "print(len(non_rep_FPs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(FNs))\n",
    "non_rep_FNs = remove_repetitions(FNs)\n",
    "print(len(non_rep_FNs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_FNs = compare_preds(non_rep_FNs, non_rep_TPs)\n",
    "print(len(non_rep_FNs), len(refined_FNs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refined metrics after removing repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = len(non_rep_TPs)/(len(non_rep_TPs)+len(non_rep_FPs))\n",
    "recall = len(non_rep_TPs)/(len(non_rep_TPs)+len(non_rep_FNs))\n",
    "accuracy = (len(non_rep_TPs)-len(refined_FPs) +len(neg_score))/(len(neg_score)+len(pos_score))\n",
    "print(precision, recall, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path + '/eval/east_non_rep_test.pkl', 'wb') as f:\n",
    "    pickle.dump([non_rep_TPs, non_rep_FPs, non_rep_FNs], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate metrics for each element category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse dataset and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element type wise precision recall analysis\n",
    "def sort_type(preds, nodes):\n",
    "    bins = np.zeros([4,4])\n",
    "    for p in preds:\n",
    "        x=nodes[p[0]][0]\n",
    "        y= nodes[p[1]][0]\n",
    "        if x == 4:\n",
    "          x = 3\n",
    "        if y == 4:\n",
    "          y = 3\n",
    "        li = [x,y]\n",
    "        li.sort()\n",
    "        x,y = li[0],li[1]\n",
    "        \n",
    "        bins[x][y] += 1\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_bins = sort_type(non_rep_TPs,node_info[0] )\n",
    "fp_bins = sort_type(non_rep_FPs,node_info[0] )\n",
    "fn_bins = sort_type(non_rep_FNs,node_info[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tp_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp_bins/(tp_bins+fn_bins)\n",
    "precision = tp_bins/(tp_bins+fp_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1:\", (2*precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate element types in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse dataset\n",
    "\n",
    "# load data\n",
    "site = 'east'\n",
    "\n",
    "def analyse_dataset(site):\n",
    "  data_path = \"/content/drive/MyDrive/graph/\"\n",
    "  edge_file = \"edges_\" + site + \"deckbox.pkl\"\n",
    "  node_file = \"nodes_\" + site + \"deckbox.pkl\"\n",
    "  with open(data_path + node_file, 'rb') as f:\n",
    "      node_info = pickle.load(f)\n",
    "  with open(data_path + edge_file, 'rb') as f:\n",
    "      edges = pickle.load(f)\n",
    "\n",
    "  # get element type counts\n",
    "  labels = np.array([i[0] for i in node_info[0]])\n",
    "  unique, counts = np.unique(labels, return_counts=True)\n",
    "  print(dict(zip(unique, counts)))\n",
    "\n",
    "  # get connection counts\n",
    "  counts = np.zeros((5,5), dtype=int)\n",
    "\n",
    "  for edge in edges:\n",
    "    x = labels[edge[0]]\n",
    "    y = labels[edge[1]]\n",
    "    if x == 4:\n",
    "      x = 3\n",
    "    if y == 4:\n",
    "      y = 3\n",
    "    li = [x,y]\n",
    "    li.sort()\n",
    "    x,y = li[0],li[1]\n",
    "    counts[x][y] += 1\n",
    "\n",
    "  return(counts)\n",
    "\n",
    "c_east = analyse_dataset('east')\n",
    "c_west = analyse_dataset('west')\n",
    "count = c_east + c_west\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation v2\n",
    "\n",
    "The dataset must have shared element ids across both the pointclouds used for parameter inference and the graph node dataset. The steps for creating the new dataset are;\n",
    "1. create subsets of merged.ifc for the 4 classes (include bend in elbow) preserving element id\n",
    "2. run refinement to get refined_ifcs (manual deletion)\n",
    "3. generate pointcloud dataset from ifcs\n",
    "4. get the element ids of refined ifcs, compare with ids in the graph and delete extra nodes / edges to get refined graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create subsets of merged.ifc for the 4 classes (include bend in elbow) preserving element id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elements_from_ifc(ifc):\n",
    "    ifc_full = ifcopenshell.open(ifc)\n",
    "    selector = Selector()\n",
    "    element_type = 'IFCPIPEFITTING'\n",
    "    elements_fitting = selector.parse(ifc_full, '.' + element_type)\n",
    "    element_type = 'IFCPIPESEGMENT'\n",
    "    elements_segments = selector.parse(ifc_full, '.' + element_type)\n",
    "    elements = elements_segments + elements_fitting\n",
    "    print(len(elements))\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ifc\n",
    "elements = get_elements_from_ifc(data_path +\"east_merged.ifc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nodes\n",
    "with open(data_path + 'nodes_eastdeckbox.pkl', 'rb') as f:\n",
    "    node_info = pickle.load(f)\n",
    "    nodes = node_info[0]\n",
    "    \n",
    "print(len(nodes))\n",
    "\n",
    "# load edges\n",
    "with open(data_path + 'edges_eastdeckbox.pkl', 'rb') as f:\n",
    "    edges = pickle.load(f)\n",
    "    \n",
    "print(edges[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get matching elements between nodes and elements\n",
    "matching_elements = {}\n",
    "\n",
    "for i, el in enumerate(tqdm(elements)):\n",
    "    for j, node in enumerate(nodes):\n",
    "        if el.id() == node[4]:\n",
    "            matching_elements[str(j)] = i\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [b for b in nodes if b[0]]\n",
    "print(len(matching_elements), nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter by class\n",
    "#types = ['FLANGE', 'ELBOW', 'TEE', 'TUBE', 'BEND']\n",
    "types = ['BEND']\n",
    "combined_metadata = {}\n",
    "for k, tp in enumerate(types):\n",
    "    class_metadata = {}\n",
    "    \n",
    "    # setup new ifc\n",
    "    ifc = setup_ifc_file(blueprint)\n",
    "    owner_history = ifc.by_type(\"IfcOwnerHistory\")[0]\n",
    "    project = ifc.by_type(\"IfcProject\")[0]\n",
    "    context = ifc.by_type(\"IfcGeometricRepresentationContext\")[0]\n",
    "    floor = ifc.by_type(\"IfcBuildingStorey\")[0]\n",
    "    k = 4\n",
    "    # add element to new ifc\n",
    "    for i, nd in enumerate(tqdm(nodes)):\n",
    "        # check if class matches\n",
    "        #print(matching_elements[str(i)])\n",
    "        count = 0\n",
    "        if nd[0] == k:\n",
    "            el = elements[matching_elements[str(i)]]\n",
    "            new_id = ifc.add(el).id()\n",
    "            class_metadata[str(new_id)] = el.id()\n",
    "            #print(el.id(), new_id)\n",
    "            \n",
    "    combined_metadata[tp] = class_metadata\n",
    "    \n",
    "    # write ifc\n",
    "    tmp_ifc = os.path.join(temp_dir, tp+'.ifc')\n",
    "    ifc.write(tmp_ifc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(combined_metadata[\"BEND\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(temp_dir, 'id_metadata.json'), 'w')\n",
    "json.dump(combined_metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2 and 3 must be carried out using IFCPARSE script (or manually) and using element_to_cloud function (or using blender for tees)\n",
    "\n",
    "4. get the element ids of refined ifcs, compare with ids in the graph and delete extra nodes / edges to get refined graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify nodes which have been deleted\n",
    "total_ids = []\n",
    "for k, tp in enumerate(types):\n",
    "    elements = get_elements_from_ifc(os.path.join(temp_dir, tp+'_ref.ifc'))\n",
    "    element_ids = [combined_metadata[tp][str(el.id())] for el in elements]\n",
    "    total_ids += element_ids\n",
    "\n",
    "print(len(total_ids))\n",
    "missing_count = 0\n",
    "\n",
    "for nd in nodes:\n",
    "    if nd[4] not in total_ids:\n",
    "        missing_count += 1\n",
    "print(\"missing\", missing_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(total_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create modified graph with refined nodes\n",
    "new_nodes, new_node_points, new_edges = [], [], []\n",
    "id_dict = {}\n",
    "for i in range(len(nodes)):\n",
    "    id_dict[str(i)] = None\n",
    "    \n",
    "for i, nd in enumerate(nodes):\n",
    "    if nd[4] in total_ids:\n",
    "        new_nodes.append(nd)\n",
    "        new_node_points.append(node_info[1][i])\n",
    "        id_dict[str(i)] = len(new_nodes)-1\n",
    "        \n",
    "print(len(new_nodes), new_nodes[0])\n",
    "\n",
    "for e in edges:\n",
    "    if id_dict[str(e[0])] is not None and id_dict[str(e[1])] is not None:\n",
    "        new_edges.append((id_dict[str(e[0])], id_dict[str(e[1])]))\n",
    "        \n",
    "print(len(new_edges), new_edges[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(temp_dir, 'nodes_westdeckbox_ref.pkl'), 'wb') as f:\n",
    "    pickle.dump([new_nodes, new_node_points], f)\n",
    "    \n",
    "with open(os.path.join(temp_dir, 'edges_westdeckbox_ref.pkl'), 'wb') as f:\n",
    "    pickle.dump(new_edges, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
