{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpvL68OfBEQC"
   },
   "source": [
    "# Element Parameter Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z7n_pw4SMWl"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ47VNF7fmTS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import interact \n",
    "import gc\n",
    "import open3d as o3d\n",
    "\n",
    "from src.elements import *\n",
    "from src.ifc import *\n",
    "from src.preparation import *\n",
    "from src.visualisation import *\n",
    "from src.chamfer import *\n",
    "from src.utils import *\n",
    "from src.morph import *\n",
    "\n",
    "random.seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.ops import utils as pytorch3d_utils\n",
    "from pytorch3d.ops.points_normals import _disambiguate_vector_directions\n",
    "from pytorch3d.ops.knn import knn_points\n",
    "from typing import Tuple, TYPE_CHECKING, Union\n",
    "from pytorch3d.common.workaround import symeig3x3\n",
    "\n",
    "\n",
    "def get_point_covariances_relative(\n",
    "    points_padded: torch.Tensor,\n",
    "    targets_padded: torch.Tensor,\n",
    "    num_points_per_cloud: torch.Tensor,\n",
    "    neighborhood_size: int,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Computes the per-point covariance matrices by of the 3D locations of\n",
    "    K-nearest neighbors of each point.\n",
    "\n",
    "    Args:\n",
    "        **points_padded**: Input point clouds as a padded tensor\n",
    "            of shape `(minibatch, num_points, dim)`.\n",
    "        **num_points_per_cloud**: Number of points per cloud\n",
    "            of shape `(minibatch,)`.\n",
    "        **neighborhood_size**: Number of nearest neighbors for each point\n",
    "            used to estimate the covariance matrices.\n",
    "\n",
    "    Returns:\n",
    "        **covariances**: A batch of per-point covariance matrices\n",
    "            of shape `(minibatch, dim, dim)`.\n",
    "        **k_nearest_neighbors**: A batch of `neighborhood_size` nearest\n",
    "            neighbors for each of the point cloud points\n",
    "            of shape `(minibatch, num_points, neighborhood_size, dim)`.\n",
    "    \"\"\"\n",
    "    # get K nearest neighbor idx for each point in the point cloud\n",
    "    k_nearest_neighbors = knn_points(\n",
    "        targets_padded,\n",
    "        points_padded,\n",
    "        lengths1=num_points_per_cloud,\n",
    "        lengths2=num_points_per_cloud,\n",
    "        K=neighborhood_size,\n",
    "        return_nn=True,\n",
    "    ).knn\n",
    "    # obtain the mean of the neighborhood\n",
    "    pt_mean = k_nearest_neighbors.mean(2, keepdim=True)\n",
    "    # compute the diff of the neighborhood and the mean of the neighborhood\n",
    "    central_diff = k_nearest_neighbors - pt_mean\n",
    "    # per-nn-point covariances\n",
    "    per_pt_cov = central_diff.unsqueeze(4) * central_diff.unsqueeze(3)\n",
    "    # per-point covariances\n",
    "    covariances = per_pt_cov.mean(2)\n",
    "\n",
    "    return covariances, k_nearest_neighbors\n",
    "\n",
    "\n",
    "# calculate eigen values and eigen vectors relative to points from a second point cloud\n",
    "def estimate_pointcloud_local_coord_frames_relative(\n",
    "    pointclouds: Union[torch.Tensor, \"Pointclouds\"],\n",
    "    targets: Union[torch.Tensor, \"Pointclouds\"],\n",
    "    neighborhood_size: int = 50,\n",
    "    disambiguate_directions: bool = True,\n",
    "    *,\n",
    "    use_symeig_workaround: bool = True,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Estimates the principal directions of curvature (which includes normals)\n",
    "    of a batch of `pointclouds`.\n",
    "\n",
    "    The algorithm first finds `neighborhood_size` nearest neighbors for each\n",
    "    point of the point clouds, followed by obtaining principal vectors of\n",
    "    covariance matrices of each of the point neighborhoods.\n",
    "    The main principal vector corresponds to the normals, while the\n",
    "    other 2 are the direction of the highest curvature and the 2nd highest\n",
    "    curvature.\n",
    "\n",
    "    Note that each principal direction is given up to a sign. Hence,\n",
    "    the function implements `disambiguate_directions` switch that allows\n",
    "    to ensure consistency of the sign of neighboring normals. The implementation\n",
    "    follows the sign disabiguation from SHOT descriptors [1].\n",
    "\n",
    "    The algorithm also returns the curvature values themselves.\n",
    "    These are the eigenvalues of the estimated covariance matrices\n",
    "    of each point neighborhood.\n",
    "\n",
    "    Args:\n",
    "      **pointclouds**: Batch of 3-dimensional points of shape\n",
    "        `(minibatch, num_point, 3)` or a `Pointclouds` object.\n",
    "      **neighborhood_size**: The size of the neighborhood used to estimate the\n",
    "        geometry around each point.\n",
    "      **disambiguate_directions**: If `True`, uses the algorithm from [1] to\n",
    "        ensure sign consistency of the normals of neighboring points.\n",
    "      **use_symeig_workaround**: If `True`, uses a custom eigenvalue\n",
    "        calculation.\n",
    "\n",
    "    Returns:\n",
    "      **curvatures**: The three principal curvatures of each point\n",
    "        of shape `(minibatch, num_point, 3)`.\n",
    "        If `pointclouds` are of `Pointclouds` class, returns a padded tensor.\n",
    "      **local_coord_frames**: The three principal directions of the curvature\n",
    "        around each point of shape `(minibatch, num_point, 3, 3)`.\n",
    "        The principal directions are stored in columns of the output.\n",
    "        E.g. `local_coord_frames[i, j, :, 0]` is the normal of\n",
    "        `j`-th point in the `i`-th pointcloud.\n",
    "        If `pointclouds` are of `Pointclouds` class, returns a padded tensor.\n",
    "\n",
    "    References:\n",
    "      [1] Tombari, Salti, Di Stefano: Unique Signatures of Histograms for\n",
    "      Local Surface Description, ECCV 2010.\n",
    "    \"\"\"\n",
    "\n",
    "    points_padded, num_points = pytorch3d_utils.convert_pointclouds_to_tensor(pointclouds)\n",
    "    targets_padded, target_num_points = pytorch3d_utils.convert_pointclouds_to_tensor(targets)\n",
    "\n",
    "    ba, N, dim = points_padded.shape\n",
    "    if dim != 3:\n",
    "        raise ValueError(\n",
    "            \"The pointclouds argument has to be of shape (minibatch, N, 3)\"\n",
    "        )\n",
    "\n",
    "    if (num_points <= neighborhood_size).any():\n",
    "        raise ValueError(\n",
    "            \"The neighborhood_size argument has to be\"\n",
    "            + \" >= size of each of the point clouds.\"\n",
    "        )\n",
    "\n",
    "    # undo global mean for stability\n",
    "    # TODO: replace with tutil.wmean once landed\n",
    "    pcl_mean = points_padded.sum(1) / num_points[:, None]\n",
    "    points_centered = points_padded - pcl_mean[:, None, :]\n",
    "    targets_centered = targets_padded - pcl_mean[:, None, :]\n",
    "\n",
    "    # get the per-point covariance and nearest neighbors used to compute it\n",
    "    cov, knns = get_point_covariances_relative(points_centered, targets_centered, num_points, neighborhood_size)\n",
    "\n",
    "    # get the local coord frames as principal directions of\n",
    "    # the per-point covariance\n",
    "    # this is done with torch.symeig / torch.linalg.eigh, which returns the\n",
    "    # eigenvectors (=principal directions) in an ascending order of their\n",
    "    # corresponding eigenvalues, and the smallest eigenvalue's eigenvector\n",
    "    # corresponds to the normal direction; or with a custom equivalent.\n",
    "    if use_symeig_workaround:\n",
    "        curvatures, local_coord_frames = symeig3x3(cov, eigenvectors=True)\n",
    "    else:\n",
    "        curvatures, local_coord_frames = torch.linalg.eigh(cov)\n",
    "\n",
    "    # disambiguate the directions of individual principal vectors\n",
    "    if disambiguate_directions:\n",
    "        # disambiguate normal\n",
    "        n = _disambiguate_vector_directions(\n",
    "            points_centered, knns, local_coord_frames[:, :, :, 0]\n",
    "        )\n",
    "        # disambiguate the main curvature\n",
    "        z = _disambiguate_vector_directions(\n",
    "            points_centered, knns, local_coord_frames[:, :, :, 2]\n",
    "        )\n",
    "        # the secondary curvature is just a cross between n and z\n",
    "        y = torch.cross(n, z, dim=2)\n",
    "        # cat to form the set of principal directions\n",
    "        local_coord_frames = torch.stack((n, y, z), dim=3)\n",
    "\n",
    "    return curvatures, local_coord_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sphere morphing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise a list of point clouds as an animation using open3d\n",
    "# use ctrl+c to copy and ctrl+v to set camera and zoom inside visualiser\n",
    "def create_point_cloud_animation(cloud_list, loss_func, save_image=False, colours=None):\n",
    "    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Debug)\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    cloud = cloud_list[0]\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(cloud)\n",
    "    if colours is not None:\n",
    "        point_cloud.colors = o3d.utility.Vector3dVector(colours[0])\n",
    "    vis.add_geometry(point_cloud)\n",
    "    stops = [9,39,99,299,999]\n",
    "\n",
    "    for i in range(len(cloud_list)):\n",
    "        time.sleep(0.01 + 0.05/(i/10+1))\n",
    "        cloud = cloud_list[i]\n",
    "        point_cloud.points = o3d.utility.Vector3dVector(cloud)\n",
    "        if colours is not None:\n",
    "            point_cloud.colors = o3d.utility.Vector3dVector(colours[i])\n",
    "        vis.update_geometry(point_cloud)\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "        if save_image and i in stops:\n",
    "            vis.capture_screen_image(\"sphere/\" + loss_func + str(i) + \".jpg\", do_render=True)\n",
    "    vis.destroy_window()\n",
    "\n",
    "    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cld1_name = \"sphere/chair.pcd\"\n",
    "# loss_func = \"chamfer\"\n",
    "# run_morph(cld1_name, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.ops import points_normals\n",
    "from eindex import eindex\n",
    "import einops\n",
    "\n",
    "# calculate chamfer loss based on local curvature\n",
    "def calc_curvature_loss_tensor(x, y, k=32, return_assignment=False):\n",
    "    chamferDist = ChamferDistance()\n",
    "    eps = 0.00001\n",
    "\n",
    "    # add a loss term for mismatched pairs\n",
    "    nn = chamferDist(\n",
    "        x, y, bidirectional=True, return_nn=True\n",
    "    )\n",
    "\n",
    "    eig_vals_x, eig_vects_x = estimate_pointcloud_local_coord_frames_relative(\n",
    "        x, y, neighborhood_size=k)\n",
    "\n",
    "    eig_vals_y, eig_vects_y = estimate_pointcloud_local_coord_frames_relative(\n",
    "        y, x, neighborhood_size=k)\n",
    "\n",
    "    corresponding_y_vals = eindex(eig_vals_y, torch.squeeze(nn[0].idx, dim=-1), \"batch [batch points] eigenvalues\")\n",
    "    corresponding_x_vals = eindex(eig_vals_x, torch.squeeze(nn[1].idx, dim=-1), \"batch [batch points] eigenvalues\")\n",
    "    \n",
    "    corresponding_y_vects = eindex(eig_vects_y, torch.squeeze(nn[0].idx, dim=-1), \"batch [batch points] eigenvalues eigenvects\")\n",
    "    corresponding_x_vects = eindex(eig_vects_x, torch.squeeze(nn[1].idx, dim=-1), \"batch [batch points] eigenvalues eigenvects\")\n",
    "    #print(\"x\", x.shape, nn[0].dists.shape, nn[0].dists.shape, torch.squeeze(nn[1].idx, dim=-1).shape)\n",
    "    \n",
    "    #print(\"sample eigen\", eig_vects_x[0,:4,:,:], eig_vals_x[0,:4])\n",
    "\n",
    "    eigen_val_dist_y = torch.sum(torch.square(eig_vals_x - corresponding_y_vals))\n",
    "    eigen_val_dist_x = torch.sum(torch.square(eig_vals_y - corresponding_x_vals))\n",
    "    \n",
    "    # calculate dot product between eigenvectors\n",
    "    dot_product_y = einops.einsum(eig_vects_x, corresponding_y_vects, \"b n p q, b n p q -> b n p\")\n",
    "    # ignore direction and take absolute value\n",
    "    dot_product_y = torch.sum(1. - torch.abs(dot_product_y))\n",
    "    dot_product_x = einops.einsum(eig_vects_y, corresponding_x_vects, \"b n p q, b n p q -> b n p\")\n",
    "    print(\"dot\", (1. - torch.abs(dot_product_x))[0,:3])\n",
    "    # ignore direction and take absolute value\n",
    "    dot_product_x = torch.sum(1. - torch.abs(dot_product_x))\n",
    "    \n",
    "\n",
    "    #corresponding_y_points = eindex(x, torch.squeeze(nn[1].idx, dim=-1), \"batch [batch points] xyz\")\n",
    "    #print(\"corresponding shape\", corresponding_y_vals.shape, corresponding_y_vects.shape)\n",
    "    #print(\"mean\", eigen_val_dist_x)\n",
    "    \n",
    "    #curvature_loss = (dot_product_x + dot_product_y)/1000 + (eigen_val_dist_x + eigen_val_dist_y)*1000\n",
    "    curvature_loss = (dot_product_x + dot_product_y)/1000\n",
    "    #print(\"curvature loss\", curvature_loss)\n",
    "\n",
    "    #print(\"eig_vals_x\", eig_vals_x.shape, eig_vects_x.shape, \"eig_vals_y\", eig_vals_y.shape, eig_vects_y.shape)\n",
    "\n",
    "    bidirectional_dist = torch.sum(nn[0].dists[:,:,0]) + torch.sum(nn[1].dists[:, :, 0])\n",
    "    print(\"d\", torch.sum(nn[1].dists[:,:,0]).item(), torch.sum(nn[0].dists[:,:,0]).item())\n",
    "\n",
    "    return curvature_loss + bidirectional_dist, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld1_name = \"sphere/plane1.pcd\"\n",
    "cld2_name = \"sphere/chair.pcd\"\n",
    "\n",
    "cuda = torch.device(\"cuda\")\n",
    "cld1 = np.array(o3d.io.read_point_cloud(cld1_name).points)\n",
    "cld2 = np.array(o3d.io.read_point_cloud(cld2_name).points)\n",
    "pcd1_tensor = torch.tensor([cld1], device=cuda)\n",
    "pcd2_tensor = torch.tensor([cld2], device=cuda)\n",
    "\n",
    "l = calc_curvature_loss_tensor(pcd1_tensor, pcd2_tensor)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# visualise animation\n",
    "cld1_name = \"sphere/plane1.pcd\"\n",
    "visualise = True\n",
    "#loss_funcs = [ \"chamfer\", \"emd\", \"balanced\", \"reverse\", \"single\"]\n",
    "#loss_funcs = [ \"balanced\", \"single\"]\n",
    "loss_funcs = [ \"curvature\"]\n",
    "for loss_func in loss_funcs:\n",
    "    print(loss_func)\n",
    "    run_morph(cld1_name, loss_func, lr=.4)\n",
    "    if visualise:\n",
    "        with open(\"sphere/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "            morphed = pickle.load(f)\n",
    "        colours = visualise_density(morphed, 'plasma_r')\n",
    "        with open(\"sphere/\" + loss_func + \"_dens.pkl\", \"wb\") as f:\n",
    "            pickle.dump(colours, f)\n",
    "        #create_point_cloud_animation(cloud_list, loss_func)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def view_density(loss_func): \n",
    "    with open(\"sphere/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "        morphed = pickle.load(f)\n",
    "    with open(\"sphere/\" + loss_func + \"_dens.pkl\", \"rb\") as f:\n",
    "        colours = pickle.load(f)\n",
    "\n",
    "    create_point_cloud_animation(morphed, loss_func, True, colours[:,:,:3])\n",
    "\n",
    "interact(view_density, loss_func=[\"curvature\", \"density\", \"balanced\", \"infocd\", \"single\", \"chamfer\", \"reverse\", \"emd\", \"direct\"]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# visualise animation\n",
    "loss_func = \"emd\"\n",
    "with open(\"sphere/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "    morphed = pickle.load(f)\n",
    "print(morphed.shape, loss_func)\n",
    "cloud_list = [m for m in morphed]\n",
    "#create_point_cloud_animation(cloud_list, loss_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = visualise_density(morphed, 'plasma_r')\n",
    "with open(\"sphere/\" + loss_func + \"_dens.pkl\", \"wb\") as f:\n",
    "    pickle.dump(colours, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample\n",
    "shapenet_path = \"../experiments/ICCV2023-HyperCD/ShapeNetCompletion/test/complete/\"\n",
    "downsampled_path = \"../experiments/ICCV2023-HyperCD/ShapeNetCompletion/downsample/\"\n",
    "# folders = os.listdir(shapenet_path)\n",
    "# choices = np.random.choice(len(points), 4096)\n",
    "\n",
    "# for fl in tqdm(folders):\n",
    "#     if not os.path.exists(downsampled_path+fl):\n",
    "#         os.mkdir(downsampled_path+fl)\n",
    "        \n",
    "#     files = os.listdir(shapenet_path + fl)\n",
    "#     cloud =  o3d.geometry.PointCloud()\n",
    "#     for cl in files:\n",
    "#         points = np.array(o3d.io.read_point_cloud(shapenet_path + fl + \"/\" + cl).points)\n",
    "#         points = points[choices]\n",
    "#         cloud.points = o3d.utility.Vector3dVector(points)\n",
    "#         o3d.io.write_point_cloud(downsampled_path+fl + \"/\" + cl, cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_func = \"balanced\"\n",
    "\n",
    "cd, emd = sphere_morph_metrics(loss_func, downsampled_path, save=False)\n",
    "print(cd[-1], emd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = \"balanced\"\n",
    "#loss_func = \"emd\"\n",
    "\n",
    "sphere_morph_metrics(loss_func, downsampled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses on same axis\n",
    "def plot_losses(losses, labels, title):\n",
    "    x = np.arange(0, len(losses[0]))\n",
    "    plt.figure(figsize=(30, 6))\n",
    "    for i, loss in enumerate(losses):\n",
    "        plt.plot(x, loss, label=labels[i])\n",
    "\n",
    "    plt.xlabel(\"point cloud index\")\n",
    "    plt.ylabel(\"distance\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plots\n",
    "loss_funcs = [\"chamfer\",]\n",
    "chamfer_list, emd_list = [], []\n",
    "for loss_func in loss_funcs:\n",
    "    with open(\"sphere/\" + loss_func + \"_metrics.pkl\", \"rb\") as f:\n",
    "        chamfer, emd, assignments = pickle.load(f)\n",
    "        chamfer_list.append(chamfer)\n",
    "        emd_list.append(emd)\n",
    "        \n",
    "plot_losses(chamfer_list, loss_funcs, \"chamfer\")\n",
    "\n",
    "ts(emd_list, loss_funcs, \"EMD\")\n",
    "        \n",
    "print(emd_list[-1], chamfer_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load losses\n",
    "loss_types = [\"reverse\", \"chamfer\", \"emd\", \"pair\"]\n",
    "losses = []\n",
    "\n",
    "for loss_func in loss_types:\n",
    "    with open(\"sphere/loss_\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "        losses.append(pickle.load(f))\n",
    "        \n",
    "plot_losses(losses, loss_types, \"loss function comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure conssistency between forward and backward correspondences for chamfer distance\n",
    "# optionally compare against the ideal assignment, as measured by EMD\n",
    "def measure_assignment_consistency(assignment, emd=None):\n",
    "    reverse_assignment = torch.gather(assignment[0], 0, assignment[1])\n",
    "    expected = torch.arange(assignment[0].shape[0], device=torch.device(\"cuda\"))\n",
    "    consistency = torch.sum(torch.eq(expected, reverse_assignment).long())\n",
    "    print(\"consistency\", consistency.item(), len(torch.unique(assignment[0])), len(torch.unique(assignment[1])))\n",
    "    \n",
    "    if emd is not None:\n",
    "        #print(emd[:5], assignment[0][:5], assignment[1][:5])\n",
    "        emd_consistency = torch.sum(torch.eq(emd, assignment[0]).long())\n",
    "        #print(\"emd_consistency\", emd_consistency.item(), len(torch.unique(emd)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare correspondences\n",
    "#TODO: include consistency in top5 matches?\n",
    "loss_func = \"emd\"\n",
    "with open(\"sphere/assignments_\" + \"emd\" + \".pkl\", \"rb\") as f:\n",
    "    emd_assignment = pickle.load(f)\n",
    "\n",
    "with open(\"sphere/assignments_\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "    assignment = pickle.load(f)\n",
    "    \n",
    "for i, ass in enumerate(assignment):\n",
    "    #measure_assignment_consistency(ass, emd_assignment[i][0])\n",
    "    measure_assignment_consistency(ass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the morphed middle ground between two different clouds\n",
    "cld1_name = \"sphere/plane1.pcd\"\n",
    "cld2_name = \"sphere/chair.pcd\"\n",
    "\n",
    "cuda = torch.device(\"cuda\")\n",
    "cld1 = np.array(o3d.io.read_point_cloud(cld1_name).points)\n",
    "cld2 = np.array(o3d.io.read_point_cloud(cld2_name).points)\n",
    "pcd1_tensor = torch.tensor([cld1], device=cuda)\n",
    "pcd2_tensor = torch.tensor([cld2], device=cuda)\n",
    "\n",
    "print(pcd1_tensor.shape, pcd2_tensor.shape)\n",
    "loss, assignment = calc_emd(pcd1_tensor, pcd2_tensor, 0.05, 50)\n",
    "assignment = assignment.detach().cpu().numpy()\n",
    "print(assignment.shape)\n",
    "\n",
    "matched_points = cld2[assignment[0]]\n",
    "print(matched_points.shape)\n",
    "\n",
    "morphed_points = (cld1 + matched_points)/2\n",
    "morphed_cloud =  o3d.geometry.PointCloud()\n",
    "morphed_cloud.points = o3d.utility.Vector3dVector(morphed_points)\n",
    "o3d.io.write_point_cloud(\"sphere/chair_plane_morph.pcd\", morphed_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
