{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpvL68OfBEQC"
   },
   "source": [
    "# Element Parameter Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z7n_pw4SMWl"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ47VNF7fmTS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import interact \n",
    "import gc\n",
    "import open3d as o3d\n",
    "\n",
    "from src.elements import *\n",
    "from src.ifc import *\n",
    "from src.preparation import *\n",
    "from src.visualisation import *\n",
    "from src.chamfer import *\n",
    "from src.utils import *\n",
    "from src.morph import *\n",
    "\n",
    "random.seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install compas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.ops import utils as pytorch3d_utils\n",
    "from pytorch3d.ops.points_normals import _disambiguate_vector_directions\n",
    "from pytorch3d.ops.knn import knn_points\n",
    "from typing import Tuple, TYPE_CHECKING, Union\n",
    "from pytorch3d.common.workaround import symeig3x3\n",
    "\n",
    "\n",
    "def get_point_covariances_relative(\n",
    "    points_padded: torch.Tensor,\n",
    "    targets_padded: torch.Tensor,\n",
    "    num_points_per_cloud: torch.Tensor,\n",
    "    neighborhood_size: int,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Computes the per-point covariance matrices by of the 3D locations of\n",
    "    K-nearest neighbors of each point.\n",
    "\n",
    "    Args:\n",
    "        **points_padded**: Input point clouds as a padded tensor\n",
    "            of shape `(minibatch, num_points, dim)`.\n",
    "        **num_points_per_cloud**: Number of points per cloud\n",
    "            of shape `(minibatch,)`.\n",
    "        **neighborhood_size**: Number of nearest neighbors for each point\n",
    "            used to estimate the covariance matrices.\n",
    "\n",
    "    Returns:\n",
    "        **covariances**: A batch of per-point covariance matrices\n",
    "            of shape `(minibatch, dim, dim)`.\n",
    "        **k_nearest_neighbors**: A batch of `neighborhood_size` nearest\n",
    "            neighbors for each of the point cloud points\n",
    "            of shape `(minibatch, num_points, neighborhood_size, dim)`.\n",
    "    \"\"\"\n",
    "    # get K nearest neighbor idx for each point in the point cloud\n",
    "    k_nearest_neighbors = knn_points(\n",
    "        targets_padded,\n",
    "        points_padded,\n",
    "        lengths1=num_points_per_cloud,\n",
    "        lengths2=num_points_per_cloud,\n",
    "        K=neighborhood_size,\n",
    "        return_nn=True,\n",
    "    ).knn\n",
    "    # obtain the mean of the neighborhood\n",
    "    pt_mean = k_nearest_neighbors.mean(2, keepdim=True)\n",
    "    # compute the diff of the neighborhood and the mean of the neighborhood\n",
    "    central_diff = k_nearest_neighbors - pt_mean\n",
    "    # per-nn-point covariances\n",
    "    per_pt_cov = central_diff.unsqueeze(4) * central_diff.unsqueeze(3)\n",
    "    # per-point covariances\n",
    "    covariances = per_pt_cov.mean(2)\n",
    "\n",
    "    return covariances, k_nearest_neighbors\n",
    "\n",
    "\n",
    "# calculate eigen values and eigen vectors relative to points from a second point cloud\n",
    "def estimate_pointcloud_local_coord_frames_relative(\n",
    "    pointclouds: Union[torch.Tensor, \"Pointclouds\"],\n",
    "    targets: Union[torch.Tensor, \"Pointclouds\"],\n",
    "    neighborhood_size: int = 50,\n",
    "    disambiguate_directions: bool = True,\n",
    "    *,\n",
    "    use_symeig_workaround: bool = True,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Estimates the principal directions of curvature (which includes normals)\n",
    "    of a batch of `pointclouds`.\n",
    "\n",
    "    The algorithm first finds `neighborhood_size` nearest neighbors for each\n",
    "    point of the point clouds, followed by obtaining principal vectors of\n",
    "    covariance matrices of each of the point neighborhoods.\n",
    "    The main principal vector corresponds to the normals, while the\n",
    "    other 2 are the direction of the highest curvature and the 2nd highest\n",
    "    curvature.\n",
    "\n",
    "    Note that each principal direction is given up to a sign. Hence,\n",
    "    the function implements `disambiguate_directions` switch that allows\n",
    "    to ensure consistency of the sign of neighboring normals. The implementation\n",
    "    follows the sign disabiguation from SHOT descriptors [1].\n",
    "\n",
    "    The algorithm also returns the curvature values themselves.\n",
    "    These are the eigenvalues of the estimated covariance matrices\n",
    "    of each point neighborhood.\n",
    "\n",
    "    Args:\n",
    "      **pointclouds**: Batch of 3-dimensional points of shape\n",
    "        `(minibatch, num_point, 3)` or a `Pointclouds` object.\n",
    "      **neighborhood_size**: The size of the neighborhood used to estimate the\n",
    "        geometry around each point.\n",
    "      **disambiguate_directions**: If `True`, uses the algorithm from [1] to\n",
    "        ensure sign consistency of the normals of neighboring points.\n",
    "      **use_symeig_workaround**: If `True`, uses a custom eigenvalue\n",
    "        calculation.\n",
    "\n",
    "    Returns:\n",
    "      **curvatures**: The three principal curvatures of each point\n",
    "        of shape `(minibatch, num_point, 3)`.\n",
    "        If `pointclouds` are of `Pointclouds` class, returns a padded tensor.\n",
    "      **local_coord_frames**: The three principal directions of the curvature\n",
    "        around each point of shape `(minibatch, num_point, 3, 3)`.\n",
    "        The principal directions are stored in columns of the output.\n",
    "        E.g. `local_coord_frames[i, j, :, 0]` is the normal of\n",
    "        `j`-th point in the `i`-th pointcloud.\n",
    "        If `pointclouds` are of `Pointclouds` class, returns a padded tensor.\n",
    "\n",
    "    References:\n",
    "      [1] Tombari, Salti, Di Stefano: Unique Signatures of Histograms for\n",
    "      Local Surface Description, ECCV 2010.\n",
    "    \"\"\"\n",
    "\n",
    "    points_padded, num_points = pytorch3d_utils.convert_pointclouds_to_tensor(pointclouds)\n",
    "    targets_padded, target_num_points = pytorch3d_utils.convert_pointclouds_to_tensor(targets)\n",
    "\n",
    "    ba, N, dim = points_padded.shape\n",
    "    if dim != 3:\n",
    "        raise ValueError(\n",
    "            \"The pointclouds argument has to be of shape (minibatch, N, 3)\"\n",
    "        )\n",
    "\n",
    "    if (num_points <= neighborhood_size).any():\n",
    "        raise ValueError(\n",
    "            \"The neighborhood_size argument has to be\"\n",
    "            + \" >= size of each of the point clouds.\"\n",
    "        )\n",
    "\n",
    "    # undo global mean for stability\n",
    "    # TODO: replace with tutil.wmean once landed\n",
    "    pcl_mean = points_padded.sum(1) / num_points[:, None]\n",
    "    points_centered = points_padded - pcl_mean[:, None, :]\n",
    "    targets_centered = targets_padded - pcl_mean[:, None, :]\n",
    "\n",
    "    # get the per-point covariance and nearest neighbors used to compute it\n",
    "    cov, knns = get_point_covariances_relative(points_centered, targets_centered, num_points, neighborhood_size)\n",
    "\n",
    "    # get the local coord frames as principal directions of\n",
    "    # the per-point covariance\n",
    "    # this is done with torch.symeig / torch.linalg.eigh, which returns the\n",
    "    # eigenvectors (=principal directions) in an ascending order of their\n",
    "    # corresponding eigenvalues, and the smallest eigenvalue's eigenvector\n",
    "    # corresponds to the normal direction; or with a custom equivalent.\n",
    "    if use_symeig_workaround:\n",
    "        curvatures, local_coord_frames = symeig3x3(cov, eigenvectors=True)\n",
    "    else:\n",
    "        curvatures, local_coord_frames = torch.linalg.eigh(cov)\n",
    "\n",
    "    # disambiguate the directions of individual principal vectors\n",
    "    if disambiguate_directions:\n",
    "        # disambiguate normal\n",
    "        n = _disambiguate_vector_directions(\n",
    "            points_centered, knns, local_coord_frames[:, :, :, 0]\n",
    "        )\n",
    "        # disambiguate the main curvature\n",
    "        z = _disambiguate_vector_directions(\n",
    "            points_centered, knns, local_coord_frames[:, :, :, 2]\n",
    "        )\n",
    "        # the secondary curvature is just a cross between n and z\n",
    "        y = torch.cross(n, z, dim=2)\n",
    "        # cat to form the set of principal directions\n",
    "        local_coord_frames = torch.stack((n, y, z), dim=3)\n",
    "\n",
    "    return curvatures, local_coord_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sphere morphing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise a list of point clouds as an animation using open3d\n",
    "# use ctrl+c to copy and ctrl+v to set camera and zoom inside visualiser\n",
    "def create_point_cloud_animation(cloud_list, loss_func, save_image=False, colours=None):\n",
    "    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Debug)\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    cloud = cloud_list[0]\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(cloud)\n",
    "    if colours is not None:\n",
    "        point_cloud.colors = o3d.utility.Vector3dVector(colours[0])\n",
    "    vis.add_geometry(point_cloud)\n",
    "    stops = [9,39,99,299,999]\n",
    "\n",
    "    for i in range(len(cloud_list)):\n",
    "        time.sleep(0.01 + 0.05/(i/10+1))\n",
    "        cloud = cloud_list[i]\n",
    "        point_cloud.points = o3d.utility.Vector3dVector(cloud)\n",
    "        if colours is not None:\n",
    "            point_cloud.colors = o3d.utility.Vector3dVector(colours[i])\n",
    "        vis.update_geometry(point_cloud)\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "        if save_image and i in stops:\n",
    "            vis.capture_screen_image(\"sphere/\" + loss_func + str(i) + \".jpg\", do_render=True)\n",
    "    vis.destroy_window()\n",
    "\n",
    "    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cld1_name = \"sphere/chair.pcd\"\n",
    "# loss_func = \"chamfer\"\n",
    "# run_morph(cld1_name, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.ops import points_normals\n",
    "from eindex import eindex\n",
    "import einops\n",
    "\n",
    "# calculate chamfer loss based on local curvature\n",
    "def calc_curvature_loss_tensor(x, y, k=32, return_assignment=False):\n",
    "    chamferDist = ChamferDistance()\n",
    "    eps = 0.00001\n",
    "\n",
    "    # add a loss term for mismatched pairs\n",
    "    nn = chamferDist(\n",
    "        x, y, bidirectional=True, return_nn=True\n",
    "    )\n",
    "\n",
    "    eig_vals_x, eig_vects_x = estimate_pointcloud_local_coord_frames_relative(\n",
    "        x, y, neighborhood_size=k)\n",
    "\n",
    "    eig_vals_y, eig_vects_y = estimate_pointcloud_local_coord_frames_relative(\n",
    "        y, x, neighborhood_size=k)\n",
    "\n",
    "    corresponding_y_vals = eindex(eig_vals_y, torch.squeeze(nn[0].idx, dim=-1), \"batch [batch points] eigenvalues\")\n",
    "    corresponding_x_vals = eindex(eig_vals_x, torch.squeeze(nn[1].idx, dim=-1), \"batch [batch points] eigenvalues\")\n",
    "    \n",
    "    corresponding_y_vects = eindex(eig_vects_y, torch.squeeze(nn[0].idx, dim=-1), \"batch [batch points] eigenvalues eigenvects\")\n",
    "    corresponding_x_vects = eindex(eig_vects_x, torch.squeeze(nn[1].idx, dim=-1), \"batch [batch points] eigenvalues eigenvects\")\n",
    "    #print(\"x\", x.shape, nn[0].dists.shape, nn[0].dists.shape, torch.squeeze(nn[1].idx, dim=-1).shape)\n",
    "    \n",
    "    #print(\"sample eigen\", eig_vects_x[0,:4,:,:], eig_vals_x[0,:4])\n",
    "\n",
    "    eigen_val_dist_y = torch.sum(torch.square(eig_vals_x - corresponding_y_vals))\n",
    "    eigen_val_dist_x = torch.sum(torch.square(eig_vals_y - corresponding_x_vals))\n",
    "    \n",
    "    # calculate dot product between eigenvectors\n",
    "    dot_product_y = einops.einsum(eig_vects_x, corresponding_y_vects, \"b n p q, b n p q -> b n p\")\n",
    "    # ignore direction and take absolute value\n",
    "    dot_product_y = torch.sum(1. - torch.abs(dot_product_y))\n",
    "    dot_product_x = einops.einsum(eig_vects_y, corresponding_x_vects, \"b n p q, b n p q -> b n p\")\n",
    "    print(\"dot\", (1. - torch.abs(dot_product_x))[0,:3])\n",
    "    # ignore direction and take absolute value\n",
    "    dot_product_x = torch.sum(1. - torch.abs(dot_product_x))\n",
    "    \n",
    "\n",
    "    #corresponding_y_points = eindex(x, torch.squeeze(nn[1].idx, dim=-1), \"batch [batch points] xyz\")\n",
    "    #print(\"corresponding shape\", corresponding_y_vals.shape, corresponding_y_vects.shape)\n",
    "    #print(\"mean\", eigen_val_dist_x)\n",
    "    \n",
    "    #curvature_loss = (dot_product_x + dot_product_y)/1000 + (eigen_val_dist_x + eigen_val_dist_y)*1000\n",
    "    curvature_loss = (dot_product_x + dot_product_y)/1000\n",
    "    #print(\"curvature loss\", curvature_loss)\n",
    "\n",
    "    #print(\"eig_vals_x\", eig_vals_x.shape, eig_vects_x.shape, \"eig_vals_y\", eig_vals_y.shape, eig_vects_y.shape)\n",
    "\n",
    "    bidirectional_dist = torch.sum(nn[0].dists[:,:,0]) + torch.sum(nn[1].dists[:, :, 0])\n",
    "    print(\"d\", torch.sum(nn[1].dists[:,:,0]).item(), torch.sum(nn[0].dists[:,:,0]).item())\n",
    "\n",
    "    return curvature_loss + bidirectional_dist, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example input tensors\n",
    "b, n, k = 2, 4, 3\n",
    "values = torch.rand(b, n, k)  # Random values tensor of shape [b, n, k]\n",
    "indices = torch.randint(0, n, (b, n, k))  # Random indices tensor of shape [b, n, k] with values from 0 to n-1\n",
    "\n",
    "# Initialize the output tensor Z of shape [b, n] with zeros\n",
    "Z = torch.zeros(b, n)\n",
    "\n",
    "# Use scatter_add to sum values according to indices\n",
    "for i in range(b):\n",
    "    Z[i].scatter_add_(0, indices[i].flatten(), values[i].flatten())\n",
    "\n",
    "print(\"Values tensor:\\n\", values)\n",
    "print(\"Indices tensor:\\n\", indices)\n",
    "print(\"Resultant tensor Z:\\n\", Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure the distance between a point and its correspondence's corresponding point\n",
    "def calc_dcd_correspondence_tensor(x, y, k=8, return_assignment=False, return_dists=False):\n",
    "\n",
    "    chamferDist = ChamferDistance()\n",
    "    nn = chamferDist(\n",
    "        x,\n",
    "        y,\n",
    "        bidirectional=True,\n",
    "        return_nn=True,\n",
    "        k=k\n",
    "    )\n",
    "\n",
    "    eps = 0.00001\n",
    "    batch_size, point_count, _ = x.shape\n",
    "\n",
    "    softmaxed_0 = torch.nn.functional.softmax(1/(nn[0].dists+eps), dim=-1)\n",
    "    softmaxed_1 = torch.nn.functional.softmax(1/(nn[1].dists+eps), dim=-1)\n",
    "\n",
    "    point_weights_1 = torch.zeros(batch_size, point_count, dtype=torch.float64).cuda()\n",
    "    for i in range(batch_size):\n",
    "        point_weights_1[i].scatter_add_(0, nn[0].idx[i].flatten(), softmaxed_0[i].flatten())\n",
    "\n",
    "    point_weights_0 = torch.zeros(batch_size, point_count, dtype=torch.float64).cuda()\n",
    "    for i in range(batch_size):\n",
    "        point_weights_0[i].scatter_add_(0, nn[1].idx[i].flatten(), softmaxed_1[i].flatten())\n",
    "\n",
    "    #print(\"w\", point_weights_1.shape)\n",
    "\n",
    "    # Extract necessary tensors from nn for better readability\n",
    "    idx_0 = nn[0].idx[:, :, 0]\n",
    "    dists_0 = nn[0].dists[:, :, 0]\n",
    "    idx_1 = nn[1].idx[:, :, 0]\n",
    "    dists_1 = nn[1].dists[:, :, 0]\n",
    "\n",
    "    # Use advanced indexing to gather point weights and calculate weighted distances\n",
    "    corresponding_weights_0 = point_weights_1.unsqueeze(2).repeat(1,1,k).gather(1, nn[0].idx)\n",
    "    corresponding_weights_1 = point_weights_0.unsqueeze(2).repeat(1,1,k).gather(1, nn[1].idx)\n",
    "    \n",
    "    _, i0 = torch.min(corresponding_weights_0, dim=2)\n",
    "    _, i1 = torch.min(corresponding_weights_1, dim=2)\n",
    "    \n",
    "    min_dist_1 = torch.gather(nn[1].dists, 2, i1.unsqueeze(2).repeat(1,1,k))[:, :, 0]\n",
    "    min_dist_0 = torch.gather(nn[0].dists, 2, i0.unsqueeze(2).repeat(1,1,k))[:, :, 0]\n",
    "\n",
    "    dcd = torch.sum(min_dist_1) + torch.sum(min_dist_0)\n",
    "    dcd = dcd / (batch_size * point_count)\n",
    "\n",
    "    # corresponding_weights_1 = point_weights_0.gather(1, nn[1].idx)\n",
    "    \n",
    "    print(\"corres\", corresponding_weights_0.shape, i0.shape, min_dist_0.shape)\n",
    "\n",
    "    bidirectional_dist = torch.sum(nn[0].dists[:,:,0]) + torch.sum(nn[1].dists[:,:,0])\n",
    "    bidirectional_dist = bidirectional_dist / (batch_size * point_count)\n",
    "    \n",
    "    print(\"dcd\", dcd.item(), bidirectional_dist.item())\n",
    "    \n",
    "    return dcd\n",
    "\n",
    "    # weighted_dist = torch.sum(weighted_distances_0) + torch.sum(weighted_distances_1)\n",
    "    # weighted_dist = weighted_dist / (batch_size * point_count) *10\n",
    "    \n",
    "    # weights = torch.sum(torch.square(point_weights_0)) + torch.sum(torch.square(point_weights_1))\n",
    "    # weights = weights / (batch_size * point_count) *0.1\n",
    "\n",
    "    # print(\"wd\", weighted_dist.item(), bidirectional_dist.item(),\n",
    "    #       \"weights\", weights.item(),\n",
    "    #       \"std\", torch.std(point_weights_0).item(), torch.std(point_weights_1).item(),\n",
    "    #       \"sum\", torch.sum(point_weights_0).item(), torch.sum(point_weights_1).item())\n",
    "\n",
    "    # return bidirectional_dist + weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld1_name = \"sphere/plane1.pcd\"\n",
    "cld2_name = \"sphere/chair.pcd\"\n",
    "\n",
    "cuda = torch.device(\"cuda\")\n",
    "cld1 = np.array(o3d.io.read_point_cloud(cld1_name).points)\n",
    "cld2 = np.array(o3d.io.read_point_cloud(cld2_name).points)\n",
    "pcd1_tensor = torch.tensor([cld1], device=cuda)\n",
    "pcd2_tensor = torch.tensor([cld2], device=cuda)\n",
    "\n",
    "l = calc_dcd_correspondence_tensor(pcd1_tensor, pcd2_tensor)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# visualise animation\n",
    "cld1_name = \"sphere/plane1.pcd\"\n",
    "visualise = False\n",
    "#loss_funcs = [ \"chamfer\", \"emd\", \"balanced\", \"reverse\", \"single\"]\n",
    "#loss_funcs = [ \"balanced\", \"single\"]\n",
    "loss_funcs = [ \"cyclic\"]\n",
    "for loss_func in loss_funcs:\n",
    "    print(loss_func)\n",
    "    run_morph(cld1_name, loss_func, lr=.01)\n",
    "    if visualise:\n",
    "        with open(\"sphere/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "            morphed = pickle.load(f)\n",
    "        colours = visualise_density(morphed, 'plasma_r')\n",
    "        with open(\"sphere/\" + loss_func + \"_dens.pkl\", \"wb\") as f:\n",
    "            pickle.dump(colours, f)\n",
    "        #create_point_cloud_animation(cloud_list, loss_func)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def view_density(loss_func): \n",
    "    with open(\"sphere/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "        morphed = pickle.load(f)\n",
    "    with open(\"sphere/\" + loss_func + \"_dens.pkl\", \"rb\") as f:\n",
    "        colours = pickle.load(f)\n",
    "\n",
    "    create_point_cloud_animation(morphed, loss_func, True, colours[:,:,:3])\n",
    "\n",
    "interact(view_density, loss_func=[\"cyclic\", \"chamfer\", \"curvature\", \"density\", \"balanced\", \"infocd\", \"single\", \"reverse\", \"emd\", \"direct\"]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Softmin(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "output = m(input)\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# visualise animation\n",
    "loss_func = \"emd\"\n",
    "with open(\"sphere/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "    morphed = pickle.load(f)\n",
    "print(morphed.shape, loss_func)\n",
    "cloud_list = [m for m in morphed]\n",
    "#create_point_cloud_animation(cloud_list, loss_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = visualise_density(morphed, 'plasma_r')\n",
    "with open(\"sphere/\" + loss_func + \"_dens.pkl\", \"wb\") as f:\n",
    "    pickle.dump(colours, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample\n",
    "shapenet_path = \"../experiments/ICCV2023-HyperCD/ShapeNetCompletion/test/complete/\"\n",
    "downsampled_path = \"../experiments/ICCV2023-HyperCD/ShapeNetCompletion/downsample/\"\n",
    "# folders = os.listdir(shapenet_path)\n",
    "# choices = np.random.choice(len(points), 4096)\n",
    "\n",
    "# for fl in tqdm(folders):\n",
    "#     if not os.path.exists(downsampled_path+fl):\n",
    "#         os.mkdir(downsampled_path+fl)\n",
    "        \n",
    "#     files = os.listdir(shapenet_path + fl)\n",
    "#     cloud =  o3d.geometry.PointCloud()\n",
    "#     for cl in files:\n",
    "#         points = np.array(o3d.io.read_point_cloud(shapenet_path + fl + \"/\" + cl).points)\n",
    "#         points = points[choices]\n",
    "#         cloud.points = o3d.utility.Vector3dVector(points)\n",
    "#         o3d.io.write_point_cloud(downsampled_path+fl + \"/\" + cl, cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = \"balanced\"\n",
    "\n",
    "cd, emd = sphere_morph_metrics(loss_func, downsampled_path, save=False)\n",
    "print(cd[-1], emd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = \"balanced\"\n",
    "#loss_func = \"emd\"\n",
    "\n",
    "sphere_morph_metrics(loss_func, downsampled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses on same axis\n",
    "def plot_losses(losses, labels, title):\n",
    "    x = np.arange(0, len(losses[0]))\n",
    "    plt.figure(figsize=(30, 6))\n",
    "    for i, loss in enumerate(losses):\n",
    "        plt.plot(x, loss, label=labels[i])\n",
    "\n",
    "    plt.xlabel(\"point cloud index\")\n",
    "    plt.ylabel(\"distance\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plots\n",
    "loss_funcs = [\"chamfer\",]\n",
    "chamfer_list, emd_list = [], []\n",
    "for loss_func in loss_funcs:\n",
    "    with open(\"sphere/\" + loss_func + \"_metrics.pkl\", \"rb\") as f:\n",
    "        chamfer, emd, assignments = pickle.load(f)\n",
    "        chamfer_list.append(chamfer)\n",
    "        emd_list.append(emd)\n",
    "        \n",
    "plot_losses(chamfer_list, loss_funcs, \"chamfer\")\n",
    "\n",
    "ts(emd_list, loss_funcs, \"EMD\")\n",
    "        \n",
    "print(emd_list[-1], chamfer_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load losses\n",
    "loss_types = [\"reverse\", \"chamfer\", \"emd\", \"pair\"]\n",
    "losses = []\n",
    "\n",
    "for loss_func in loss_types:\n",
    "    with open(\"sphere/loss_\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "        losses.append(pickle.load(f))\n",
    "        \n",
    "plot_losses(losses, loss_types, \"loss function comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure conssistency between forward and backward correspondences for chamfer distance\n",
    "# optionally compare against the ideal assignment, as measured by EMD\n",
    "def measure_assignment_consistency(assignment, emd=None):\n",
    "    reverse_assignment = torch.gather(assignment[0], 0, assignment[1])\n",
    "    expected = torch.arange(assignment[0].shape[0], device=torch.device(\"cuda\"))\n",
    "    consistency = torch.sum(torch.eq(expected, reverse_assignment).long())\n",
    "    print(\"consistency\", consistency.item(), len(torch.unique(assignment[0])), len(torch.unique(assignment[1])))\n",
    "    \n",
    "    if emd is not None:\n",
    "        #print(emd[:5], assignment[0][:5], assignment[1][:5])\n",
    "        emd_consistency = torch.sum(torch.eq(emd, assignment[0]).long())\n",
    "        #print(\"emd_consistency\", emd_consistency.item(), len(torch.unique(emd)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare correspondences\n",
    "#TODO: include consistency in top5 matches?\n",
    "loss_func = \"emd\"\n",
    "with open(\"sphere/assignments_\" + \"emd\" + \".pkl\", \"rb\") as f:\n",
    "    emd_assignment = pickle.load(f)\n",
    "\n",
    "with open(\"sphere/assignments_\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "    assignment = pickle.load(f)\n",
    "    \n",
    "for i, ass in enumerate(assignment):\n",
    "    #measure_assignment_consistency(ass, emd_assignment[i][0])\n",
    "    measure_assignment_consistency(ass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the morphed middle ground between two different clouds\n",
    "cld1_name = \"sphere/plane1.pcd\"\n",
    "cld2_name = \"sphere/chair.pcd\"\n",
    "\n",
    "cuda = torch.device(\"cuda\")\n",
    "cld1 = np.array(o3d.io.read_point_cloud(cld1_name).points)\n",
    "cld2 = np.array(o3d.io.read_point_cloud(cld2_name).points)\n",
    "pcd1_tensor = torch.tensor([cld1], device=cuda)\n",
    "pcd2_tensor = torch.tensor([cld2], device=cuda)\n",
    "\n",
    "print(pcd1_tensor.shape, pcd2_tensor.shape)\n",
    "loss, assignment = calc_emd(pcd1_tensor, pcd2_tensor, 0.05, 50)\n",
    "assignment = assignment.detach().cpu().numpy()\n",
    "print(assignment.shape)\n",
    "\n",
    "matched_points = cld2[assignment[0]]\n",
    "print(matched_points.shape)\n",
    "\n",
    "morphed_points = (cld1 + matched_points)/2\n",
    "morphed_cloud =  o3d.geometry.PointCloud()\n",
    "morphed_cloud.points = o3d.utility.Vector3dVector(morphed_points)\n",
    "o3d.io.write_point_cloud(\"sphere/chair_plane_morph.pcd\", morphed_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spherical projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def load_point_cloud(file_path):\n",
    "    \"\"\"\n",
    "    Loads a point cloud from the specified file path.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the point cloud file.\n",
    "\n",
    "    Returns:\n",
    "        o3d.geometry.PointCloud: The loaded point cloud.\n",
    "    \"\"\"\n",
    "    pcd = o3d.io.read_point_cloud(file_path)\n",
    "    if not pcd.has_points():\n",
    "        raise ValueError(\"The point cloud has no points.\")\n",
    "    return pcd\n",
    "\n",
    "def point_cloud_to_equirectangular(pcd, img_width=512, img_height=512):\n",
    "    \"\"\"\n",
    "    Projects a point cloud onto a 2D equirectangular image.\n",
    "\n",
    "    Args:\n",
    "        pcd (o3d.geometry.PointCloud): The input point cloud.\n",
    "        img_width (int): Width of the output image.\n",
    "        img_height (int): Height of the output image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The equirectangular image as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Extract points and colors\n",
    "    points = np.asarray(pcd.points)\n",
    "    has_colors = pcd.has_colors()\n",
    "    if has_colors:\n",
    "        colors = np.asarray(pcd.colors)\n",
    "    else:\n",
    "        # If colors are not available, use depth for grayscale intensity\n",
    "        colors = None\n",
    "\n",
    "    # Initialize the image with black pixels\n",
    "    if has_colors:\n",
    "        # For RGB image\n",
    "        img = np.zeros((img_height, img_width, 3), dtype=np.uint8)\n",
    "    else:\n",
    "        # For grayscale image\n",
    "        img = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "\n",
    "    # Initialize depth buffer with infinities\n",
    "    depth_buffer = np.full((img_height, img_width), np.inf)\n",
    "\n",
    "    # Compute spherical coordinates for each point\n",
    "    x = points[:, 0]\n",
    "    y = points[:, 1]\n",
    "    z = points[:, 2]\n",
    "    r = np.sqrt(x**2 + y**2 + z**2)\n",
    "    theta = np.arctan2(z, x)  # Longitude\n",
    "    phi = np.arcsin(y / r)    # Latitude\n",
    "\n",
    "    # Normalize angles to [0, 1] range for mapping to image pixels\n",
    "    u = (theta + np.pi) / (2 * np.pi)  # Range: [0, 1]\n",
    "    v = (np.pi/2 - phi) / np.pi        # Range: [0, 1]\n",
    "\n",
    "    # Map to pixel coordinates\n",
    "    pixel_x = (u * img_width).astype(np.int32)\n",
    "    pixel_y = (v * img_height).astype(np.int32)\n",
    "\n",
    "    # Ensure pixel indices are within image bounds\n",
    "    pixel_x = np.clip(pixel_x, 0, img_width - 1)\n",
    "    pixel_y = np.clip(pixel_y, 0, img_height - 1)\n",
    "\n",
    "    # Iterate over all points to update the image and depth buffer\n",
    "    for idx in range(points.shape[0]):\n",
    "        x_pix = pixel_x[idx]\n",
    "        y_pix = pixel_y[idx]\n",
    "        depth = r[idx]\n",
    "        if depth < depth_buffer[y_pix, x_pix]:\n",
    "            depth_buffer[y_pix, x_pix] = depth\n",
    "            if has_colors:\n",
    "                color = (colors[idx] * 255).astype(np.uint8)\n",
    "                img[y_pix, x_pix, :] = color\n",
    "            else:\n",
    "                # Map depth to intensity (closer points are brighter)\n",
    "                # Normalize depth for visualization\n",
    "                # Here, you might need to adjust the depth scaling based on your data\n",
    "                max_depth = np.max(r)\n",
    "                min_depth = np.min(r)\n",
    "                if max_depth == min_depth:\n",
    "                    intensity = 255\n",
    "                else:\n",
    "                    intensity = ((1 - (depth - min_depth) / (max_depth - min_depth)) * 255).astype(np.uint8)\n",
    "                img[y_pix, x_pix] = intensity\n",
    "\n",
    "    return img\n",
    "\n",
    "def save_image(img_array, output_path):\n",
    "    \"\"\"\n",
    "    Saves the image array to the specified path.\n",
    "\n",
    "    Args:\n",
    "        img_array (np.ndarray): The image data.\n",
    "        output_path (str): Path to save the image.\n",
    "    \"\"\"\n",
    "    img = Image.fromarray(img_array)\n",
    "    img.save(output_path)\n",
    "    #print(f\"Image saved to {output_path}\")\n",
    "\n",
    "# Path to your point cloud file (e.g., .ply, .pcd)\n",
    "point_cloud_path = \"1d63eb2b1f78aa88acf77e718d93f3e1.pcd\"\n",
    "# Path to save the output image\n",
    "output_image_path = \"equirectangular_projection.png\"\n",
    "\n",
    "# Load point cloud\n",
    "print(\"Loading point cloud...\")\n",
    "pcd = load_point_cloud(point_cloud_path)\n",
    "\n",
    "# Generate equirectangular projection\n",
    "print(\"Generating equirectangular projection...\")\n",
    "img_width = 512  # You can adjust the image size as needed\n",
    "img_height = 512\n",
    "img = point_cloud_to_equirectangular(pcd, img_width, img_height)\n",
    "\n",
    "# Save the resulting image\n",
    "print(\"Saving image...\")\n",
    "save_image(img, output_image_path)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def image_to_point_cloud(img_array, intensity_threshold=10):\n",
    "    \"\"\"\n",
    "    Converts a 2D equirectangular image (using pixel intensity as depth) back into a 3D point cloud.\n",
    "    Filters out points based on an intensity threshold to prevent far background points.\n",
    "\n",
    "    Args:\n",
    "        img_array (np.ndarray): The input equirectangular grayscale image.\n",
    "        intensity_threshold (int): The minimum intensity value to consider a valid point.\n",
    "\n",
    "    Returns:\n",
    "        o3d.geometry.PointCloud: The reconstructed 3D point cloud.\n",
    "    \"\"\"\n",
    "    img_height, img_width = img_array.shape[:2]\n",
    "    point_cloud = []\n",
    "\n",
    "    # Assume the depth is normalized between some min and max values\n",
    "    max_depth = 10.0  # Adjust this value as needed\n",
    "    min_depth = 0.1   # Minimum depth\n",
    "\n",
    "    for y in range(img_height):\n",
    "        for x in range(img_width):\n",
    "            intensity = img_array[y, x]\n",
    "            \n",
    "            # Apply the intensity threshold to filter out background\n",
    "            if intensity < intensity_threshold:\n",
    "                continue  # Skip points with low intensity (likely background)\n",
    "\n",
    "            # Normalize intensity to [0, 1]\n",
    "            normalized_intensity = intensity / 255.0\n",
    "\n",
    "            # Map normalized intensity to depth range\n",
    "            depth = min_depth + (1 - normalized_intensity) * (max_depth - min_depth)\n",
    "\n",
    "            # Recover spherical coordinates from pixel coordinates\n",
    "            u = x / img_width  # Normalized longitude\n",
    "            v = y / img_height  # Normalized latitude\n",
    "            theta = u * 2 * np.pi - np.pi  # Longitude in radians\n",
    "            phi = np.pi/2 - v * np.pi       # Latitude in radians\n",
    "\n",
    "            # Convert spherical coordinates back to Cartesian coordinates\n",
    "            x_coord = depth * np.cos(phi) * np.cos(theta)\n",
    "            y_coord = depth * np.sin(phi)\n",
    "            z_coord = depth * np.cos(phi) * np.sin(theta)\n",
    "            #print(x_coord, y_coord, z_coord)\n",
    "\n",
    "            # Append the point to the point cloud\n",
    "            point_cloud.append((x_coord, y_coord, z_coord))\n",
    "\n",
    "    # Create the Open3D point cloud object\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    points = np.array(point_cloud)\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "    return pcd\n",
    "\n",
    "def load_grayscale_image(image_path):\n",
    "    \"\"\"\n",
    "    Loads a grayscale image from the specified path.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the equirectangular grayscale image file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The loaded image array.\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path).convert(\"L\")  # Convert image to grayscale\n",
    "    img_array = np.array(img)\n",
    "    print(img_array.shape)\n",
    "    return img_array\n",
    "\n",
    "def save_point_cloud(pcd, output_path):\n",
    "    \"\"\"\n",
    "    Saves the point cloud to a file.\n",
    "\n",
    "    Args:\n",
    "        pcd (o3d.geometry.PointCloud): The point cloud to save.\n",
    "        output_path (str): Path to save the point cloud file.\n",
    "    \"\"\"\n",
    "    o3d.io.write_point_cloud(output_path, pcd)\n",
    "    print(f\"Point cloud saved to {output_path}\")\n",
    "\n",
    "# Path to the equirectangular grayscale image\n",
    "image_path = \"equirectangular_projection.png\"\n",
    "output_point_cloud_path = \"reconstructed_point_cloud.ply\"\n",
    "\n",
    "# Load the grayscale image\n",
    "print(\"Loading grayscale image...\")\n",
    "img_array = load_grayscale_image(image_path)\n",
    "\n",
    "# Reconstruct the point cloud from the grayscale image\n",
    "print(\"Reconstructing point cloud...\")\n",
    "pcd = image_to_point_cloud(img_array, intensity_threshold=10)\n",
    "\n",
    "# Save the reconstructed point cloud\n",
    "print(\"Saving point cloud...\")\n",
    "save_point_cloud(pcd, output_point_cloud_path)\n",
    "print(\"Done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../experiments/2d_autoencoder/outputs/\"\n",
    "output_point_cloud_path = \"../experiments/2d_autoencoder/outputs/pcd/\"\n",
    "\n",
    "for i in range(16):\n",
    "    img_array = load_grayscale_image(image_path + f\"{i}.png\")\n",
    "    pcd = image_to_point_cloud(img_array, intensity_threshold=10)\n",
    "    save_point_cloud(pcd, output_point_cloud_path + f\"{i}.pcd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_image_to_point_cloud(images, intensity_threshold=10):\n",
    "    \"\"\"\n",
    "    Converts a batch of 2D equirectangular images (using pixel intensity as depth) back into 3D point clouds.\n",
    "    Filters out points based on an intensity threshold to prevent far background points.\n",
    "\n",
    "    Args:\n",
    "        images (tf.Tensor): A batch of input equirectangular grayscale images of shape (batch_size, height, width).\n",
    "        intensity_threshold (int): The minimum intensity value to consider a valid point.\n",
    "\n",
    "    Returns:\n",
    "        point_clouds (tf.Tensor): The batch of reconstructed 3D point clouds, with shape (batch_size, num_points, 3).\n",
    "    \"\"\"\n",
    "    batch_size, img_height, img_width = tf.shape(images)[0], tf.shape(images)[1], tf.shape(images)[2]\n",
    "    \n",
    "    # Generate grid of coordinates\n",
    "    x = tf.linspace(0.0, 1.0, img_width)\n",
    "    y = tf.linspace(0.0, 1.0, img_height)\n",
    "    x_grid, y_grid = tf.meshgrid(x, y)\n",
    "    \n",
    "    # Compute longitude and latitude in radians\n",
    "    theta = x_grid * 2.0 * tf.constant(np.pi) - tf.constant(np.pi)\n",
    "    phi = tf.constant(np.pi/2) - y_grid * tf.constant(np.pi)\n",
    "    \n",
    "    # Reshape for broadcasting\n",
    "    theta = tf.reshape(theta, (1, img_height, img_width))\n",
    "    phi = tf.reshape(phi, (1, img_height, img_width))\n",
    "    \n",
    "    # Normalize intensities and calculate depth\n",
    "    normalized_intensity = images / 255.0\n",
    "    max_depth = 10.0\n",
    "    min_depth = 0.1\n",
    "    depth = min_depth + (1.0 - normalized_intensity) * (max_depth - min_depth)\n",
    "    \n",
    "    # Apply the intensity threshold\n",
    "    mask = tf.greater_equal(images, intensity_threshold)\n",
    "    depth = tf.where(mask, depth, tf.zeros_like(depth))\n",
    "    \n",
    "    # Convert spherical coordinates to Cartesian coordinates\n",
    "    x_coord = depth * tf.cos(phi) * tf.cos(theta)\n",
    "    y_coord = depth * tf.sin(phi)\n",
    "    z_coord = depth * tf.cos(phi) * tf.sin(theta)\n",
    "    \n",
    "    # Stack the coordinates into point clouds\n",
    "    point_clouds = tf.stack([x_coord, y_coord, z_coord], axis=-1)\n",
    "    \n",
    "    # Reshape to (batch_size, num_points, 3)\n",
    "    point_clouds = tf.reshape(point_clouds, (batch_size, -1, 3))\n",
    "    \n",
    "    # Filter out zero-depth points (these are the masked-out points)\n",
    "    point_clouds = tf.boolean_mask(point_clouds, tf.not_equal(tf.norm(point_clouds, axis=-1), 0), axis=1)\n",
    "\n",
    "    return point_clouds\n",
    "\n",
    "# Path to the equirectangular grayscale image\n",
    "image_path = \"equirectangular_projection.png\"\n",
    "output_point_cloud_path = \"reconstructed_point_cloud.ply\"\n",
    "\n",
    "# Load the grayscale image\n",
    "print(\"Loading grayscale image...\")\n",
    "img_array = load_grayscale_image(image_path)\n",
    "img_tensor = tf.tensor([img_array])\n",
    "\n",
    "# Reconstruct the point cloud from the grayscale image\n",
    "print(\"Reconstructing point cloud...\")\n",
    "cld = batch_image_to_point_cloud(img_array, intensity_threshold=10)\n",
    "\n",
    "# Create the Open3D point cloud object\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "points = np.array(cld)\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "# Save the reconstructed point cloud\n",
    "print(\"Saving point cloud...\")\n",
    "save_point_cloud(pcd, output_point_cloud_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEALPix\n",
    "import open3d as o3d\n",
    "import healpy as hp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def point_cloud_to_healpix(pcd, nside=512):\n",
    "    \"\"\"\n",
    "    Projects a 3D point cloud into a 2D HEALPix image using pixel intensity as depth.\n",
    "\n",
    "    Args:\n",
    "        pcd (o3d.geometry.PointCloud): The input 3D point cloud.\n",
    "        nside (int): The resolution of the HEALPix map (higher nside means higher resolution).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The resulting HEALPix map with depth encoded as pixel intensity.\n",
    "    \"\"\"\n",
    "    # Extract points from the point cloud\n",
    "    points = np.asarray(pcd.points)\n",
    "    x, y, z = points[:, 0], points[:, 1], points[:, 2]\n",
    "\n",
    "    # Calculate the spherical coordinates\n",
    "    r = np.sqrt(x**2 + y**2 + z**2)\n",
    "    theta = np.arccos(z / r)  # colatitude (0 to pi)\n",
    "    phi = np.arctan2(y, x)    # longitude (0 to 2*pi)\n",
    "\n",
    "    # Map spherical coordinates to HEALPix pixel indices\n",
    "    pix_indices = hp.ang2pix(nside, theta, phi)\n",
    "\n",
    "    # Initialize the HEALPix map\n",
    "    healpix_map = np.zeros(hp.nside2npix(nside), dtype=np.float32)\n",
    "\n",
    "    # Aggregate depth values (use the closest point depth for each pixel)\n",
    "    for i, pix in enumerate(pix_indices):\n",
    "        if healpix_map[pix] == 0 or r[i] < healpix_map[pix]:  # closer point\n",
    "            healpix_map[pix] = r[i]\n",
    "\n",
    "    # Normalize depth to intensity (0-255 range)\n",
    "    healpix_map = np.log1p(healpix_map)  # Use log scale for better depth perception\n",
    "    healpix_map = (healpix_map - np.min(healpix_map)) / (np.max(healpix_map) - np.min(healpix_map))\n",
    "    healpix_map = (healpix_map * 255).astype(np.uint8)\n",
    "\n",
    "    return healpix_map\n",
    "\n",
    "def visualize_healpix(healpix_map, nside):\n",
    "    \"\"\"\n",
    "    Visualizes the HEALPix map using a Mollweide projection.\n",
    "\n",
    "    Args:\n",
    "        healpix_map (np.ndarray): The HEALPix map to visualize.\n",
    "        nside (int): The resolution of the HEALPix map.\n",
    "    \"\"\"\n",
    "    hp.mollview(healpix_map, coord=['C'], title='HEALPix Projection of Point Cloud', cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Load the point cloud\n",
    "    point_cloud_path = \"sphere/chair.pcd\"\n",
    "    print(\"Loading point cloud...\")\n",
    "    pcd = o3d.io.read_point_cloud(point_cloud_path)\n",
    "\n",
    "    # Convert the point cloud to a HEALPix image\n",
    "    nside = 512  # Adjust the resolution (nside) as needed\n",
    "    print(\"Projecting point cloud to HEALPix...\")\n",
    "    healpix_map = point_cloud_to_healpix(pcd, nside=nside)\n",
    "\n",
    "    # Visualize the HEALPix image\n",
    "    print(\"Visualizing HEALPix projection...\")\n",
    "    visualize_healpix(healpix_map, nside)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create equirectangular dataset from PCN\n",
    "\n",
    "root_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/ShapeNetCompletion/train/\"\n",
    "input_path = root_path + \"partial/\"\n",
    "\n",
    "classes = os.listdir(input_path)\n",
    "\n",
    "for cl in classes:\n",
    "    print(cl)\n",
    "    folders = os.listdir(input_path + cl)\n",
    "    for fl in tqdm(folders):\n",
    "        files = os.listdir(input_path + cl + \"/\" + fl)\n",
    "        for fl2 in files:\n",
    "            pcd = o3d.io.read_point_cloud(input_path + cl + \"/\" + fl + \"/\" + fl2)\n",
    "            img = point_cloud_to_equirectangular(pcd)\n",
    "            img_path = root_path + \"2d_partial/\" + cl + \"/\" + fl + \"/\"\n",
    "            if not os.path.exists(img_path):\n",
    "                os.makedirs(img_path)\n",
    "            img_path = root_path + \"2d_partial/\" + cl + \"/\" + fl + \"/\" + fl2[:-4] + \".png\"\n",
    "            save_image(img, img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/ShapeNetCompletion/train/\"\n",
    "input_path = root_path + \"complete/\"\n",
    "\n",
    "classes = os.listdir(input_path)\n",
    "\n",
    "for cl in classes:\n",
    "    print(cl)\n",
    "    files = os.listdir(input_path + cl)\n",
    "    for fl in tqdm(files):\n",
    "        pcd = o3d.io.read_point_cloud(input_path + cl + \"/\" + fl )\n",
    "        img = point_cloud_to_equirectangular(pcd)\n",
    "        img_path = root_path + \"2d_complete/\" + cl + \"/\"\n",
    "        if not os.path.exists(img_path):\n",
    "            os.makedirs(img_path)\n",
    "        img_path = root_path + \"2d_complete/\" + cl + \"/\" + fl[:-4] + \".png\"\n",
    "        save_image(img, img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
