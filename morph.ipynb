{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpvL68OfBEQC"
   },
   "source": [
    "# Element Parameter Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z7n_pw4SMWl"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ47VNF7fmTS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import os.path\n",
    "import torch\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import importlib\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import functorch\n",
    "from numpy.random import default_rng\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import interact \n",
    "import gc\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from chamferdist import ChamferDistance\n",
    "from pathlib import Path\n",
    "\n",
    "import ifcopenshell\n",
    "import open3d as o3d\n",
    "\n",
    "from src.elements import *\n",
    "from src.ifc import *\n",
    "from src.preparation import *\n",
    "from src.dataset import *\n",
    "from src.pointnet import *\n",
    "from src.visualisation import *\n",
    "from src.geometry import sq_distance, get_oriented_bbox_from_points\n",
    "from src.icp import icp_finetuning\n",
    "from src.chamfer import *\n",
    "from src.utils import *\n",
    "from src.plots import plot_error_graph, plot_parameter_errors\n",
    "from src.pca import testset_PCA\n",
    "from src.finetune import chamfer_fine_tune, mahalanobis_fine_tune\n",
    "from src.cloud import add_noise\n",
    "\n",
    "\n",
    "random.seed = 42\n",
    "rng = default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sphere morphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise a list of point clouds as an animation using open3d\n",
    "# use ctrl+c to copy and ctrl+v to set camera and zoom inside visualiser\n",
    "def create_point_cloud_animation(cloud_list, loss_func, save_image=False, colours=None):\n",
    "    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Debug)\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    cloud = cloud_list[0]\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(cloud)\n",
    "    if colours is not None:\n",
    "        point_cloud.colors = o3d.utility.Vector3dVector(colours[0])\n",
    "    vis.add_geometry(point_cloud)\n",
    "    stops = [9,39,99,299,999]\n",
    "\n",
    "    for i in range(len(cloud_list)):\n",
    "        time.sleep(0.01 + 0.05/(i/10+1))\n",
    "        cloud = cloud_list[i]\n",
    "        point_cloud.points = o3d.utility.Vector3dVector(cloud)\n",
    "        if colours is not None:\n",
    "            point_cloud.colors = o3d.utility.Vector3dVector(colours[i])\n",
    "        vis.update_geometry(point_cloud)\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "        if save_image and i in stops:\n",
    "            vis.capture_screen_image(\"sphere/\" + loss_func + str(i) + \".jpg\", do_render=True)\n",
    "    vis.destroy_window()\n",
    "\n",
    "    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_direct(x, y):\n",
    "    return torch.sum(torch.square(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# morph a sphere into the shape of an input point cloud\n",
    "# by optimising chamfer loss iteratively\n",
    "# total points = num_points**2\n",
    "def morph_sphere(src_pcd_tensor, num_points, iterations, learning_rate, stops=[],\n",
    "                 loss_func= \"chamfer\", measure_consistency=True, sphere=True, return_assignment=True):\n",
    "    \n",
    "    cuda = torch.device(\"cuda\")\n",
    "    if sphere:\n",
    "        # gnerate sphere\n",
    "        # Generate spherical coordinates\n",
    "        theta = np.linspace(0, 2 * np.pi, num_points)\n",
    "        phi = np.linspace(0, np.pi, num_points)\n",
    "\n",
    "        # Create a meshgrid from spherical coordinates\n",
    "        theta, phi = np.meshgrid(theta, phi)\n",
    "\n",
    "        # Convert spherical coordinates to Cartesian coordinates\n",
    "        x = np.sin(phi) * np.cos(theta)\n",
    "        y = np.sin(phi) * np.sin(theta)\n",
    "        z = np.cos(phi)\n",
    "\n",
    "        # Stack the coordinates to form a 3D point cloud and reshape to (num_points * num_points, 3)\n",
    "        sphere_points = np.stack([x, y, z], axis=-1).reshape(-1, 3)\n",
    "        sphere_points = np.array([sphere_points for i in range(len(src_pcd_tensor))])\n",
    "        sphere_points = torch.tensor(sphere_points, device=cuda, \n",
    "                                     requires_grad=True)\n",
    "    else:\n",
    "        sphere_points = torch.rand(1, num_points**2, 3, device=cuda, \n",
    "                                   dtype=torch.double, requires_grad=True)\n",
    "    \n",
    "    # optimise\n",
    "    optimizer = torch.optim.Adam([sphere_points], lr=learning_rate)\n",
    "    intermediate, losses, assingments = [], [], []\n",
    "    chamferDist = ChamferDistance()\n",
    "    assignments = []\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if loss_func == \"chamfer\":\n",
    "            nn = chamferDist(\n",
    "                src_pcd_tensor, sphere_points, bidirectional=True, return_nn=True)\n",
    "            loss = torch.sum(nn[1].dists) + torch.sum(nn[0].dists)\n",
    "            assignment = [nn[0].idx[:,:,0].detach().cpu().numpy(), nn[1].idx[:,:,0].detach().cpu().numpy()]\n",
    "        elif loss_func == \"emd\":\n",
    "            loss, assignment = calc_emd(sphere_points, src_pcd_tensor, 0.05, 50)\n",
    "            assignment = assignment.detach().cpu().numpy()\n",
    "        elif loss_func == \"direct\":\n",
    "            loss = calc_direct(sphere_points, src_pcd_tensor)\n",
    "            assignment = None\n",
    "        elif loss_func == \"pair\":\n",
    "            loss, assignment = get_pair_loss_clouds_tensor(src_pcd_tensor, sphere_points, add_pair_loss=True, it=i)\n",
    "        elif loss_func == \"jittery\":\n",
    "            loss = get_jittery_cd_tensor(src_pcd_tensor, sphere_points, k=1, it=i)\n",
    "        elif loss_func == \"self\":\n",
    "            loss = get_self_cd_tensor(src_pcd_tensor, sphere_points)\n",
    "        elif loss_func == \"reverse\":\n",
    "            loss, assignment = calc_reverse_weighted_cd_tensor(src_pcd_tensor, sphere_points, return_assignment=True, k=32)\n",
    "        elif loss_func == \"prob\":\n",
    "            loss, assignment = calc_pairing_probabilty_loss_tensor(src_pcd_tensor, sphere_points, k=64)\n",
    "        elif loss_func == \"balanced\":\n",
    "            loss, assignment = calc_balanced_chamfer_loss_tensor(src_pcd_tensor, sphere_points, return_assignment=True, k=32)\n",
    "        elif loss_func == \"single\":\n",
    "            loss, assignment = calc_balanced_single_chamfer_loss_tensor(src_pcd_tensor, sphere_points, return_assignment=True, k=32)\n",
    "        elif loss_func == \"infocd\":\n",
    "            loss, assignment = calc_cd_like_InfoV2(src_pcd_tensor, sphere_points, return_assignment=True)\n",
    "        elif loss_func == \"density\":\n",
    "            loss = calc_relative_density_loss_tensor(src_pcd_tensor, sphere_points, return_assignment=False)\n",
    "        else:\n",
    "            print(\"unspecified loss\")\n",
    "            \n",
    "        #print(\"a\", assignment[0].shape)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(\"iteration\", i, \"loss\", loss.item())\n",
    "        \n",
    "        if i in stops:\n",
    "            intermediate.append(sphere_points.clone())\n",
    "            losses.append(loss.item())\n",
    "            if measure_consistency:\n",
    "                assignments.append(assignment)\n",
    "            \n",
    "    # calculate final chamfer loss\n",
    "    dist = chamferDist(\n",
    "                src_pcd_tensor, sphere_points, bidirectional=True)\n",
    "    emd_loss, _ = calc_emd(sphere_points, src_pcd_tensor, 0.05, 50)\n",
    "    print(\"final chamfer dist\", dist.item(), \"emd\", emd_loss.item())\n",
    "    \n",
    "    # save assignments for analysis\n",
    "#     if measure_consistency:\n",
    "#         with open(\"sphere/assignments_\" + loss_func + \".pkl\", \"wb\") as f:\n",
    "#             pickle.dump(assingments, f)\n",
    "            \n",
    "    intermediate = torch.stack(intermediate)\n",
    "    if return_assignment:\n",
    "        assignments = assignments\n",
    "        return intermediate, losses, assignments\n",
    "    return intermediate, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_morph(cld1_name, loss_func):\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    cld1 = np.array(o3d.io.read_point_cloud(cld1_name).points)\n",
    "    src_pcd_tensor = torch.tensor([cld1], device=cuda)\n",
    "\n",
    "    iterations = 1000\n",
    "    #stops = [0, 10, 50, 100, 150, 500, 999]\n",
    "    stops = [i for i in range(0,iterations,2)]\n",
    "\n",
    "    #morphed, losses = morph_sphere(src_pcd_tensor, 64, iterations, 0.01, stops, loss_func=loss_func, return_assignment=False)\n",
    "    morphed, losses = morph_sphere(src_pcd_tensor, 64, iterations, 0.01, stops, measure_consistency=False, loss_func=loss_func, return_assignment=False)\n",
    "    morphed = torch.flatten(morphed, start_dim=1, end_dim=2)\n",
    "    morphed = morphed.cpu().detach().numpy()\n",
    "    #print(morphed.shape)\n",
    "\n",
    "    # save frames\n",
    "    with open(\"sphere/\" + loss_func + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(morphed, f)\n",
    "\n",
    "    with open(\"sphere/loss_\" + loss_func + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(losses, f)\n",
    "\n",
    "    # Save the PointCloud to a PCD file\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(morphed[-1])\n",
    "    o3d.io.write_point_cloud(\"sphere/sphere_\" + loss_func + \".pcd\", point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cld1_name = \"sphere/chair.pcd\"\n",
    "# loss_func = \"chamfer\"\n",
    "# run_morph(cld1_name, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# visualise animation\n",
    "cld1_name = \"sphere/plane1.pcd\"\n",
    "visualise = True\n",
    "#loss_funcs = [ \"chamfer\", \"emd\", \"balanced\", \"reverse\", \"single\"]\n",
    "#loss_funcs = [ \"balanced\", \"single\"]\n",
    "loss_funcs = [ \"density\"]\n",
    "for loss_func in loss_funcs:\n",
    "    print(loss_func)\n",
    "    run_morph(cld1_name, loss_func)\n",
    "    if visualise:\n",
    "        with open(\"sphere/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "            morphed = pickle.load(f)\n",
    "        colours = visualise_density(morphed, 'plasma_r')\n",
    "        with open(\"sphere/\" + loss_func + \"_dens.pkl\", \"wb\") as f:\n",
    "            pickle.dump(colours, f)\n",
    "        #create_point_cloud_animation(cloud_list, loss_func)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def view_density(loss_func): \n",
    "    with open(\"sphere/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "        morphed = pickle.load(f)\n",
    "    with open(\"sphere/\" + loss_func + \"_dens.pkl\", \"rb\") as f:\n",
    "        colours = pickle.load(f)\n",
    "\n",
    "    create_point_cloud_animation(morphed, loss_func, True, colours[:,:,:3])\n",
    "    \n",
    "interact(view_density, loss_func=[ \"density\", \"balanced\", \"infocd\", \"single\", \"chamfer\", \"reverse\", \"emd\", \"direct\"]); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# visualise animation\n",
    "loss_func = \"emd\"\n",
    "with open(\"sphere/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "    morphed = pickle.load(f)\n",
    "print(morphed.shape, loss_func)\n",
    "cloud_list = [m for m in morphed]\n",
    "#create_point_cloud_animation(cloud_list, loss_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = visualise_density(morphed, 'plasma_r')\n",
    "with open(\"sphere/\" + loss_func + \"_dens.pkl\", \"wb\") as f:\n",
    "    pickle.dump(colours, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch sphere optimisation (for metrics)\n",
    "\n",
    "def sphere_morph_metrics(loss_func, shapenet_path):\n",
    "    # load shapenet test dataset\n",
    "    folders = os.listdir(shapenet_path)[7:8]\n",
    "    print(loss_func)\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    chamferDist = ChamferDistance()\n",
    "\n",
    "    iterations = 1001\n",
    "    stops = [i for i in range(0,1001,10)]\n",
    "    chamfer_results = np.zeros(len(stops))\n",
    "    emd_results = np.zeros(len(stops))\n",
    "    count = 0\n",
    "    assignments_folders = []\n",
    "    \n",
    "    for fl in folders:\n",
    "        files = os.listdir(shapenet_path + fl)\n",
    "        clouds = []\n",
    "        for cl in files:\n",
    "            clouds.append(np.array(o3d.io.read_point_cloud(shapenet_path + fl + \"/\" + cl).points))\n",
    "        clouds = np.array(clouds)\n",
    "        clouds = torch.tensor(clouds, device=cuda)\n",
    "        count += len(clouds)\n",
    "        #print(clouds.shape)\n",
    "\n",
    "        # optimise spheres and gather intermediate clouds\n",
    "        \n",
    "        morphed, losses, assignments = morph_sphere(clouds, 64, iterations, 0.01, stops, loss_func=loss_func, \n",
    "                                       return_assignment=True)\n",
    "        assignments_folders.append(assignments)\n",
    "\n",
    "        #calculate chamfer and EMD\n",
    "        print(morphed.shape)\n",
    "\n",
    "        # loop through stops\n",
    "        for i, mr in enumerate(tqdm(morphed)):\n",
    "            nn = chamferDist(clouds, mr, bidirectional=True, return_nn=True)\n",
    "            cd_loss = torch.sum(nn[1].dists) + torch.sum(nn[0].dists)\n",
    "            #cd_assignment = [nn[0].idx[0,:,0].detach().cpu().numpy(), nn[1].idx[0,:,0].detach().cpu().numpy()]\n",
    "            emd_loss, _ = calc_emd(clouds, mr, 0.05, 50)\n",
    "            #emd_assignment = emd_assignment.detach().cpu().numpy()\n",
    "\n",
    "            #print(cd_loss.item(), emd_loss.item())\n",
    "#             cd_assignments_cat.append(cd_assignment)\n",
    "#             emd_assignments_cat.append(emd_assignment)\n",
    "            chamfer_results[i] += cd_loss\n",
    "            emd_results[i] += emd_loss\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(count, chamfer_results[0])\n",
    "    chamfer_results = chamfer_results/count\n",
    "    emd_results = emd_results/count\n",
    "\n",
    "    # save results\n",
    "    with open(\"sphere/\" + loss_func + \"_metrics8.pkl\", \"wb\") as f:\n",
    "        pickle.dump([chamfer_results, emd_results, assignments_folders], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample\n",
    "shapenet_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/ShapeNetCompletion/test/complete/\"\n",
    "downsampled_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/ShapeNetCompletion/downsample/\"\n",
    "# folders = os.listdir(shapenet_path)\n",
    "# choices = np.random.choice(len(points), 4096)\n",
    "\n",
    "# for fl in tqdm(folders):\n",
    "#     if not os.path.exists(downsampled_path+fl):\n",
    "#         os.mkdir(downsampled_path+fl)\n",
    "        \n",
    "#     files = os.listdir(shapenet_path + fl)\n",
    "#     cloud =  o3d.geometry.PointCloud()\n",
    "#     for cl in files:\n",
    "#         points = np.array(o3d.io.read_point_cloud(shapenet_path + fl + \"/\" + cl).points)\n",
    "#         points = points[choices]\n",
    "#         cloud.points = o3d.utility.Vector3dVector(points)\n",
    "#         o3d.io.write_point_cloud(downsampled_path+fl + \"/\" + cl, cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = \"balanced\"\n",
    "#loss_func = \"emd\"\n",
    "\n",
    "sphere_morph_metrics(loss_func, downsampled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plots\n",
    "loss_funcs = [\"chamfer\"]\n",
    "chamfer_list, emd_list = [], []\n",
    "for loss_func in loss_funcs:\n",
    "    with open(\"sphere/\" + loss_func + \"_metrics.pkl\", \"rb\") as f:\n",
    "        chamfer, emd, assignments = pickle.load(f)\n",
    "        chamfer_list.append(chamfer)\n",
    "        emd_list.append(emd)\n",
    "        \n",
    "plot_dists(chamfer_list, loss_funcs, \"chamfer\")\n",
    "plot_dists(emd_list, loss_funcs, \"EMD\")\n",
    "        \n",
    "print(emd_list[-1], chamfer_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses on same axis\n",
    "def plot_dists(losses, labels, title):\n",
    "    x = np.arange(0, len(losses[0]))\n",
    "\n",
    "#     # scale chamfer distance to be comparable with mahalanobis\n",
    "#     max = []\n",
    "\n",
    "#     mahal_dist_max = np.max(mahal_dist_sk)\n",
    "#     chamfer_dist_max = np.max(chamfer_dist)\n",
    "#     chamfer_dist = chamfer_dist / chamfer_dist_max * mahal_dist_max\n",
    "\n",
    "    plt.figure(figsize=(30, 6))\n",
    "    for i, loss in enumerate(losses):\n",
    "        plt.plot(x, loss, label=labels[i])\n",
    "\n",
    "    plt.xlabel(\"point cloud index\")\n",
    "    plt.ylabel(\"distance\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assignments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load losses\n",
    "loss_types = [\"reverse\", \"chamfer\", \"emd\", \"pair\"]\n",
    "losses = []\n",
    "\n",
    "for loss_func in loss_types:\n",
    "    with open(\"sphere/loss_\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "        losses.append(pickle.load(f))\n",
    "        \n",
    "plot_dists(losses, loss_types, \"loss function comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure conssistency between forward and backward correspondences for chamfer distance\n",
    "# optionally compare against the ideal assignment, as measured by EMD\n",
    "def measure_assignment_consistency(assignment, emd=None):\n",
    "    reverse_assignment = torch.gather(assignment[0], 0, assignment[1])\n",
    "    expected = torch.arange(assignment[0].shape[0], device=torch.device(\"cuda\"))\n",
    "    consistency = torch.sum(torch.eq(expected, reverse_assignment).long())\n",
    "    print(\"consistency\", consistency.item(), len(torch.unique(assignment[0])), len(torch.unique(assignment[1])))\n",
    "    \n",
    "    if emd is not None:\n",
    "        #print(emd[:5], assignment[0][:5], assignment[1][:5])\n",
    "        emd_consistency = torch.sum(torch.eq(emd, assignment[0]).long())\n",
    "        #print(\"emd_consistency\", emd_consistency.item(), len(torch.unique(emd)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare correspondences\n",
    "#TODO: include consistency in top5 matches?\n",
    "loss_func = \"emd\"\n",
    "with open(\"sphere/assignments_\" + \"emd\" + \".pkl\", \"rb\") as f:\n",
    "    emd_assignment = pickle.load(f)\n",
    "\n",
    "with open(\"sphere/assignments_\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "    assignment = pickle.load(f)\n",
    "    \n",
    "for i, ass in enumerate(assignment):\n",
    "    #measure_assignment_consistency(ass, emd_assignment[i][0])\n",
    "    measure_assignment_consistency(ass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
