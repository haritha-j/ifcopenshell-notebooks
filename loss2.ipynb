{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpvL68OfBEQC"
   },
   "source": [
    "# Loss function visualisations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z7n_pw4SMWl"
   },
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ47VNF7fmTS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import os.path\n",
    "import torch\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import importlib\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import functorch\n",
    "from numpy.random import default_rng\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import interact \n",
    "import gc\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from chamferdist import ChamferDistance\n",
    "from pathlib import Path\n",
    "\n",
    "import ifcopenshell\n",
    "import open3d as o3d\n",
    "\n",
    "from src.elements import *\n",
    "from src.preparation import *\n",
    "from src.dataset import *\n",
    "from src.pointnet import *\n",
    "from src.visualisation import *\n",
    "from src.chamfer import *\n",
    "from src.utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains additional experiments on loss functions.\n",
    "\n",
    "Specifically it contains;\n",
    "\n",
    "1. Visualising point-wise distances from a loss function\n",
    "2. visualising results form point cloud reconstruction and point cloud completion methods\n",
    "\n",
    "#### data loading and pre-processing\n",
    "\n",
    "All data from sphere morphing must be loaded for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visusalise correpsondences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visaulise losses\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "# load clouds\n",
    "cld1_name = \"output/elbow/test/24102.pcd\"\n",
    "cld2_name = \"output/elbow/test/24106.pcd\"\n",
    "\n",
    "cld1 = np.array(o3d.io.read_point_cloud(cld1_name).points)\n",
    "cld2 = np.array(o3d.io.read_point_cloud(cld2_name).points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure pairing loss\n",
    "src = torch.Tensor([cld1, cld1])\n",
    "tgt = torch.Tensor([cld2, cld2])\n",
    "\n",
    "chamferDist = ChamferDistance()\n",
    "nn = chamferDist(\n",
    "    src, tgt, bidirectional=True, return_nn=True, k=1\n",
    ")\n",
    "dist = torch.sum(nn[0].dists) + torch.sum(nn[1].dists)\n",
    "\n",
    "# compute the cyclical index (closest point of closest point). this should ideally be 0->n_points in order\n",
    "perfect_idx = torch.range(0,nn[0].idx.shape[1]-1, dtype=int) # 0-> N_points\n",
    "perfect_idx = perfect_idx[:,None] # add extra dimension\n",
    "perfect_idx = perfect_idx.repeat(nn[0].idx.shape[0], 1, 1) # batch_size\n",
    "true_idx_fwd = torch.gather(nn[0].idx, 1, nn[1].idx) # tgt[[src[match]]]\n",
    "true_idx_bwd = torch.gather(nn[1].idx, 1, nn[0].idx) # tgt[[src[match]]]\n",
    "pair_loss = torch.sum(perfect_idx == true_idx_fwd)\n",
    "\n",
    "#print(nn[0].knn.shape, true_idx.shape, torch.flatten(true_idx[0]).shape)\n",
    "#pairs = torch.gather(nn[1].knn, 1, true_idx)\n",
    "paired_points_fwd = torch.stack([nn[0].knn[i][torch.flatten(nn[1].idx[i])] for i in range(nn[1].idx.shape[0])])\n",
    "#paired_points_fwd = torch.stack([nn[1].knn[i][torch.flatten(true_idx_fwd[i])] for i in range(true_idx_fwd.shape[0])])\n",
    "#paired_points_fwd = torch.stack([tgt[i][torch.flatten(true_idx_fwd[i])] for i in range(true_idx_fwd.shape[0])])\n",
    "paired_points_fwd = paired_points_fwd.reshape((paired_points_fwd.shape[0], \n",
    "                                   paired_points_fwd.shape[1], \n",
    "                                   paired_points_fwd.shape[3])) \n",
    "pair_dist_fwd = torch.sum(torch.square(paired_points_fwd - tgt))\n",
    "\n",
    "\n",
    "print(\"DS\", true_idx_bwd.shape, nn[0].knn.shape, src.shape, nn[0].idx.shape)\n",
    "paired_points_bwd = torch.stack([nn[1].knn[i][torch.flatten(nn[0].idx[i])] for i in range(nn[0].idx.shape[0])])\n",
    "#paired_points_bwd = torch.stack([src[i][torch.flatten(true_idx_bwd[i])] for i in range(true_idx_bwd.shape[0])])\n",
    "print(paired_points_bwd.shape)\n",
    "#paired_points_bwd = torch.stack([nn[0].knn[i][torch.flatten(true_idx_bwd[i])] for i in range(true_idx_bwd.shape[0])])\n",
    "paired_points_bwd = paired_points_bwd.reshape((paired_points_bwd.shape[0], \n",
    "                                   paired_points_bwd.shape[1], \n",
    "                                   paired_points_bwd.shape[3])) \n",
    "pair_dist_bwd = torch.sum(torch.square(paired_points_bwd - src))\n",
    "\n",
    "print(\"pair\", true_idx_bwd[0].flatten().shape)\n",
    "print(dist, pair_loss, pair_dist_fwd, pair_dist_bwd)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise cyclical pairs\n",
    "v = visualise_loss(cld1, cld2, blueprint, pairs=true_idx_bwd[0].flatten(), loss=\"pair\", same_cloud=True)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_idx_fwd = torch.gather(nn[0].idx, 1, nn[1].idx) # tgt[[src[match]]]\n",
    "true_idx_bwd = torch.gather(nn[1].idx, 1, nn[0].idx) # tgt[[src[match]]]\n",
    "\n",
    "# manual chamfer loss\n",
    "paired_points_bwd = torch.stack([tgt[i][torch.flatten(nn[0].idx[i])] for i in range(nn[0].idx.shape[0])])\n",
    "pair_dist_bwd = paired_points_bwd - src\n",
    "#pair_dist_bwd = torch.sum(torch.square(paired_points_bwd - x))\n",
    "#paired_points_fwd = torch.stack([x[i][torch.flatten(nn[1].idx[i])] for i in range(nn[1].idx.shape[0])])\n",
    "paired_points_fwd = torch.stack([src[i][torch.flatten(true_idx_bwd[i])] for i in range(true_idx_bwd.shape[0])])\n",
    "pair_dist_fwd = paired_points_fwd - paired_points_bwd\n",
    "\n",
    "pair_dist = pair_dist_bwd + pair_dist_fwd\n",
    "pair_dist = torch.mul(torch.square(pair_dist), torch.square(pair_dist_bwd))\n",
    "pair_dist = torch.sum(pair_dist) \n",
    "print(pair_dist, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise EMD\n",
    "src = torch.Tensor([cld1]).cuda()\n",
    "tgt = torch.Tensor([cld2]).cuda()\n",
    "loss, ass = calc_emd(src, tgt, 0.05, 1000)\n",
    "\n",
    "ass = ass.detach().cpu().numpy()\n",
    "unique = np.unique(ass)\n",
    "print(\"unique\", len(unique))\n",
    "\n",
    "v = visualise_loss(cld1, cld2, blueprint, pairs=ass.flatten(), loss=\"pair\", same_cloud=False)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise regular chamfer loss\n",
    "v = visualise_loss(cld1, cld2, blueprint)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise reverse weighted chamfer loss\n",
    "src = torch.Tensor([cld1]).cuda()\n",
    "tgt = torch.Tensor([cld2]).cuda()\n",
    "loss, ass = calc_reverse_weighted_cd_tensor(src, tgt, k=32, return_assignment=True)\n",
    "\n",
    "ass = ass[0].detach().cpu().numpy()\n",
    "\n",
    "unique = np.unique(ass)\n",
    "print(\"unique\", len(unique))\n",
    "\n",
    "v = visualise_loss(cld1, cld2, blueprint, pairs=ass, loss=\"pair\", same_cloud=False)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clouds\n",
    "cld1_name = \"data/24102s.pcd\"\n",
    "cld2_name = \"data/24103s.pcd\"\n",
    "\n",
    "cld1 = np.array(o3d.io.read_point_cloud(cld1_name).points)\n",
    "cld2 = np.array(o3d.io.read_point_cloud(cld2_name).points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualise chamfer loss with coplanarity\n",
    "target_pcd_tensor = torch.tensor([cld2], device=cuda)\n",
    "src_pcd_tensor = torch.tensor([cld1], device=cuda)\n",
    "\n",
    "vect, dists = knn_vectors(src_pcd_tensor, target_pcd_tensor, 3)\n",
    "coplanarity = check_coplanarity(vect)\n",
    "coplanarity = coplanarity[0].detach().cpu().numpy()\n",
    "print(\"shapes\", coplanarity.shape, dists.shape)\n",
    "\n",
    "v = visualise_loss(cld1, cld2, blueprint, strength=coplanarity, k=3)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visaulise direct correspondence loss\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "v = visualise_parameter_pair(predictions_list, label_list, cat, blueprint, 4)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v_s = [visualise_parameter_pair(predictions_list, label_list, cat, blueprint, i) for i in range(20)]\n",
    "v_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualise cross section correspondence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_o3d_correspondences(a, b, ass):\n",
    "    points = np.vstack([a,b])\n",
    "    start_idx = np.arange(a.shape[0])\n",
    "    end_idx = ass + a.shape[0]\n",
    "    lines = np.stack([start_idx, end_idx], axis=1)\n",
    "    print(points.shape, lines.shape, lines[:3])\n",
    "    \n",
    "    colors = [[1, 0, 0] for i in range(len(lines))]\n",
    "    line_set = o3d.geometry.LineSet(\n",
    "        points=o3d.utility.Vector3dVector(points),\n",
    "        lines=o3d.utility.Vector2iVector(lines),\n",
    "    )\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    return line_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_path = \"sphere/plane_slice2.pcd\"\n",
    "points = np.array(o3d.io.read_point_cloud(cloud_path).points)\n",
    "# flatten\n",
    "points[:,1] = 0\n",
    "gt = o3d.geometry.PointCloud()\n",
    "gt.points = o3d.utility.Vector3dVector(points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points with some random variations\n",
    "source = o3d.geometry.PointCloud()\n",
    "\n",
    "points2 = np.copy(points)\n",
    "variation = (np.random.rand(points2.shape[0], points2.shape[1]) - 0.5)/10\n",
    "variation[:,1] = 0\n",
    "points2 += variation\n",
    "source.points = o3d.utility.Vector3dVector(points2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get chamfer correspondences\n",
    "# chamferDist = ChamferDistance()\n",
    "\n",
    "# nn = chamferDist(\n",
    "#     x, y, bidirectional=True, return_nn=True)\n",
    "# assignment = nn[0].idx[:,:,0][0].cpu().numpy()\n",
    "# print(assignment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get balanced correspondences\n",
    "# chamferDist = ChamferDistance()\n",
    "# eps = 0.00001\n",
    "\n",
    "# k = 16\n",
    "# k2 = 8 # reduce k to check density in smaller patches\n",
    "# power = 1\n",
    "\n",
    "# # add a loss term for mismatched pairs\n",
    "# nn = chamferDist(\n",
    "#     x, y, bidirectional=True, return_nn=True, k=k\n",
    "# )\n",
    "\n",
    "# # measure density with itself\n",
    "# nn_x = chamferDist(x, x, bidirectional=False, return_nn=True, k=k2)\n",
    "# density_x = torch.mean(nn_x[0].dists[:,:,1:], dim=2)\n",
    "# density_x = 1 / (density_x + eps)\n",
    "# high, low = torch.max(density_x), torch.min(density_x)\n",
    "# diff = high - low\n",
    "# density_x = (density_x - low) / diff\n",
    "\n",
    "# # measure density with other cloud\n",
    "# density_xy = torch.mean(nn[0].dists[:,:,:k2-1], dim=2)\n",
    "# density_xy = 1 / (density_xy + eps)\n",
    "# high, low = torch.max(density_xy), torch.min(density_xy)\n",
    "# diff = high - low\n",
    "# density_xy = (density_xy - low) / diff\n",
    "# w_x = torch.div(density_xy, density_x)\n",
    "# #print(\"w\", w_x.shape, w_x[0])\n",
    "# w_x = torch.pow(w_x, power)\n",
    "# scaling_factors_1 = w_x.unsqueeze(2).repeat(1, 1, k)\n",
    "# multiplier = torch.gather(scaling_factors_1, 1, nn[1].idx)\n",
    "\n",
    "# scaled_dist_1 = torch.mul(nn[1].dists, multiplier)\n",
    "# scaled_dist_1x, i1 = torch.min(scaled_dist_1, 2)\n",
    "\n",
    "# # measure density with itself\n",
    "# nn_y = chamferDist(y, y, bidirectional=False, return_nn=True, k=k2)\n",
    "# density_y = torch.mean(nn_y[0].dists[:,:,1:], dim=2)\n",
    "# density_y = 1 / (density_y + eps)\n",
    "# high, low = torch.max(density_y), torch.min(density_y)\n",
    "# diff = high - low\n",
    "# density_y = (density_y - low) / diff\n",
    "\n",
    "# # measure density with other cloud\n",
    "# density_yx = torch.mean(nn[1].dists[:,:,:k2-1], dim=2)\n",
    "# density_yx = 1 / (density_yx + eps)\n",
    "# high, low = torch.max(density_yx), torch.min(density_yx)\n",
    "# diff = high - low\n",
    "# density_yx = (density_yx - low) / diff\n",
    "# w_y = torch.div(density_yx, density_y)\n",
    "# #print(\"w\", w_x.shape, w_x[0])\n",
    "# w_y = torch.pow(w_y, power)\n",
    "# scaling_factors_0 = w_y.unsqueeze(2).repeat(1, 1, k)\n",
    "# multiplier = torch.gather(scaling_factors_0, 1, nn[0].idx)\n",
    "\n",
    "# scaled_dist_0 = torch.mul(nn[0].dists, multiplier)\n",
    "# scaled_dist_0x, i0 = torch.min(scaled_dist_0, 2)\n",
    "\n",
    "# min_ind_0 = torch.gather(nn[0].idx, 2, i0.unsqueeze(2).repeat(1,1,k))[:, :, 0]\n",
    "# min_ind_1 = torch.gather(nn[1].idx, 2, i1.unsqueeze(2).repeat(1,1,k))[:, :, 0]\n",
    "\n",
    "# assignment = min_ind_0[0].cpu().numpy()\n",
    "# print(assignment.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_direct(x, y):\n",
    "    return torch.sum(torch.square(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "# morph a sphere into the shape of an input point cloud\n",
    "# by optimising chamfer loss iteratively\n",
    "# total points = num_points**2\n",
    "def optimise_shape(src_pcd_tensor, tgt_pcd_tensor, iterations=5, learning_rate=0.01, loss_func= \"chamfer\"):\n",
    "    \n",
    "    cuda = torch.device(\"cuda\")\n",
    "    \n",
    "    # optimise\n",
    "    optimizer = torch.optim.Adam([tgt_pcd_tensor], lr=learning_rate)\n",
    "    intermediate, losses, assingments = [], [], []\n",
    "    chamferDist = ChamferDistance()\n",
    "    assignments = []\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if loss_func == \"chamfer\":\n",
    "            nn = chamferDist(\n",
    "                src_pcd_tensor, tgt_pcd_tensor, bidirectional=True, return_nn=True)\n",
    "            loss = torch.sum(nn[1].dists) + torch.sum(nn[0].dists)\n",
    "            assignment = [nn[0].idx[:,:,0].detach().cpu().numpy(), nn[1].idx[:,:,0].detach().cpu().numpy()]\n",
    "        elif loss_func == \"emd\":\n",
    "            loss, assignment = calc_emd(tgt_pcd_tensor, src_pcd_tensor, 0.05, 50)\n",
    "            assignment = assignment.detach().cpu().numpy()\n",
    "        elif loss_func == \"balanced\":\n",
    "            loss, assignment = calc_balanced_chamfer_loss_tensor(src_pcd_tensor, tgt_pcd_tensor, return_assignment=True, k=4)\n",
    "        elif loss_func == \"infocd\":\n",
    "            loss, assignment = calc_cd_like_InfoV2(src_pcd_tensor, tgt_pcd_tensor, return_assignment=True)\n",
    "        elif loss_func == \"direct\":\n",
    "            loss = calc_direct(src_pcd_tensor, tgt_pcd_tensor)\n",
    "        else:\n",
    "            print(\"unspecified loss\")\n",
    "            \n",
    "        #print(\"a\", assignment[0].shape)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"iteration\", i, \"loss\", loss.item())\n",
    "        \n",
    "        intermediate.append(tgt_pcd_tensor.clone())\n",
    "\n",
    "    # calculate final chamfer loss\n",
    "    dist = chamferDist(src_pcd_tensor, tgt_pcd_tensor, bidirectional=True)\n",
    "    emd_loss, _ = calc_emd(tgt_pcd_tensor, src_pcd_tensor, 0.05, 50)\n",
    "    print(\"final chamfer dist\", dist.item(), \"emd\", emd_loss.item())\n",
    "    \n",
    "    # save assignments for analysis\n",
    "#     if measure_consistency:\n",
    "#         with open(\"sphere/assignments_\" + loss_func + \".pkl\", \"wb\") as f:\n",
    "#             pickle.dump(assingments, f)\n",
    "            \n",
    "    intermediate = torch.stack(intermediate)\n",
    "    return intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create tensors\n",
    "cuda = torch.device(\"cuda\")\n",
    "x = torch.tensor([points], device=cuda)\n",
    "y = torch.tensor([points2], device=cuda, requires_grad=True)\n",
    "\n",
    "iterations = 50\n",
    "intermediate = optimise_shape(x, y, iterations=iterations, learning_rate=0.01, loss_func=\"chamfer\")\n",
    "print(intermediate.shape)\n",
    "intermediate = intermediate.reshape((iterations, x.shape[1], x.shape[2])).detach().cpu().numpy()\n",
    "print(intermediate.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_o3d_paths(a, stops):\n",
    "    steps = stops.shape[0]\n",
    "    print(stops.shape)\n",
    "    points = np.vstack([a]+[stops[i] for i in range(steps)])\n",
    "    line_sets = []\n",
    "    #print(points.shape)\n",
    "    for j in  range(steps):\n",
    "        if j ==0:\n",
    "            start_idx = np.arange(a.shape[0])\n",
    "        else:\n",
    "            start_idx = start_idx + a.shape[0]\n",
    "        end_idx = start_idx + a.shape[0]\n",
    "        lines = np.stack([start_idx, end_idx], axis=1)\n",
    "        #print(points.shape, lines.shape, lines[:3])\n",
    "\n",
    "        colors = [[1, 1-0.1*j, 0] for i in range(len(lines))]\n",
    "        line_set = o3d.geometry.LineSet(\n",
    "            points=o3d.utility.Vector3dVector(points),\n",
    "            lines=o3d.utility.Vector2iVector(lines),\n",
    "        )\n",
    "        line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "        \n",
    "        line_sets.append(line_set)\n",
    "    \n",
    "    return line_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interm = o3d.geometry.PointCloud()\n",
    "interm.points = o3d.utility.Vector3dVector(intermediate[0])\n",
    "\n",
    "end = o3d.geometry.PointCloud()\n",
    "end.points = o3d.utility.Vector3dVector(intermediate[-1])\n",
    "\n",
    "\n",
    "source.paint_uniform_color([0., 0.706, 1])\n",
    "end.paint_uniform_color([0.7, 0.70, 0])\n",
    "interm.paint_uniform_color([0.5, 0.5, 0])\n",
    "gt.paint_uniform_color([1., 0.706, 1])\n",
    "\n",
    "#line_sets = draw_o3d_paths(points2, intermediate[1:])\n",
    "#o3d.visualization.draw_geometries([gt, source, interm] + line_sets)\n",
    "\n",
    "# start from the 2nd step onwards\n",
    "line_sets = draw_o3d_paths(points2, intermediate)\n",
    "#o3d.visualization.draw_geometries([source, gt, end] + line_sets)\n",
    "o3d.visualization.draw_geometries([ gt, end] )\n",
    "#o3d.visualization.draw_geometries([interm, gt, end] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.paint_uniform_color([0., 0.706, 1])\n",
    "gt.paint_uniform_color([1., 0.706, 1])\n",
    "gt.paint_uniform_color([1., 0.706, 1])\n",
    "\n",
    "line_set = draw_o3d_correspondences(points2, points, assignment)\n",
    "o3d.visualization.draw_geometries([gt, source, line_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate autoencoder results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise npy point cloud set\n",
    "\n",
    "# chamfer\n",
    "ch_savepath = \"/home/haritha/documents/experiments/PointSWD/logs2/reconstruction/model/modelnet40/\"\n",
    "ch_inp_list = np.load(os.path.join(ch_savepath, \"input.npy\"))\n",
    "ch_rec_list = np.load(os.path.join(ch_savepath, \"reconstruction.npy\"))\n",
    "\n",
    "# new\n",
    "savepath = \"/home/haritha/documents/experiments/PointSWD/logs24/reconstruction/model/modelnet40/\"\n",
    "inp_list = np.load(os.path.join(savepath, \"input.npy\"))\n",
    "rec_list = np.load(os.path.join(savepath, \"reconstruction.npy\"))\n",
    "\n",
    "print(rec_list.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blueprint = \"data/sample.ifc\"\n",
    "ifc = setup_ifc_file(blueprint)\n",
    "limit = 10\n",
    "vis = []\n",
    "\n",
    "for i in range(limit):\n",
    "    vis.append(vis_ifc_and_cloud(ifc, [ch_inp_list[i].astype(\"float64\"), ch_rec_list[i].astype(\"float64\")]))\n",
    "\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blueprint = \"data/sample.ifc\"\n",
    "ifc = setup_ifc_file(blueprint)\n",
    "limit = 10\n",
    "vis = []\n",
    "\n",
    "for i in range(limit):\n",
    "    vis.append(vis_ifc_and_cloud(ifc, [inp_list[i].astype(\"float64\"), rec_list[i].astype(\"float64\")]))\n",
    "\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare losses\n",
    "def evaluate_autoencoder_results(inp_list, rec_list, loss_funcs, batch_size=None):\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    if batch_size == None:\n",
    "        batch_size = len(inp_list)\n",
    "        \n",
    "    # create empty dict\n",
    "    all_losses = {func:[] for func in loss_funcs}\n",
    "    \n",
    "    # split into batches\n",
    "    for i in tqdm(range(math.ceil(len(inp_list)/batch_size))):\n",
    "        a = i*batch_size\n",
    "        b = min(a+batch_size, len(inp_list))\n",
    "        #print(a,b)\n",
    "        \n",
    "        # compute losses\n",
    "        inp_tensor = torch.tensor(inp_list[a:b], device=cuda)\n",
    "        rec_tensor = torch.tensor(rec_list[a:b], device=cuda)\n",
    "        losses = calculate_3d_loss(inp_tensor, rec_tensor, loss_funcs)\n",
    "        \n",
    "        # sum losses\n",
    "        for k, v in losses.items():\n",
    "            all_losses[k].append(v)\n",
    "            \n",
    "    # average losses\n",
    "    for k in all_losses:\n",
    "        all_losses[k] = np.average(np.array(all_losses[k]))\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "loss_funcs = [\"chamfer\", \"reverse\", \"emd\"]\n",
    "\n",
    "# chamfer\n",
    "ch_loss = evaluate_autoencoder_results(ch_inp_list, ch_rec_list, loss_funcs, 512)\n",
    "\n",
    "# new\n",
    "loss = evaluate_autoencoder_results(inp_list, rec_list, loss_funcs, 512)\n",
    "\n",
    "print(\"chamfer tuned\", ch_loss)\n",
    "print(\"new\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### completion evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DCD (VCN plus, MVP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce density colourmaps that are normalised with their pairs\n",
    "\n",
    "# get densities\n",
    "def get_density(clouds):\n",
    "    # compute nearest neighbours to calculated density\n",
    "    clouds = torch.tensor(clouds, device=\"cuda\")\n",
    "    chamferDist = ChamferDistance()\n",
    "    nn = chamferDist(clouds, clouds, bidirectional=False, return_nn=True, k=32)\n",
    "    \n",
    "    density = torch.mean(nn[0].dists[:,:,1:], dim=2)\n",
    "    eps = 0.00001\n",
    "    density = 1 / (density + eps)\n",
    "    return density\n",
    "\n",
    "\n",
    "# normalise for each example across prediction sets\n",
    "def normalise_densities(density_sets):\n",
    "    densities = torch.stack(density_sets)\n",
    "    highs, lows = torch.max(densities, 2).values, torch.min(densities, 2).values\n",
    "    highs, lows = torch.max(highs, 0).values, torch.min(lows, 0).values\n",
    "    #print(densities.shape, highs.shape)\n",
    "    \n",
    "    #highs = torch.reshape(highs, densities.shape)\n",
    "    highs = highs.unsqueeze(0).unsqueeze(-1)\n",
    "    highs = highs.expand(densities.shape[0], densities.shape[1], densities.shape[2])\n",
    "    lows = lows.unsqueeze(0).unsqueeze(-1)\n",
    "    lows = lows.expand(densities.shape[0], densities.shape[1], densities.shape[2])\n",
    "    diff = highs - lows\n",
    "    densities = (densities - lows) / diff\n",
    "    \n",
    "    return densities[0], densities[1], densities[2], densities[3]\n",
    "    \n",
    "\n",
    "# represent density with colour and combine with point cloud\n",
    "def get_coloured_clouds(clouds, density, colormap_name='plasma_r'):\n",
    "    density = density.detach().cpu().numpy()\n",
    "    colours = np.zeros((density.shape[0], density.shape[1], 4))\n",
    "    colormap = plt.get_cmap(colormap_name)\n",
    "    \n",
    "    for i, cloud in enumerate(density):\n",
    "        for j, pt in enumerate(cloud):\n",
    "            colours[i,j] = colormap(pt)\n",
    "            \n",
    "#     clouds = clouds.detach().cpu().numpy()\n",
    "    colours = colours[:,:,:3]\n",
    "    pcds = []\n",
    "    \n",
    "    for i, cl in enumerate(clouds):\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(cl)\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colours[i])\n",
    "        pcds.append(pcd)\n",
    "    \n",
    "    return pcds, colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cloud_list_vcn(path, prefix):\n",
    "    limit = 20\n",
    "    cloud_sets = []\n",
    "    \n",
    "    for i in range(limit):\n",
    "        f_name = \"fine\"+str(i)+\".pkl\"\n",
    "        with open(path + f_name, \"rb\") as f:\n",
    "            pred, partial, gt = pickle.load(f)\n",
    "            if prefix == \"pred\":\n",
    "                clouds = pred\n",
    "            elif prefix == \"gt\":\n",
    "                clouds = gt\n",
    "            else:\n",
    "                clouds = partial\n",
    "        cloud_sets.append(clouds.detach().cpu().numpy())\n",
    "    cloud_sets = np.vstack(cloud_sets)\n",
    "    print(cloud_sets.shape)\n",
    "    \n",
    "    return cloud_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_path = \"/home/haritha/documents/experiments/Density_aware_Chamfer_Distance/outputs_balanced/\"\n",
    "dcd_path = \"/home/haritha/documents/experiments/Density_aware_Chamfer_Distance/outputs_dcd/\"\n",
    "\n",
    "cld_balanced = get_cloud_list_vcn(balanced_path, \"pred\")\n",
    "cld_dcd = get_cloud_list_vcn(dcd_path, \"pred\")\n",
    "cld_gt = get_cloud_list_vcn(dcd_path, \"gt\")\n",
    "cld_partial = get_cloud_list_vcn(dcd_path, \"partial\")\n",
    "\n",
    "d_balanced = get_density(cld_balanced)\n",
    "d_dcd = get_density(cld_dcd)\n",
    "d_gt = get_density(cld_gt)\n",
    "d_partial = get_density(cld_partial)\n",
    "\n",
    "print(d_partial.shape)\n",
    "\n",
    "# produce density colourmaps that are normalised with their pairs\n",
    "d_balanced, d_dcd, d_gt, d_partial = normalise_densities([d_balanced, d_dcd, d_gt, d_partial])\n",
    "\n",
    "v_balanced, _ = get_coloured_clouds(cld_balanced, d_balanced)\n",
    "v_dcd, _ = get_coloured_clouds(cld_dcd, d_dcd)\n",
    "v_gt, col = get_coloured_clouds(cld_gt, d_gt)\n",
    "v_partial, _ = get_coloured_clouds(cld_partial, d_partial)\n",
    "# c_dcd = get_colours(d_dcd)\n",
    "# c_gt = get_colours(d_gt)\n",
    "# c_partial = get_colours(d_partial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pointAttN\n",
    "# balanced_path = \"/home/haritha/documents/experiments/PointAttN/outputs/\"\n",
    "# dcd_path = \"/home/haritha/documents/experiments/PointAttN/outputs_cd/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_side_by_side(v1, v2, v3, v4, i):\n",
    "    # shift points\n",
    "    cl1, cl2, cl3, cl4 = v1[i], v2[i], v3[i], v4[i]\n",
    "    cl1.points = o3d.utility.Vector3dVector(np.array(cl1.points) - np.array([1,0,0]))\n",
    "    cl3.points = o3d.utility.Vector3dVector(np.array(cl3.points) + np.array([1,0,0]))\n",
    "    cl4.points = o3d.utility.Vector3dVector(np.array(cl4.points) + np.array([2,0,0]))\n",
    "    \n",
    "    o3d.visualization.draw_geometries([cl1, cl2, cl3, cl4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(v_balanced))\n",
    "#shortlist = [226,227,228,102,103,104,0,1,2] = [39,140,219,106,1,7,148,156,11,43,53,194,4,17,91,44,66,3,27,274,87,55,158,35,103,129,112,170,195]\n",
    "shortlist = [39,140,7,148,53,194,91,274,55,158,129,112,195]\n",
    "#for i in range(250,300):\n",
    "for i in shortlist:\n",
    "    print(i)\n",
    "    view_side_by_side(v_gt, v_dcd, v_balanced, v_partial, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a desired set of clouds\n",
    "def save_cloud(clouds,  name, i, colours=None):\n",
    "    directory = \"mitsuba/\"\n",
    "#     pcd = o3d.geometry.PointCloud()\n",
    "#     pcd.points = o3d.utility.Vector3dVector(clouds[i])\n",
    "#     o3d.io.write_point_cloud(directory + name + str(i) + \".ply\", pcd)\n",
    "    with open(directory + name + str(i) + \".npy\", \"wb\") as f:\n",
    "        np.save(f, clouds[i])\n",
    "    if colours is not None:\n",
    "        with open(directory + name + str(i) + \"c_.npy\", \"wb\") as f:\n",
    "            np.save(f, colours[i])\n",
    "    \n",
    "i = 4\n",
    "# save_cloud(cld_balanced, \"balanced\", i)\n",
    "# save_cloud(cld_dcd, \"dcd\", i)\n",
    "# save_cloud(cld_partial, \"partial\", i)\n",
    "# save_cloud(cld_gt, \"gt\", i)\n",
    "\n",
    "for i in shortlist:\n",
    "    save_cloud(cld_gt, \"gt\", i+1)\n",
    "    save_cloud(cld_balanced, \"bal\", i+1)\n",
    "    save_cloud(cld_dcd, \"dcd\", i+1)\n",
    "    save_cloud(cld_partial, \"part\", i+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_cloud_list_vcn(path, prefix, ifc, col=0):\n",
    "    limit = 2\n",
    "    vis = []\n",
    "    \n",
    "    for i in range(limit):\n",
    "        f_name = \"fine\"+str(i)+\".pkl\"\n",
    "        with open(path + f_name, \"rb\") as f:\n",
    "            pred, partial, gt = pickle.load(f)\n",
    "            if prefix == \"pred\":\n",
    "                clouds = pred\n",
    "            elif prefix == \"gt\":\n",
    "                clouds = gt\n",
    "            else:\n",
    "                clouds = partial\n",
    "        for cl in clouds:\n",
    "            cloud_list = [None, None, None]\n",
    "            cloud_list[col] = cl.detach().cpu().numpy().astype(np.double)\n",
    "            vis.append(vis_ifc_and_cloud(ifc, cloud_list))\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_path = \"/home/haritha/documents/experiments/Density_aware_Chamfer_Distance/outputs_balanced/\"\n",
    "dcd_path = \"/home/haritha/documents/experiments/Density_aware_Chamfer_Distance/outputs_dcd/\"\n",
    "blueprint = \"data/sample.ifc\"\n",
    "ifc = setup_ifc_file(blueprint)\n",
    "\n",
    "v_balanced = view_cloud_list_vcn(balanced_path, \"pred\", ifc, 0)\n",
    "v_dcd = view_cloud_list_vcn(dcd_path, \"pred\", ifc, 1)\n",
    "v_gt = view_cloud_list_vcn(dcd_path, \"gt\", ifc, 2)\n",
    "v_partial = view_cloud_list_vcn(dcd_path, \"partial\", ifc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(v_balanced))\n",
    "for i in range(len(v_balanced)):\n",
    "    print(v_balanced[i], v_dcd[i], v_gt[i], v_partial[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([point_cloud, gt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"devices\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InfoCD (Seedformer, PCN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "\n",
    "# load NPYs from each. (GT, Complete, CD, InfoCD, UniformCD)\n",
    "category_id = \"02933112/\"\n",
    "cd_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/pcn/train_pcn_Log_2024_02_22_14_49_07/outputs_cd/\"\n",
    "info_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/pcn/train_pcn_Log_2024_02_22_14_49_07/outputs_info/\"\n",
    "uniform_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/pcn/train_pcn_Log_2024_02_29_23_43_45/outputs_uniform/\"\n",
    "\n",
    "blueprint = \"data/sample.ifc\"\n",
    "ifc = setup_ifc_file(blueprint)\n",
    "\n",
    "def view_cloud_list_seedformer(category_id, path, prefix, ifc, col=0):\n",
    "    files = os.listdir(os.path.join(path,category_id))\n",
    "    files_filtered = [f for f in files if prefix in f]\n",
    "    files_filtered.sort()\n",
    "    #print(len(files), len(files_filtered))\n",
    "    \n",
    "    limit = 10\n",
    "    vis = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files_filtered:\n",
    "        count +=1\n",
    "        if count == limit:\n",
    "            break\n",
    "        cloud = np.load(os.path.join(path, category_id, f))\n",
    "        cloud_list = [None, None, None]\n",
    "        cloud_list[col] = cloud.astype(\"float64\")\n",
    "        vis.append(vis_ifc_and_cloud(ifc, cloud_list))\n",
    "    return vis\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v_cd = view_cloud_list_seedformer(category_id, cd_path, \"pred\", ifc, 0)\n",
    "v_info = view_cloud_list_seedformer(category_id, info_path, \"pred\", ifc, 1)\n",
    "v_uniform = view_cloud_list_seedformer(category_id, uniform_path, \"pred\", ifc, 2)\n",
    "v_complete = view_cloud_list_seedformer(category_id, cd_path, \"gt\", ifc, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(v_cd)):\n",
    "    print(v_cd[i], v_info[i], v_uniform[i], v_complete[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
