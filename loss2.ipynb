{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpvL68OfBEQC"
   },
   "source": [
    "# Loss function visualisations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z7n_pw4SMWl"
   },
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ47VNF7fmTS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import os.path\n",
    "import torch\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import importlib\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import functorch\n",
    "from numpy.random import default_rng\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import interact\n",
    "import gc\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from chamferdist import ChamferDistance\n",
    "from pathlib import Path\n",
    "import einops\n",
    "\n",
    "import ifcopenshell\n",
    "import open3d as o3d\n",
    "\n",
    "from src.elements import *\n",
    "from src.preparation import *\n",
    "from src.dataset import *\n",
    "from src.pointnet import *\n",
    "from src.visualisation import *\n",
    "from src.chamfer import *\n",
    "from src.utils import *\n",
    "from src.completion_vis import *\n",
    "import transforms3d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains additional experiments on loss functions.\n",
    "\n",
    "Specifically it contains;\n",
    "\n",
    "1. Visualising point-wise distances from a loss function\n",
    "2. visualising results form point cloud reconstruction and point cloud completion methods\n",
    "\n",
    "#### data loading and pre-processing\n",
    "\n",
    "All data from sphere morphing must be loaded for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visusalise correpsondences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visaulise losses\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "# load clouds\n",
    "cld1_name = \"output/elbow/test/24102.pcd\"\n",
    "cld2_name = \"output/elbow/test/24106.pcd\"\n",
    "\n",
    "cld1 = np.array(o3d.io.read_point_cloud(cld1_name).points)\n",
    "cld2 = np.array(o3d.io.read_point_cloud(cld2_name).points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure pairing loss\n",
    "src = torch.Tensor([cld1, cld1])\n",
    "tgt = torch.Tensor([cld2, cld2])\n",
    "\n",
    "chamferDist = ChamferDistance()\n",
    "nn = chamferDist(\n",
    "    src, tgt, bidirectional=True, return_nn=True, k=1\n",
    ")\n",
    "dist = torch.sum(nn[0].dists) + torch.sum(nn[1].dists)\n",
    "\n",
    "# compute the cyclical index (closest point of closest point). this should ideally be 0->n_points in order\n",
    "perfect_idx = torch.range(0,nn[0].idx.shape[1]-1, dtype=int) # 0-> N_points\n",
    "perfect_idx = perfect_idx[:,None] # add extra dimension\n",
    "perfect_idx = perfect_idx.repeat(nn[0].idx.shape[0], 1, 1) # batch_size\n",
    "true_idx_fwd = torch.gather(nn[0].idx, 1, nn[1].idx) # tgt[[src[match]]]\n",
    "true_idx_bwd = torch.gather(nn[1].idx, 1, nn[0].idx) # tgt[[src[match]]]\n",
    "pair_loss = torch.sum(perfect_idx == true_idx_fwd)\n",
    "\n",
    "#print(nn[0].knn.shape, true_idx.shape, torch.flatten(true_idx[0]).shape)\n",
    "#pairs = torch.gather(nn[1].knn, 1, true_idx)\n",
    "paired_points_fwd = torch.stack([nn[0].knn[i][torch.flatten(nn[1].idx[i])] for i in range(nn[1].idx.shape[0])])\n",
    "#paired_points_fwd = torch.stack([nn[1].knn[i][torch.flatten(true_idx_fwd[i])] for i in range(true_idx_fwd.shape[0])])\n",
    "#paired_points_fwd = torch.stack([tgt[i][torch.flatten(true_idx_fwd[i])] for i in range(true_idx_fwd.shape[0])])\n",
    "paired_points_fwd = paired_points_fwd.reshape((paired_points_fwd.shape[0], \n",
    "                                   paired_points_fwd.shape[1], \n",
    "                                   paired_points_fwd.shape[3])) \n",
    "pair_dist_fwd = torch.sum(torch.square(paired_points_fwd - tgt))\n",
    "\n",
    "\n",
    "print(\"DS\", true_idx_bwd.shape, nn[0].knn.shape, src.shape, nn[0].idx.shape)\n",
    "paired_points_bwd = torch.stack([nn[1].knn[i][torch.flatten(nn[0].idx[i])] for i in range(nn[0].idx.shape[0])])\n",
    "#paired_points_bwd = torch.stack([src[i][torch.flatten(true_idx_bwd[i])] for i in range(true_idx_bwd.shape[0])])\n",
    "print(paired_points_bwd.shape)\n",
    "#paired_points_bwd = torch.stack([nn[0].knn[i][torch.flatten(true_idx_bwd[i])] for i in range(true_idx_bwd.shape[0])])\n",
    "paired_points_bwd = paired_points_bwd.reshape((paired_points_bwd.shape[0], \n",
    "                                   paired_points_bwd.shape[1], \n",
    "                                   paired_points_bwd.shape[3])) \n",
    "pair_dist_bwd = torch.sum(torch.square(paired_points_bwd - src))\n",
    "\n",
    "print(\"pair\", true_idx_bwd[0].flatten().shape)\n",
    "print(dist, pair_loss, pair_dist_fwd, pair_dist_bwd)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise cyclical pairs\n",
    "v = visualise_loss(cld1, cld2, blueprint, pairs=true_idx_bwd[0].flatten(), loss=\"pair\", same_cloud=True)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_idx_fwd = torch.gather(nn[0].idx, 1, nn[1].idx) # tgt[[src[match]]]\n",
    "true_idx_bwd = torch.gather(nn[1].idx, 1, nn[0].idx) # tgt[[src[match]]]\n",
    "\n",
    "# manual chamfer loss\n",
    "paired_points_bwd = torch.stack([tgt[i][torch.flatten(nn[0].idx[i])] for i in range(nn[0].idx.shape[0])])\n",
    "pair_dist_bwd = paired_points_bwd - src\n",
    "#pair_dist_bwd = torch.sum(torch.square(paired_points_bwd - x))\n",
    "#paired_points_fwd = torch.stack([x[i][torch.flatten(nn[1].idx[i])] for i in range(nn[1].idx.shape[0])])\n",
    "paired_points_fwd = torch.stack([src[i][torch.flatten(true_idx_bwd[i])] for i in range(true_idx_bwd.shape[0])])\n",
    "pair_dist_fwd = paired_points_fwd - paired_points_bwd\n",
    "\n",
    "pair_dist = pair_dist_bwd + pair_dist_fwd\n",
    "pair_dist = torch.mul(torch.square(pair_dist), torch.square(pair_dist_bwd))\n",
    "pair_dist = torch.sum(pair_dist) \n",
    "print(pair_dist, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise EMD\n",
    "src = torch.Tensor([cld1]).cuda()\n",
    "tgt = torch.Tensor([cld2]).cuda()\n",
    "loss, ass = calc_emd(src, tgt, 0.05, 1000)\n",
    "\n",
    "ass = ass.detach().cpu().numpy()\n",
    "unique = np.unique(ass)\n",
    "print(\"unique\", len(unique))\n",
    "\n",
    "v = visualise_loss(cld1, cld2, blueprint, pairs=ass.flatten(), loss=\"pair\", same_cloud=False)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise regular chamfer loss\n",
    "v = visualise_loss(cld1, cld2, blueprint)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise reverse weighted chamfer loss\n",
    "src = torch.Tensor([cld1]).cuda()\n",
    "tgt = torch.Tensor([cld2]).cuda()\n",
    "loss, ass = calc_reverse_weighted_cd_tensor(src, tgt, k=32, return_assignment=True)\n",
    "\n",
    "ass = ass[0].detach().cpu().numpy()\n",
    "\n",
    "unique = np.unique(ass)\n",
    "print(\"unique\", len(unique))\n",
    "\n",
    "v = visualise_loss(cld1, cld2, blueprint, pairs=ass, loss=\"pair\", same_cloud=False)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clouds\n",
    "cld1_name = \"data/24102s.pcd\"\n",
    "cld2_name = \"data/24103s.pcd\"\n",
    "\n",
    "cld1 = np.array(o3d.io.read_point_cloud(cld1_name).points)\n",
    "cld2 = np.array(o3d.io.read_point_cloud(cld2_name).points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualise chamfer loss with coplanarity\n",
    "target_pcd_tensor = torch.tensor([cld2], device=cuda)\n",
    "src_pcd_tensor = torch.tensor([cld1], device=cuda)\n",
    "\n",
    "vect, dists = knn_vectors(src_pcd_tensor, target_pcd_tensor, 3)\n",
    "coplanarity = check_coplanarity(vect)\n",
    "coplanarity = coplanarity[0].detach().cpu().numpy()\n",
    "print(\"shapes\", coplanarity.shape, dists.shape)\n",
    "\n",
    "v = visualise_loss(cld1, cld2, blueprint, strength=coplanarity, k=3)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visaulise direct correspondence loss\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "v = visualise_parameter_pair(predictions_list, label_list, cat, blueprint, 4)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v_s = [visualise_parameter_pair(predictions_list, label_list, cat, blueprint, i) for i in range(20)]\n",
    "v_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualise cross section correspondence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_o3d_correspondences(a, b, ass):\n",
    "    points = np.vstack([a,b])\n",
    "    start_idx = np.arange(a.shape[0])\n",
    "    end_idx = ass + a.shape[0]\n",
    "    lines = np.stack([start_idx, end_idx], axis=1)\n",
    "    print(points.shape, lines.shape, lines[:3])\n",
    "    \n",
    "    colors = [[1, 0, 0] for i in range(len(lines))]\n",
    "    line_set = o3d.geometry.LineSet(\n",
    "        points=o3d.utility.Vector3dVector(points),\n",
    "        lines=o3d.utility.Vector2iVector(lines),\n",
    "    )\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    return line_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_path = \"sphere/plane_slice2.pcd\"\n",
    "points = np.array(o3d.io.read_point_cloud(cloud_path).points)\n",
    "# flatten\n",
    "points[:,1] = 0\n",
    "gt = o3d.geometry.PointCloud()\n",
    "gt.points = o3d.utility.Vector3dVector(points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points with some random variations\n",
    "source = o3d.geometry.PointCloud()\n",
    "\n",
    "points2 = np.copy(points)\n",
    "variation = (np.random.rand(points2.shape[0], points2.shape[1]) - 0.5)/10\n",
    "variation[:,1] = 0\n",
    "points2 += variation\n",
    "source.points = o3d.utility.Vector3dVector(points2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get chamfer correspondences\n",
    "# chamferDist = ChamferDistance()\n",
    "\n",
    "# nn = chamferDist(\n",
    "#     x, y, bidirectional=True, return_nn=True)\n",
    "# assignment = nn[0].idx[:,:,0][0].cpu().numpy()\n",
    "# print(assignment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get balanced correspondences\n",
    "# chamferDist = ChamferDistance()\n",
    "# eps = 0.00001\n",
    "\n",
    "# k = 16\n",
    "# k2 = 8 # reduce k to check density in smaller patches\n",
    "# power = 1\n",
    "\n",
    "# # add a loss term for mismatched pairs\n",
    "# nn = chamferDist(\n",
    "#     x, y, bidirectional=True, return_nn=True, k=k\n",
    "# )\n",
    "\n",
    "# # measure density with itself\n",
    "# nn_x = chamferDist(x, x, bidirectional=False, return_nn=True, k=k2)\n",
    "# density_x = torch.mean(nn_x[0].dists[:,:,1:], dim=2)\n",
    "# density_x = 1 / (density_x + eps)\n",
    "# high, low = torch.max(density_x), torch.min(density_x)\n",
    "# diff = high - low\n",
    "# density_x = (density_x - low) / diff\n",
    "\n",
    "# # measure density with other cloud\n",
    "# density_xy = torch.mean(nn[0].dists[:,:,:k2-1], dim=2)\n",
    "# density_xy = 1 / (density_xy + eps)\n",
    "# high, low = torch.max(density_xy), torch.min(density_xy)\n",
    "# diff = high - low\n",
    "# density_xy = (density_xy - low) / diff\n",
    "# w_x = torch.div(density_xy, density_x)\n",
    "# #print(\"w\", w_x.shape, w_x[0])\n",
    "# w_x = torch.pow(w_x, power)\n",
    "# scaling_factors_1 = w_x.unsqueeze(2).repeat(1, 1, k)\n",
    "# multiplier = torch.gather(scaling_factors_1, 1, nn[1].idx)\n",
    "\n",
    "# scaled_dist_1 = torch.mul(nn[1].dists, multiplier)\n",
    "# scaled_dist_1x, i1 = torch.min(scaled_dist_1, 2)\n",
    "\n",
    "# # measure density with itself\n",
    "# nn_y = chamferDist(y, y, bidirectional=False, return_nn=True, k=k2)\n",
    "# density_y = torch.mean(nn_y[0].dists[:,:,1:], dim=2)\n",
    "# density_y = 1 / (density_y + eps)\n",
    "# high, low = torch.max(density_y), torch.min(density_y)\n",
    "# diff = high - low\n",
    "# density_y = (density_y - low) / diff\n",
    "\n",
    "# # measure density with other cloud\n",
    "# density_yx = torch.mean(nn[1].dists[:,:,:k2-1], dim=2)\n",
    "# density_yx = 1 / (density_yx + eps)\n",
    "# high, low = torch.max(density_yx), torch.min(density_yx)\n",
    "# diff = high - low\n",
    "# density_yx = (density_yx - low) / diff\n",
    "# w_y = torch.div(density_yx, density_y)\n",
    "# #print(\"w\", w_x.shape, w_x[0])\n",
    "# w_y = torch.pow(w_y, power)\n",
    "# scaling_factors_0 = w_y.unsqueeze(2).repeat(1, 1, k)\n",
    "# multiplier = torch.gather(scaling_factors_0, 1, nn[0].idx)\n",
    "\n",
    "# scaled_dist_0 = torch.mul(nn[0].dists, multiplier)\n",
    "# scaled_dist_0x, i0 = torch.min(scaled_dist_0, 2)\n",
    "\n",
    "# min_ind_0 = torch.gather(nn[0].idx, 2, i0.unsqueeze(2).repeat(1,1,k))[:, :, 0]\n",
    "# min_ind_1 = torch.gather(nn[1].idx, 2, i1.unsqueeze(2).repeat(1,1,k))[:, :, 0]\n",
    "\n",
    "# assignment = min_ind_0[0].cpu().numpy()\n",
    "# print(assignment.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_direct(x, y):\n",
    "    return torch.sum(torch.square(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "# morph a sphere into the shape of an input point cloud\n",
    "# by optimising chamfer loss iteratively\n",
    "# total points = num_points**2\n",
    "def optimise_shape(src_pcd_tensor, tgt_pcd_tensor, iterations=5, learning_rate=0.01, loss_func= \"chamfer\"):\n",
    "    \n",
    "    cuda = torch.device(\"cuda\")\n",
    "    \n",
    "    # optimise\n",
    "    optimizer = torch.optim.Adam([tgt_pcd_tensor], lr=learning_rate)\n",
    "    intermediate, losses, assingments = [], [], []\n",
    "    chamferDist = ChamferDistance()\n",
    "    assignments = []\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if loss_func == \"chamfer\":\n",
    "            nn = chamferDist(\n",
    "                src_pcd_tensor, tgt_pcd_tensor, bidirectional=True, return_nn=True)\n",
    "            loss = torch.sum(nn[1].dists) + torch.sum(nn[0].dists)\n",
    "            assignment = [nn[0].idx[:,:,0].detach().cpu().numpy(), nn[1].idx[:,:,0].detach().cpu().numpy()]\n",
    "        elif loss_func == \"emd\":\n",
    "            loss, assignment = calc_emd(tgt_pcd_tensor, src_pcd_tensor, 0.05, 50)\n",
    "            assignment = assignment.detach().cpu().numpy()\n",
    "        elif loss_func == \"balanced\":\n",
    "            loss, assignment = calc_balanced_chamfer_loss_tensor(src_pcd_tensor, tgt_pcd_tensor, return_assignment=True, k=4)\n",
    "        elif loss_func == \"infocd\":\n",
    "            loss, assignment = calc_cd_like_InfoV2(src_pcd_tensor, tgt_pcd_tensor, return_assignment=True)\n",
    "        elif loss_func == \"direct\":\n",
    "            loss = calc_direct(src_pcd_tensor, tgt_pcd_tensor)\n",
    "        else:\n",
    "            print(\"unspecified loss\")\n",
    "            \n",
    "        #print(\"a\", assignment[0].shape)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"iteration\", i, \"loss\", loss.item())\n",
    "        \n",
    "        intermediate.append(tgt_pcd_tensor.clone())\n",
    "\n",
    "    # calculate final chamfer loss\n",
    "    dist = chamferDist(src_pcd_tensor, tgt_pcd_tensor, bidirectional=True)\n",
    "    emd_loss, _ = calc_emd(tgt_pcd_tensor, src_pcd_tensor, 0.05, 50)\n",
    "    print(\"final chamfer dist\", dist.item(), \"emd\", emd_loss.item())\n",
    "    \n",
    "    # save assignments for analysis\n",
    "#     if measure_consistency:\n",
    "#         with open(\"sphere/assignments_\" + loss_func + \".pkl\", \"wb\") as f:\n",
    "#             pickle.dump(assingments, f)\n",
    "            \n",
    "    intermediate = torch.stack(intermediate)\n",
    "    return intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create tensors\n",
    "cuda = torch.device(\"cuda\")\n",
    "x = torch.tensor([points], device=cuda)\n",
    "y = torch.tensor([points2], device=cuda, requires_grad=True)\n",
    "\n",
    "iterations = 50\n",
    "intermediate = optimise_shape(x, y, iterations=iterations, learning_rate=0.01, loss_func=\"chamfer\")\n",
    "print(intermediate.shape)\n",
    "intermediate = intermediate.reshape((iterations, x.shape[1], x.shape[2])).detach().cpu().numpy()\n",
    "print(intermediate.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_o3d_paths(a, stops):\n",
    "    steps = stops.shape[0]\n",
    "    print(stops.shape)\n",
    "    points = np.vstack([a]+[stops[i] for i in range(steps)])\n",
    "    line_sets = []\n",
    "    #print(points.shape)\n",
    "    for j in  range(steps):\n",
    "        if j ==0:\n",
    "            start_idx = np.arange(a.shape[0])\n",
    "        else:\n",
    "            start_idx = start_idx + a.shape[0]\n",
    "        end_idx = start_idx + a.shape[0]\n",
    "        lines = np.stack([start_idx, end_idx], axis=1)\n",
    "        #print(points.shape, lines.shape, lines[:3])\n",
    "\n",
    "        colors = [[1, 1-0.1*j, 0] for i in range(len(lines))]\n",
    "        line_set = o3d.geometry.LineSet(\n",
    "            points=o3d.utility.Vector3dVector(points),\n",
    "            lines=o3d.utility.Vector2iVector(lines),\n",
    "        )\n",
    "        line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "        \n",
    "        line_sets.append(line_set)\n",
    "    \n",
    "    return line_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interm = o3d.geometry.PointCloud()\n",
    "interm.points = o3d.utility.Vector3dVector(intermediate[0])\n",
    "\n",
    "end = o3d.geometry.PointCloud()\n",
    "end.points = o3d.utility.Vector3dVector(intermediate[-1])\n",
    "\n",
    "\n",
    "source.paint_uniform_color([0., 0.706, 1])\n",
    "end.paint_uniform_color([0., 0.706, 1])\n",
    "interm.paint_uniform_color([0.5, 0.5, 0])\n",
    "gt.paint_uniform_color([1., 0.706, 1])\n",
    "\n",
    "#line_sets = draw_o3d_paths(points2, intermediate[1:])\n",
    "#o3d.visualization.draw_geometries([gt, source, interm] + line_sets)\n",
    "\n",
    "# start from the 2nd step onwards\n",
    "line_sets = draw_o3d_paths(points2, intermediate)\n",
    "#o3d.visualization.draw_geometries([source, gt, end] + line_sets)\n",
    "#o3d.visualization.draw_geometries([source, gt] )\n",
    "o3d.visualization.draw_geometries([end, gt] )\n",
    "#o3d.visualization.draw_geometries([interm, gt, end] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.paint_uniform_color([0., 0.706, 1])\n",
    "gt.paint_uniform_color([1., 0.706, 1])\n",
    "gt.paint_uniform_color([1., 0.706, 1])\n",
    "\n",
    "line_set = draw_o3d_correspondences(points2, points, assignment)\n",
    "o3d.visualization.draw_geometries([gt, source, line_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate autoencoder results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise npy point cloud set\n",
    "\n",
    "# chamfer\n",
    "ch_savepath = \"/home/haritha/documents/experiments/PointSWD/logs2/reconstruction/model/modelnet40/\"\n",
    "ch_inp_list = np.load(os.path.join(ch_savepath, \"input.npy\"))\n",
    "ch_rec_list = np.load(os.path.join(ch_savepath, \"reconstruction.npy\"))\n",
    "\n",
    "# new\n",
    "savepath = \"/home/haritha/documents/experiments/PointSWD/logs24/reconstruction/model/modelnet40/\"\n",
    "inp_list = np.load(os.path.join(savepath, \"input.npy\"))\n",
    "rec_list = np.load(os.path.join(savepath, \"reconstruction.npy\"))\n",
    "\n",
    "print(rec_list.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blueprint = \"data/sample.ifc\"\n",
    "ifc = setup_ifc_file(blueprint)\n",
    "limit = 10\n",
    "vis = []\n",
    "\n",
    "for i in range(limit):\n",
    "    vis.append(vis_ifc_and_cloud(ifc, [ch_inp_list[i].astype(\"float64\"), ch_rec_list[i].astype(\"float64\")]))\n",
    "\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blueprint = \"data/sample.ifc\"\n",
    "ifc = setup_ifc_file(blueprint)\n",
    "limit = 10\n",
    "vis = []\n",
    "\n",
    "for i in range(limit):\n",
    "    vis.append(vis_ifc_and_cloud(ifc, [inp_list[i].astype(\"float64\"), rec_list[i].astype(\"float64\")]))\n",
    "\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare losses\n",
    "def evaluate_autoencoder_results(inp_list, rec_list, loss_funcs, batch_size=None):\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    if batch_size == None:\n",
    "        batch_size = len(inp_list)\n",
    "        \n",
    "    # create empty dict\n",
    "    all_losses = {func:[] for func in loss_funcs}\n",
    "    \n",
    "    # split into batches\n",
    "    for i in tqdm(range(math.ceil(len(inp_list)/batch_size))):\n",
    "        a = i*batch_size\n",
    "        b = min(a+batch_size, len(inp_list))\n",
    "        #print(a,b)\n",
    "        \n",
    "        # compute losses\n",
    "        inp_tensor = torch.tensor(inp_list[a:b], device=cuda)\n",
    "        rec_tensor = torch.tensor(rec_list[a:b], device=cuda)\n",
    "        losses = calculate_3d_loss(inp_tensor, rec_tensor, loss_funcs)\n",
    "        \n",
    "        # sum losses\n",
    "        for k, v in losses.items():\n",
    "            all_losses[k].append(v)\n",
    "            \n",
    "    # average losses\n",
    "    for k in all_losses:\n",
    "        all_losses[k] = np.average(np.array(all_losses[k]))\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "loss_funcs = [\"chamfer\", \"reverse\", \"emd\"]\n",
    "\n",
    "# chamfer\n",
    "ch_loss = evaluate_autoencoder_results(ch_inp_list, ch_rec_list, loss_funcs, 512)\n",
    "\n",
    "# new\n",
    "loss = evaluate_autoencoder_results(inp_list, rec_list, loss_funcs, 512)\n",
    "\n",
    "print(\"chamfer tuned\", ch_loss)\n",
    "print(\"new\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### completion evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DCD (VCN plus, MVP) / PointAttn (PCN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCD(VCN)\n",
    "balanced_path = \"/home/haritha/documents/experiments/Density_aware_Chamfer_Distance/outputs_balanced/\"\n",
    "dcd_path = \"/home/haritha/documents/experiments/Density_aware_Chamfer_Distance/outputs_dcd/\"\n",
    "\n",
    "#pointAttN\n",
    "# balanced_path = \"/home/haritha/documents/experiments/PointAttN/outputs/\"\n",
    "# dcd_path = \"/home/haritha/documents/experiments/PointAttN/outputs_cd/\"\n",
    "\n",
    "cld_balanced = get_cloud_list_vcn(balanced_path, \"pred\")\n",
    "cld_dcd = get_cloud_list_vcn(dcd_path, \"pred\")\n",
    "cld_gt = get_cloud_list_vcn(dcd_path, \"gt\")\n",
    "cld_partial = get_cloud_list_vcn(dcd_path, \"partial\")\n",
    "\n",
    "d_balanced = get_density(cld_balanced)\n",
    "d_dcd = get_density(cld_dcd)\n",
    "d_gt = get_density(cld_gt)\n",
    "d_partial = get_density(cld_partial)\n",
    "\n",
    "print(d_partial.shape, d_gt.shape)\n",
    "\n",
    "# produce density colourmaps that are normalised with their pairs\n",
    "d_balanced, d_dcd, d_gt, d_partial = normalise_densities([d_balanced, d_dcd, d_gt, d_partial])\n",
    "\n",
    "v_balanced, _ = get_coloured_clouds(cld_balanced, d_balanced)\n",
    "v_dcd, _ = get_coloured_clouds(cld_dcd, d_dcd)\n",
    "v_gt, col = get_coloured_clouds(cld_gt, d_gt)\n",
    "v_partial, _ = get_coloured_clouds(cld_partial, d_partial)\n",
    "# c_dcd = get_colours(d_dcd)\n",
    "# c_gt = get_colours(d_gt)\n",
    "# c_partial = get_colours(d_partial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d_partial.shape, d_gt.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(v_balanced))\n",
    "#shortlist = [226,227,228,102,103,104,0,1,2] = [39,140,219,106,1,7,148,156,11,43,53,194,4,17,91,44,66,3,27,274,87,55,158,35,103,129,112,170,195]\n",
    "#shortlist = [39,140,7,148,53,194,91,274,55,158,129,112,195]\n",
    "for i in range(100,150):\n",
    "#for i in shortlist:\n",
    "    print(i)\n",
    "    view_side_by_side(v_partial, v_dcd, v_balanced, v_gt, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a desired set of clouds\n",
    "def save_cloud(clouds,  name, i, colours=None):\n",
    "    directory = \"mitsuba/\"\n",
    "#     pcd = o3d.geometry.PointCloud()\n",
    "#     pcd.points = o3d.utility.Vector3dVector(clouds[i])\n",
    "#     o3d.io.write_point_cloud(directory + name + str(i) + \".ply\", pcd)\n",
    "    with open(directory + name + str(i) + \".npy\", \"wb\") as f:\n",
    "        np.save(f, clouds[i])\n",
    "    if colours is not None:\n",
    "        with open(directory + name + str(i) + \"c_.npy\", \"wb\") as f:\n",
    "            np.save(f, colours[i])\n",
    "\n",
    "i = 4\n",
    "# save_cloud(cld_balanced, \"balanced\", i)\n",
    "# save_cloud(cld_dcd, \"dcd\", i)\n",
    "# save_cloud(cld_partial, \"partial\", i)\n",
    "# save_cloud(cld_gt, \"gt\", i)\n",
    "\n",
    "for i in shortlist:\n",
    "    save_cloud(cld_gt, \"gt\", i+1)\n",
    "    save_cloud(cld_balanced, \"bal\", i+1)\n",
    "    save_cloud(cld_dcd, \"dcd\", i+1)\n",
    "    save_cloud(cld_partial, \"part\", i+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_path = \"/home/haritha/documents/experiments/Density_aware_Chamfer_Distance/outputs_balanced/\"\n",
    "dcd_path = \"/home/haritha/documents/experiments/Density_aware_Chamfer_Distance/outputs_dcd/\"\n",
    "blueprint = \"data/sample.ifc\"\n",
    "ifc = setup_ifc_file(blueprint)\n",
    "\n",
    "v_balanced = view_cloud_list_vcn(balanced_path, \"pred\", ifc, 0)\n",
    "v_dcd = view_cloud_list_vcn(dcd_path, \"pred\", ifc, 1)\n",
    "v_gt = view_cloud_list_vcn(dcd_path, \"gt\", ifc, 2)\n",
    "v_partial = view_cloud_list_vcn(dcd_path, \"partial\", ifc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(v_balanced))\n",
    "for i in range(len(v_balanced)):\n",
    "    print(v_balanced[i], v_dcd[i], v_gt[i], v_partial[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([point_cloud, gt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InfoCD (Seedformer, PCN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "\n",
    "# load NPYs from each. (GT, Complete, CD, InfoCD, UniformCD)\n",
    "category_id = \"02933112/\"\n",
    "cd_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/pcn/train_pcn_Log_2024_02_22_14_49_07/outputs_cd/\"\n",
    "info_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/pcn/train_pcn_Log_2024_02_22_14_49_07/outputs_info/\"\n",
    "uniform_path = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/pcn/train_pcn_Log_2024_02_29_23_43_45/outputs_uniform/\"\n",
    "\n",
    "blueprint = \"data/sample.ifc\"\n",
    "ifc = setup_ifc_file(blueprint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v_cd = view_cloud_list_seedformer(category_id, cd_path, \"pred\", ifc, 0)\n",
    "v_info = view_cloud_list_seedformer(category_id, info_path, \"pred\", ifc, 1)\n",
    "v_uniform = view_cloud_list_seedformer(category_id, uniform_path, \"pred\", ifc, 2)\n",
    "v_complete = view_cloud_list_seedformer(category_id, cd_path, \"gt\", ifc, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedformer_output_path = \"/home/haritha/documents/experiments/seedformer_clone/ICCV2023-HyperCD/output_vis/bbox/02691156/\"\n",
    "\n",
    "view_seedformer_outputs(seedformer_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise completion results and their bboxes\n",
    "#DCD(VCN)\n",
    "#balanced_path = \"/home/haritha/documents/experiments/Density_aware_Chamfer_Distance/outputs/\"\n",
    "# dcd_path = \"/home/haritha/documents/experiments/Density_aware_Chamfer_Distance/outputs_dcd/\"\n",
    "\n",
    "#pointAttN\n",
    "balanced_path = \"/home/haritha/documents/experiments/PointAttN/outputs/\"\n",
    "#balanced_path = \"/home/haritha/documents/experiments/PointAttN/outputs_corrected_bbox/\"\n",
    "#dcd_path = \"/home/haritha/documents/experiments/PointAttN/outputs_cd/\"\n",
    "\n",
    "cld_completed = get_cloud_list_vcn(balanced_path, \"pred\")\n",
    "cld_gt = get_cloud_list_vcn(balanced_path, \"gt\")\n",
    "cld_partial = get_cloud_list_vcn(balanced_path, \"partial\")\n",
    "cld_partial = cld_partial.transpose(0, 2, 1)\n",
    "\n",
    "print(cld_partial.shape)\n",
    "\n",
    "completed_bboxes = get_bboxes(cld_completed)\n",
    "gt_bboxes = get_bboxes(cld_gt)\n",
    "partial_bboxes = get_bboxes(cld_partial)\n",
    "\n",
    "limit = 15\n",
    "#visualize_point_clouds_with_bboxes_and_cameras(cld_completed[:limit], cld_partial[:limit], cld_gt[:limit], completed_bboxes, gt_bboxes, partial_bboxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_point_clouds_with_bboxes_and_cameras(cld_completed[:limit], cld_partial[:limit], cld_gt[:limit], completed_bboxes, gt_bboxes, partial_bboxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and filter by bounding box overlap between ground truth and partial cloud\n",
    "def calc_bbox_overlap(output, gt):\n",
    "    pred_min_values, _ = torch.min(output, dim=1)\n",
    "    pred_max_values, _ = torch.max(output, dim=1)\n",
    "    gt_min_values, _ = torch.min(gt, dim=1)\n",
    "    gt_max_values, _ = torch.max(gt, dim=1)\n",
    "\n",
    "    intersection_min = torch.max(pred_min_values, gt_min_values)\n",
    "    intersection_max = torch.min(pred_max_values, gt_max_values)\n",
    "    intersection_dims = torch.clamp(intersection_max - intersection_min, min=0)\n",
    "    intersection_volume = torch.prod(intersection_dims, dim=-1)\n",
    "\n",
    "    pred_volume = torch.prod(pred_max_values - pred_min_values, dim=-1)\n",
    "    gt_volume = torch.prod(gt_max_values - gt_min_values, dim=-1)\n",
    "    union_volume = pred_volume + gt_volume - intersection_volume\n",
    "    iou = intersection_volume / union_volume\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "\n",
    "cld_partial_t = torch.tensor(cld_partial).cuda()\n",
    "cld_gt_t = torch.tensor(cld_gt).cuda()\n",
    "overlap = calc_bbox_overlap(cld_partial_t, cld_gt_t)\n",
    "print(overlap.shape, overlap.max(), overlap.min(), overlap.mean())\n",
    "\n",
    "threshold = 0.8\n",
    "above_threshold = overlap > threshold\n",
    "print(above_threshold.sum().item())\n",
    "\n",
    "# plot the distribution of bbox overlaps\n",
    "plt.hist(overlap.cpu().numpy(), bins=50, color='#72195a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise bbox only (frozen) predictions (pointattN)\n",
    "data_path = \"/home/haritha/documents/experiments/PointAttN/outputs_frozen_bbox/\"\n",
    "\n",
    "cld_completed = get_cloud_list_vcn(data_path, \"pred\", pred_bbox=True)\n",
    "cld_gt = get_cloud_list_vcn(data_path, \"gt\", pred_bbox=True)\n",
    "pred_bboxes = get_cloud_list_vcn(data_path, \"pred_bbox\", pred_bbox=True)\n",
    "cld_partial = get_cloud_list_vcn(data_path, \"partial\", pred_bbox=True)\n",
    "cld_partial = cld_partial.transpose(0, 2, 1)\n",
    "\n",
    "print(cld_partial.shape)\n",
    "\n",
    "gt_bboxes = get_bboxes(cld_gt)\n",
    "print(\"gt bbox\", gt_bboxes.shape)\n",
    "completed_bboxes = convert_bboxes_to_min_max(pred_bboxes)\n",
    "\n",
    "\n",
    "limit = 5\n",
    "visualize_point_clouds_with_bboxes_and_cameras(cld_completed[:limit], cld_partial[:limit], cld_gt[:limit], completed_bboxes, gt_bboxes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualise camera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise camera predictions (pointattN)\n",
    "#data_path = \"/home/haritha/documents/experiments/PointAttN/outputs_frozen_camera/\"\n",
    "data_path = \"/home/haritha/documents/experiments/PointAttN/outputs_camera_original/\"\n",
    "data_path_ucd = \"/home/haritha/documents/experiments/PointAttN/outputs_camera_ucd/\"\n",
    "data_path_cd = \"/home/haritha/documents/experiments/PointAttN/outputs_camera_cd/\"\n",
    "\n",
    "limit = 100\n",
    "\n",
    "pred_camera_cd = get_cloud_list_vcn(data_path_cd, \"pred_camera\", pred_camera=True, limit=limit)\n",
    "pred_camera_ucd = get_cloud_list_vcn(data_path_ucd, \"pred_camera\", pred_camera=True, limit=limit)\n",
    "pred_camera_original = get_cloud_list_vcn(data_path, \"pred_camera\", pred_camera=True, limit=limit)\n",
    "\n",
    "cld_completed = get_cloud_list_vcn(data_path, \"pred\", pred_camera=True, limit=limit)\n",
    "cld_gt = get_cloud_list_vcn(data_path, \"gt\", pred_camera=True, limit=limit)\n",
    "gt_camera = get_cloud_list_vcn(data_path, \"gt_camera\", pred_camera=True, limit=limit)\n",
    "cld_partial = get_cloud_list_vcn(data_path, \"partial\", pred_camera=True, limit=limit)\n",
    "cld_partial = cld_partial.transpose(0, 2, 1)\n",
    "\n",
    "#print(gt_camera.shape)\n",
    "\n",
    "#print(\"gt\", gt_camera, \"pred\", pred_camera_ucd)\n",
    "visualize_point_clouds_with_bboxes_and_cameras(cld_gt[:limit], cld_partial[:limit],\n",
    "                                  cameras1=pred_camera_original[:limit], cameras2=pred_camera_cd[:limit],\n",
    "                                  cameras3=pred_camera_ucd[:limit], cameras4=gt_camera[:limit])\n",
    "# visualize_point_clouds_with_bboxes_and_cameras(cld_completed[:limit], cld_partial[:limit], cld_gt[:limit],\n",
    "#                                  cameras1=pred_camera[:limit], cameras2=gt_camera[:limit])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_dir = \"/home/haritha/documents/experiments/pcn/pcn_camera/test/pose/\"\n",
    "pcd_dir = \"/home/haritha/documents/experiments/pcn/pcn_camera/test/pcd/\"\n",
    "file_name = \"02958343/1b23106e1c447efc36c8c5d8a85eb4f9/0\"\n",
    "\n",
    "# load camera parameters and point cloud\n",
    "with open(pose_dir + file_name + \".txt\", \"rb\") as f:\n",
    "    pose = np.loadtxt(f)\n",
    "    location, direction = extract_camera_location_and_direction(pose)\n",
    "\n",
    "print(pose)\n",
    "pcd = o3d.io.read_point_cloud(pcd_dir + file_name + \".pcd\")\n",
    "pcd = np.asarray(pcd.points)\n",
    "# visualise\n",
    "visualize_point_cloud_with_camera_axis(pcd, location, direction)\n",
    "\n",
    "\n",
    "#visualize_point_cloud_with_camera_cone(pcd, gt_camera[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that transformations for the camera are correct\n",
    "\n",
    "# perform transformations\n",
    "rnd_value = np.random.uniform(0, 1)\n",
    "\n",
    "trfm_mat = transforms3d.zooms.zfdir2mat(1)\n",
    "trfm_mat_x = np.dot(transforms3d.zooms.zfdir2mat(-1, [1, 0, 0]), trfm_mat)\n",
    "trfm_mat_z = np.dot(transforms3d.zooms.zfdir2mat(-1, [0, 0, 1]), trfm_mat)\n",
    "if rnd_value <= 0.25:\n",
    "    trfm_mat = np.dot(trfm_mat_x, trfm_mat)\n",
    "    trfm_mat = np.dot(trfm_mat_z, trfm_mat)\n",
    "elif rnd_value > 0.25 and rnd_value <= 0.5:\n",
    "    trfm_mat = np.dot(trfm_mat_x, trfm_mat)\n",
    "elif rnd_value > 0.5 and rnd_value <= 0.75:\n",
    "    trfm_mat = np.dot(trfm_mat_z, trfm_mat)\n",
    "\n",
    "# Apply transformation to point clouds\n",
    "pcd_tr = np.dot(pcd, trfm_mat.T)\n",
    "\n",
    "# Transform camera position and axis\n",
    "transformed_camera_position = np.dot(location, trfm_mat.T)\n",
    "transformed_camera_axis = np.dot(direction, trfm_mat.T)\n",
    "transformed_camera_axis = transformed_camera_axis / np.linalg.norm(transformed_camera_axis)  # Ensure it's a unit vector\n",
    "\n",
    "# visualise results\n",
    "visualize_point_cloud_with_camera_axis(pcd_tr, transformed_camera_position, transformed_camera_axis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confidence visualisations and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# normalise for each example across prediction sets\n",
    "def normalise_confidences(confidences, log=True):\n",
    "    #print(confidences.shape)\n",
    "    if log:\n",
    "        confidences = np.log(confidences + 1)\n",
    "    high = np.max(confidences)*0.8\n",
    "    low = np.min(confidences)\n",
    "    # high = 0.11\n",
    "    # low = 0\n",
    "    print(\"high\", high)\n",
    "    confidences = (confidences - low) / (high - low)\n",
    "\n",
    "    return confidences\n",
    "\n",
    "\n",
    "# visualise confidence predictions (pointattN)\n",
    "balanced_path = \"/home/haritha/documents/experiments/PointAttN/outputs_confidence/\"\n",
    "#balanced_path = \"/home/haritha/documents/experiments/PointAttN/outputs_corrected_bbox/\"\n",
    "#dcd_path = \"/home/haritha/documents/experiments/PointAttN/outputs_cd/\"\n",
    "load_idx = True\n",
    "limit = 149\n",
    "cld_completed = get_cloud_list_vcn(balanced_path, \"pred\", pred_confidence=True, limit=limit, load_idx=load_idx)\n",
    "cld_gt = get_cloud_list_vcn(balanced_path, \"gt\", pred_confidence=True, limit=limit, load_idx=load_idx)\n",
    "cld_partial = get_cloud_list_vcn(balanced_path, \"partial\", pred_confidence=True, limit=limit, load_idx=load_idx)\n",
    "confidences = get_cloud_list_vcn(balanced_path, \"confidence\", pred_confidence=True, limit=limit, load_idx=load_idx)\n",
    "idxs = get_cloud_list_vcn(balanced_path, \"idx\", pred_confidence=True, limit=limit, load_idx=load_idx)\n",
    "cld_partial = cld_partial.transpose(0, 2, 1)\n",
    "\n",
    "#print(cld_partial.shape)\n",
    "\n",
    "completed_bboxes = get_bboxes(cld_completed)\n",
    "gt_bboxes = get_bboxes(cld_gt)\n",
    "partial_bboxes = get_bboxes(cld_partial)\n",
    "\n",
    "limit = 2000\n",
    "\n",
    "# normalise densities and colour clouds\n",
    "confidences = normalise_confidences(confidences[:limit])\n",
    "\n",
    "#v_completed, _ = get_coloured_clouds(cld_completed[:limit], confidences[:limit])\n",
    "\n",
    "# visualize_point_clouds_with_bboxes_and_cameras(v_completed[:limit], cld_partial[:limit], cld_gt[:limit],\n",
    "#                                                completed_bboxes, gt_bboxes, partial_bboxes, paint_preds=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_idx:\n",
    "    for i, id in enumerate(idxs):\n",
    "        print(i, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(confidences), np.min(confidences), np.max(confidences[12]), np.min(confidences[12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### confidence plotting\n",
    "print(max(confidences.flatten()))\n",
    "# plot the distribution of confidence values\n",
    "#plt.hist(confidences.flatten(), bins=500, color='#72195a')\n",
    "\n",
    "# compute pointwise chamfer distance\n",
    "def calc_pointwise_chamfer_distance(pred, gt):\n",
    "    chamferDist = ChamferDistance()\n",
    "    nn = chamferDist(\n",
    "        pred, gt, bidirectional=True, return_nn=True)\n",
    "    return nn[0].dists\n",
    "\n",
    "print(cld_completed.shape, cld_gt.shape)\n",
    "cld_completed_t = torch.tensor(cld_completed).cuda()\n",
    "cld_gt_t = torch.tensor(cld_gt).cuda()\n",
    "\n",
    "# calculate pointwise chamfer distance\n",
    "pointwise_cd = calc_pointwise_chamfer_distance(cld_completed_t, cld_gt_t).squeeze().detach().cpu().numpy()\n",
    "print(\"pointwise cd\", pointwise_cd.shape, confidences.shape)\n",
    "\n",
    "# pointwise_cd = pointwise_cd.flatten()\n",
    "# confidences = confidences.flatten()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.exponential(scale=1.0, size=1000)\n",
    "y = np.random.exponential(scale=1.0, size=1000)\n",
    "\n",
    "log_x = np.log(x + 1)  # Adding 1 to avoid log(0)\n",
    "log_y = np.log(y + 1)\n",
    "\n",
    "# Scatter plot of log-transformed data\n",
    "plt.scatter(confidences, np.log(pointwise_cd+1), alpha=0.5, color='purple')\n",
    "#plt.scatter(log_x, log_y, alpha=0.5, color='purple')\n",
    "\n",
    "plt.title('Log-Transformed Scatter Plot of Two Negative Exponential Distributions')\n",
    "plt.xlabel('Log of Distribution 1 (x)')\n",
    "plt.ylabel('Log of Distribution 2 (y)')\n",
    "plt.show()\n",
    "\n",
    "# # Create a scatter plot to visualize correlation\n",
    "# plt.scatter(x, y, alpha=0.5, color='purple')\n",
    "# plt.title('Scatter Plot of Two Negative Exponential Distributions')\n",
    "# plt.xlabel('Distribution 1 (x)')\n",
    "# plt.ylabel('Distribution 2 (y)')\n",
    "# plt.show()\n",
    "\n",
    "# # Optionally, calculate and print the correlation coefficient\n",
    "correlation = np.corrcoef(confidences, pointwise_cd)[0, 1]\n",
    "print(f\"Correlation coefficient: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X and Y are the input arrays of shape [n, m]\n",
    "def split_and_calculate_means(X, Y, threshold):\n",
    "    # Initialize arrays to store the means and standard deviations\n",
    "    mean_set1 = np.zeros(X.shape[0])\n",
    "    std_set1 = np.zeros(X.shape[0])\n",
    "    mean_set2 = np.zeros(X.shape[0])\n",
    "    std_set2 = np.zeros(X.shape[0])\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        # Apply threshold to create masks\n",
    "        mask_set1 = X[i] < threshold\n",
    "        mask_set2 = ~mask_set1\n",
    "\n",
    "        # Calculate mean and std for the first subset (where X < threshold)\n",
    "        if np.any(mask_set1):\n",
    "            mean_set1[i] = np.mean(Y[i][mask_set1])\n",
    "            std_set1[i] = np.std(Y[i][mask_set1])\n",
    "        else:\n",
    "            mean_set1[i] = np.nan\n",
    "            std_set1[i] = np.nan\n",
    "\n",
    "        # Calculate mean and std for the second subset (where X >= threshold)\n",
    "        if np.any(mask_set2):\n",
    "            mean_set2[i] = np.mean(Y[i][mask_set2])\n",
    "            std_set2[i] = np.std(Y[i][mask_set2])\n",
    "        else:\n",
    "            mean_set2[i] = np.nan\n",
    "            std_set2[i] = np.nan\n",
    "\n",
    "    return mean_set1, std_set1, mean_set2, std_set2\n",
    "\n",
    "# Example usage\n",
    "n, m = 5, 10  # Example dimensions\n",
    "X = np.random.rand(n, m)  # Random example data\n",
    "Y = np.random.rand(n, m)\n",
    "threshold = 0.075 # Example threshold\n",
    "\n",
    "mean_set1, std_set1, mean_set2, std_set2  = split_and_calculate_means(confidences, pointwise_cd, threshold)\n",
    "# print(\"Mean values for set 1 (X < threshold):\", mean_set1)\n",
    "# print(\"Mean values for set 2 (X >= threshold):\", mean_set2)\n",
    "\n",
    "print(mean_set1.shape, mean_set2.shape)\n",
    "print(max(mean_set1), min(mean_set1), max(mean_set2), min(mean_set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_means(mean_set1, std_set1, mean_set2, std_set2):\n",
    "    # Get the indices that would sort mean_set2 in descending order\n",
    "    sorted_indices = np.argsort(mean_set2)[::-1]  # Descending order\n",
    "\n",
    "    # Sort all means and standard deviations based on the sorted indices\n",
    "    sorted_mean_set1 = mean_set1[sorted_indices]\n",
    "    sorted_std_set1 = std_set1[sorted_indices]\n",
    "    sorted_mean_set2 = mean_set2[sorted_indices]\n",
    "    sorted_std_set2 = std_set2[sorted_indices]\n",
    "\n",
    "    return sorted_mean_set1, sorted_std_set1, sorted_mean_set2, sorted_std_set2\n",
    "\n",
    "# Modified plotting function to show shaded areas for standard deviations with marker size control\n",
    "def plot_mean_sets_with_std(mean_set1, std_set1, mean_set2, std_set2, marker_size=8):\n",
    "    n = len(mean_set1)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    x_values = np.arange(n)\n",
    "\n",
    "    # Plot mean_set2 with markers only\n",
    "    plt.plot(x_values, mean_set2, label='High uncertainty points', marker='o', linestyle='None', \n",
    "             color='tomato', markersize=marker_size, linewidth=2)\n",
    "\n",
    "    # Fill between mean +/- std for set 2\n",
    "    plt.fill_between(x_values, mean_set2 - std_set2/2, mean_set2 + std_set2/2, color='tomato', alpha=0.3)\n",
    "\n",
    "    # Plot mean_set1 with markers only\n",
    "    plt.plot(x_values, mean_set1, label='Low uncertainty points', marker='o', linestyle='None', \n",
    "             color='blueviolet', markersize=marker_size, linewidth=2)\n",
    "\n",
    "    # Fill between mean +/- std for set 1\n",
    "    plt.fill_between(x_values, mean_set1 - std_set1/2, mean_set1 + std_set1/2, color='blueviolet', alpha=0.4)\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.title('Chamfer distance distribution mean plot', fontsize=15)\n",
    "    plt.xlabel('Point Cloud Index', fontsize=14)\n",
    "    plt.ylabel('Point-wise Chamfer distance', fontsize=14)\n",
    "\n",
    "    plt.xticks(fontsize=12)  # X-axis tick labels font size\n",
    "    plt.yticks(fontsize=12)  # Y-axis tick labels font size\n",
    "\n",
    "    # Add grid and legend\n",
    "    plt.grid(True)\n",
    "    plt.legend(fontsize=14)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "# Sort the means and standard deviations based on mean_set2 in descending order\n",
    "sorted_mean_set1, sorted_std_set1, sorted_mean_set2, sorted_std_set2 = sort_means(mean_set1, std_set1, mean_set2, std_set2)\n",
    "\n",
    "# Plot the sorted means and standard deviations\n",
    "plot_mean_sets_with_std(sorted_mean_set1, sorted_std_set1, sorted_mean_set2, sorted_std_set2, marker_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### visualise confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 150\n",
    "limit = 175\n",
    "\n",
    "v_completed, _ = get_coloured_clouds(cld_completed[start:limit], confidences[start:limit])\n",
    "\n",
    "print(len(v_completed))\n",
    "\n",
    "visualize_point_clouds_with_bboxes_and_cameras(v_completed, cld_partial[start:limit],\n",
    "                                               cld_gt[start:limit], completed_bboxes[start:limit],\n",
    "                                               gt_bboxes[start:limit],\n",
    "                                               paint_preds=False)\n",
    "\n",
    "#visualize_point_clouds_with_bboxes_and_cameras(v_completed, paint_preds=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cloud with open io\n",
    "#print(v_completed[0].points)\n",
    "# gt_chair = o3d.geometry.PointCloud()\n",
    "# gt_chair.points = o3d.utility.Vector3dVector(cld_partial[12])\n",
    "# o3d.io.write_point_cloud(\"completion/cahir_gt.pcd\", gt_chair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_random_pad_point_clouds(point_clouds, masks):\n",
    "    \"\"\"\n",
    "    Filter point clouds using masks and pad them by randomly repeating existing points \n",
    "    to the size of the largest filtered point cloud.\n",
    "\n",
    "    Parameters:\n",
    "    - point_clouds (np.ndarray): Array of shape (b, n, 3) where `b` is the batch size, \n",
    "                                 `n` is the number of points, and 3 are the coordinates (x, y, z).\n",
    "    - masks (np.ndarray): Boolean array of shape (b, n) where True indicates the point is kept.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: The padded point clouds with uniform size within the batch.\n",
    "    \"\"\"\n",
    "    batch_size = point_clouds.shape[0]\n",
    "    filtered_point_clouds = [point_clouds[i][masks[i]] for i in range(batch_size)]\n",
    "    max_points = max(len(pc) for pc in filtered_point_clouds)  # Find the maximum size\n",
    "\n",
    "    # Pad each filtered point cloud to have `max_points` by randomly repeating points\n",
    "    padded_point_clouds = []\n",
    "    for pc in filtered_point_clouds:\n",
    "        if len(pc) < max_points:\n",
    "            # Randomly choose indices from the existing points to repeat\n",
    "            indices_to_repeat = np.random.choice(len(pc), size=max_points - len(pc), replace=True)\n",
    "            repeated_points = pc[indices_to_repeat]\n",
    "            # Append the repeated points to the original filtered points\n",
    "            padded_pc = np.vstack([pc, repeated_points])\n",
    "        else:\n",
    "            padded_pc = pc\n",
    "        padded_point_clouds.append(padded_pc)\n",
    "\n",
    "    return np.array(padded_point_clouds)\n",
    "\n",
    "# clip using confidence\n",
    "def clip_by_confidence(cld, confidence, threshold=0.1):\n",
    "    #confidence = np.log(confidence + 1)\n",
    "    #print(cld.shape, confidence)\n",
    "    #high = np.max(confidence)\n",
    "    high = 1.\n",
    "    confidence = confidence / high\n",
    "    print(\"high\", high)\n",
    "    #threshold = threshold * high\n",
    "    mask = confidence < threshold\n",
    "    #mask = einops.repeat(mask, 'b n -> b n c', c=3)\n",
    "    print(\"conf\", confidence.shape, np.max(confidence, axis=1))\n",
    "    print(\"mask\", mask.shape, np.sum(mask, axis=1))\n",
    "    clipped_cld = filter_and_random_pad_point_clouds(cld, mask)\n",
    "    #cld = cld[mask]\n",
    "    print(clipped_cld.shape)\n",
    "    return clipped_cld\n",
    "\n",
    "start = 15\n",
    "limit = 35\n",
    "cld_completed_clipped = clip_by_confidence(cld_completed[start:limit], confidences[start:limit],\n",
    "                                           threshold=0.75)\n",
    "\n",
    "v_completed, _ = get_coloured_clouds(cld_completed_clipped, confidences[start:limit])\n",
    "\n",
    "print(cld_completed_clipped.shape)\n",
    "clipped_bboxes = get_bboxes(cld_completed_clipped)\n",
    "\n",
    "visualize_point_clouds_with_bboxes_and_cameras(cld_completed[start:limit], cld_completed_clipped,\n",
    "                                               cld_gt[start:limit], completed_bboxes[start:limit],\n",
    "                                               gt_bboxes[start:limit], clipped_bboxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confidence distributions of clouds\n",
    "def plot_confidence_distributions(data, num_bins=100):\n",
    "    \"\"\"\n",
    "    Plots the distribution of confidence values for each pointset with normalized axes.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): A numpy array of shape (batchsize, 16384) containing confidence values.\n",
    "    - num_bins (int): The number of bins to use in the histogram.\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size = data.shape[0]\n",
    "    global_min = np.min(data)\n",
    "    global_max = np.max(data)\n",
    "\n",
    "    # Pre-calculate histogram data to find global maximum frequency\n",
    "    hist_data = [np.histogram(data[i], bins=num_bins, range=(global_min, global_max)) for i in range(batch_size)]\n",
    "    max_frequency = max([max(hist[0]) for hist in hist_data])  # Maximum frequency across all histograms\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(data[i], bins=num_bins, range=(global_min, global_max), alpha=0.75, color='blue', edgecolor='black')\n",
    "        plt.title(f'Distribution of Confidence Values for Pointset {i+1}')\n",
    "        plt.xlabel('Confidence Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True)\n",
    "        plt.ylim(0, max_frequency + max_frequency * 0.1)  # Setting the y-axis limit slightly higher for aesthetics\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plot_confidence_distributions(confidences[:limit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kitti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointattn_kitt_path = \"/home/haritha/documents/experiments/PointAttN/outputs_kitti/\"\n",
    "\n",
    "cld_completed = load_kitti_results_pointattn(pointattn_kitt_path, \"pred\")\n",
    "bbox = load_kitti_results_pointattn(pointattn_kitt_path, \"bbox\")\n",
    "cld_partial = load_kitti_results_pointattn(pointattn_kitt_path, \"partial\")\n",
    "\n",
    "cld_partial = cld_partial.transpose(0, 2, 1)\n",
    "completed_bboxes = get_bboxes(cld_completed)\n",
    "gt_bboxes = get_bboxes(bbox)\n",
    "#print(gt_bboxes.shape)\n",
    "\n",
    "limit = 10\n",
    "#visualize_point_clouds_with_bboxes_and_cameras(cld_completed[:limit], cld_partial[:limit], None, completed_bboxes, gt_bboxes)\n",
    "visualize_point_clouds_with_bboxes_and_cameras(cld_completed[45:50], cld_partial[45:50], None, completed_bboxes[45:50], gt_bboxes[45:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FIX ORIENTATION, AND PICK SOME CLOUDS TO TEST ON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IFC sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifcopenshell\n",
    "import pywavefront\n",
    "import numpy as np\n",
    "import uuid\n",
    "import base64\n",
    "\n",
    "def create_ifc_guid(): return ifcopenshell.guid.compress(uuid.uuid1().hex)\n",
    "\n",
    "\n",
    "def setup_ifc_file(schema_version, blueprint):\n",
    "    \"\"\"Setup the basic IFC file structure with a project, site, and geometric context.\"\"\"\n",
    "\n",
    "    ifc = ifcopenshell.open(blueprint)\n",
    "    new_ifc = ifcopenshell.file(schema=schema_version)\n",
    "\n",
    "    project = ifc.by_type(\"IfcProject\")[0]\n",
    "    context = ifc.by_type(\"IfcGeometricRepresentationContext\")[0]\n",
    "    new_ifc.createIfcOwnerHistory()\n",
    "\n",
    "    new_ifc.add(project)\n",
    "    new_ifc.add(context)\n",
    "\n",
    "    owner_history = new_ifc.by_type(\"IfcOwnerHistory\")[0]\n",
    "    project = new_ifc.by_type(\"IfcProject\")[0]\n",
    "    project.Name = \"Road Infrastructure Project\"\n",
    "    context = new_ifc.by_type(\"IfcGeometricRepresentationContext\")[0]\n",
    "\n",
    "    # ifc_file = ifcopenshell.file(schema=schema_version)\n",
    "    # owner_history = ifc_file.createIfcOwnerHistory()\n",
    "    # project = ifc_file.createIfcProject(create_ifc_guid(), owner_history, Name=\"Road Infrastructure Project\")\n",
    "    site = new_ifc.createIfcSite(create_ifc_guid(), owner_history, Name=\"Site for Road Signs\")\n",
    "    new_ifc.createIfcRelAggregates(create_ifc_guid(), owner_history, RelatingObject=project, RelatedObjects=[site])\n",
    "\n",
    "    # # Ensure CoordinateSpaceDimension is passed as an integer, not a string\n",
    "    # world_coordinate_system = ifc_file.createIfcAxis2Placement3D(ifc_file.createIfcCartesianPoint([0.0, 0.0, 0.0]))\n",
    "    # context = ifc_file.createIfcGeometricRepresentationContext(None, \"Model\", 3, 1e-5, world_coordinate_system)  # Corrected argument order and types\n",
    "    # ifc_file.createIfcRelAggregates(create_ifc_guid(), owner_history, RelatingObject=project, RelatedObjects=[context])\n",
    "\n",
    "    return new_ifc, project, site, context\n",
    "\n",
    "\n",
    "def load_obj_data(obj_file):\n",
    "    \"\"\"Load the OBJ file and prepare vertex and index data.\"\"\"\n",
    "    scene = pywavefront.Wavefront(obj_file, collect_faces=True)\n",
    "    f = scene.mesh_list[0].faces\n",
    "    v = scene.vertices\n",
    "\n",
    "    fx = []\n",
    "    for i in f:\n",
    "        fx.append([i[0]+1, i[1]+1, i[2]+1])\n",
    "\n",
    "    return v, fx\n",
    "\n",
    "\n",
    "def create_ifc_sign(ifc_file, site, context, vertices, indices):\n",
    "    \"\"\"Create an IfcSign using the geometric data from the OBJ file.\"\"\"\n",
    "    point_list = ifc_file.createIfcCartesianPointList3D(vertices)\n",
    "    faces = [ifc_file.createIfcIndexedPolygonalFace(index) for index in indices]\n",
    "    poly_face_set = ifc_file.createIfcPolygonalFaceSet(point_list, False, faces)\n",
    "    shape_rep = ifc_file.createIfcShapeRepresentation(context, 'Body', 'Tessellation', [poly_face_set])\n",
    "    product_def_shape = ifc_file.createIfcProductDefinitionShape(Representations=[shape_rep])\n",
    "\n",
    "    direction = (0.0, 0.0, 1.0)  # Z-axis pointing up\n",
    "    ref_direction = (1.0, 0.0, 0.0)  # X-axis reference\n",
    "\n",
    "    B1_Point = ifc_file.createIfcCartesianPoint((0.0, 0.0, 0.0))\n",
    "    B1_Axis2Placement = ifc_file.createIfcAxis2Placement3D(B1_Point)\n",
    "    B1_Axis2Placement.Axis = ifc_file.createIfcDirection(direction)\n",
    "    B1_Axis2Placement.RefDirection = ifc_file.createIfcDirection(ref_direction)\n",
    "    B1_Placement = ifc_file.createIfcLocalPlacement(site.ObjectPlacement, B1_Axis2Placement)\n",
    "\n",
    "    sign = ifc_file.createIfcSign(create_ifc_guid(), ifc_file.by_type('IfcOwnerHistory')[0], Name=\"Road Sign\", ObjectPlacement=B1_Placement, Representation=product_def_shape)\n",
    "    ifc_file.createIfcRelContainedInSpatialStructure(create_ifc_guid(), ifc_file.by_type('IfcOwnerHistory')[0], RelatedElements=[sign], RelatingStructure=site)\n",
    "    return sign\n",
    "\n",
    "\n",
    "def add_properties(ifc_file, sign):\n",
    "    \"\"\"Add custom properties to the IfcSign.\"\"\"\n",
    "    pset = ifc_file.createIfcPropertySet(\n",
    "        GlobalId=create_ifc_guid(),\n",
    "        OwnerHistory=ifc_file.by_type('IfcOwnerHistory')[0],\n",
    "        Name=\"Pset_SignCommon\",\n",
    "        Description=\"Custom Properties for Road Sign\",\n",
    "        HasProperties=[\n",
    "            ifc_file.createIfcPropertySingleValue(\"SignID\", \"Identifier of the Sign\", ifc_file.create_entity('IfcIdentifier', ''), None),\n",
    "            ifc_file.createIfcPropertySingleValue(\"SignChainageValue\", \"Chainage value of the Sign\", ifc_file.create_entity('IfcText', ''), None),\n",
    "            ifc_file.createIfcPropertySingleValue(\"Latitude\", \"Latitude of the Sign\", ifc_file.create_entity('IfcReal', 0.), None),\n",
    "            ifc_file.createIfcPropertySingleValue(\"Longitude\", \"Longitude of the Sign\", ifc_file.create_entity('IfcReal', 0.), None),\n",
    "            ifc_file.createIfcPropertySingleValue(\"Altitude\", \"Altitude of the Sign\", ifc_file.create_entity('IfcReal', 0.), None),\n",
    "            ifc_file.createIfcPropertySingleValue(\"BoardFace\", \"Visible face of the board\", ifc_file.create_entity('IfcText', ''), None),\n",
    "            ifc_file.createIfcPropertySingleValue(\"BoardBack\", \"Back face of the board\", ifc_file.create_entity('IfcText', ''), None),\n",
    "            # Add more properties as required\n",
    "        ]\n",
    "    )\n",
    "    ifc_file.createIfcRelDefinesByProperties(create_ifc_guid(), ifc_file.by_type('IfcOwnerHistory')[0], None, None, RelatedObjects=[sign], RelatingPropertyDefinition=pset)\n",
    "\n",
    "obj_file = \"untitled222.obj\"\n",
    "blueprint = 'data/sample.ifc'\n",
    "output_file = \"output_file.ifc\"\n",
    "ifc_file, project, site, context = setup_ifc_file(\"IFC4X3_RC2\", blueprint)\n",
    "vertices, indices = load_obj_data(obj_file)\n",
    "sign = create_ifc_sign(ifc_file, site, context, vertices, indices)\n",
    "add_properties(ifc_file, sign)\n",
    "ifc_file.write(output_file)\n",
    "print(\"Generated IFC file with IfcSign from OBJ data and custom properties.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(8)\n",
    "y = torch.rand(8)\n",
    "print(x[:-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create a grid of x and y values\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.linspace(0, 10, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "# Calculate the loss value based on the given condition\n",
    "threshold = 0.1\n",
    "l_conditional_corrected = np.where(np.abs(x - y) < (threshold*(x+y)), (x + y + (threshold*(x+y))) / 2, np.maximum(x, y))\n",
    "\n",
    "# Create the plot\n",
    "fig = go.Figure(data=[go.Surface(z=l_conditional_corrected, x=x, y=y)])\n",
    "\n",
    "# Update the layout with increased height\n",
    "fig.update_layout(\n",
    "    title='3D Surface plot of l = (x+y)/2 if |x-y| < 1 else max(x,y)',\n",
    "    scene=dict(\n",
    "        xaxis_title='x',\n",
    "        yaxis_title='y',\n",
    "        zaxis_title='l'\n",
    "    ),\n",
    "    height=800  # Increase the height of the plot\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through test set, and filter point clouds with low overlap between partial and gt\n",
    "root = \"/home/haritha/documents/experiments/ICCV2023-HyperCD/ShapeNetCompletion/test/\"\n",
    "testset_path = root + \"partial/\"\n",
    "output_path = root + \"partial_overlap/\"\n",
    "gt_path = root + \"complete/\"\n",
    "save_count = 0\n",
    "total = 0\n",
    "class_folders = os.listdir(testset_path)\n",
    "\n",
    "for class_f in tqdm(class_folders):\n",
    "    print(class_f)\n",
    "    class_path = os.path.join(testset_path, class_f)\n",
    "    output_class_path = os.path.join(output_path, class_f)\n",
    "    gt_class_path = os.path.join(gt_path, class_f)\n",
    "    os.makedirs(output_class_path, exist_ok=True)\n",
    "\n",
    "    obj_folders = os.listdir(class_path)\n",
    "    print(len(obj_folders))\n",
    "\n",
    "    for obj_f in obj_folders:\n",
    "        total+=1\n",
    "        output_obj_path = os.path.join(output_class_path, obj_f)\n",
    "        gt_pcd = np.array(o3d.io.read_point_cloud(os.path.join(gt_class_path, obj_f+\".pcd\")).points)\n",
    "        partial_pcd = np.array(o3d.io.read_point_cloud(os.path.join(class_path, obj_f, \"00.pcd\")).points)\n",
    "        overlap = calc_bbox_overlap(torch.tensor(np.expand_dims(partial_pcd, 0)).cuda(),\n",
    "                                    torch.tensor(np.expand_dims(gt_pcd, 0)).cuda())\n",
    "        overlap = overlap.cpu().numpy()[0]\n",
    "        #print(overlap)\n",
    "\n",
    "        if overlap <= 0.8:\n",
    "            os.makedirs(output_obj_path, exist_ok=True)\n",
    "            o3d.io.write_point_cloud(os.path.join(output_obj_path, \"00.pcd\"),\n",
    "                                    o3d.geometry.PointCloud(o3d.utility.Vector3dVector(partial_pcd)))\n",
    "            save_count += 1\n",
    "\n",
    "print(save_count, total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test for bijectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "\n",
    "# # load data (pointattN) SCD\n",
    "# balanced_path = \"/home/haritha/documents/experiments/PointAttN/outputs_scd/\"\n",
    "# #balanced_path = \"/home/haritha/documents/experiments/PointAttN/outputs_corrected_bbox/\"\n",
    "# #dcd_path = \"/home/haritha/documents/experiments/PointAttN/outputs_cd/\"\n",
    "# load_idx = True\n",
    "# limit = 150\n",
    "# cld_completed = get_cloud_list_vcn(balanced_path, \"pred\", limit=limit, load_idx=load_idx)\n",
    "# cld_gt = get_cloud_list_vcn(balanced_path, \"gt\", limit=limit, load_idx=load_idx)\n",
    "\n",
    "# # # load data (pointattN) UCD\n",
    "# balanced_path = \"/home/haritha/documents/experiments/PointAttN/outputs_ucd/\"\n",
    "# #balanced_path = \"/home/haritha/documents/experiments/PointAttN/outputs_corrected_bbox/\"\n",
    "# #dcd_path = \"/home/haritha/documents/experiments/PointAttN/outputs_cd/\"\n",
    "# load_idx = True\n",
    "# limit = 150\n",
    "# cld_completed = get_cloud_list_vcn(balanced_path, \"pred\", pred_confidence=True, limit=limit, load_idx=load_idx)\n",
    "# cld_gt = get_cloud_list_vcn(balanced_path, \"gt\", pred_confidence=True, limit=limit, load_idx=load_idx)\n",
    "\n",
    "# load data (pointattN) CD\n",
    "balanced_path = \"/home/haritha/documents/experiments/PointAttN/outputs_ucd/\"\n",
    "#balanced_path = \"/home/haritha/documents/experiments/PointAttN/outputs_corrected_bbox/\"\n",
    "#dcd_path = \"/home/haritha/documents/experiments/PointAttN/outputs_cd/\"\n",
    "load_idx = True\n",
    "limit = 150\n",
    "cld_completed = get_cloud_list_vcn(balanced_path, \"pred\", limit=limit, load_idx=load_idx)\n",
    "cld_gt = get_cloud_list_vcn(balanced_path, \"gt\", limit=limit, load_idx=load_idx)\n",
    "\n",
    "cld_completed_tensor = torch.tensor(cld_completed).cuda()\n",
    "cld_gt_tensor = torch.tensor(cld_gt).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pointwise chamfer distance point assignments\n",
    "def calc_pointwise_chamfer_distance(pred, gt):\n",
    "    chamferDist = ChamferDistance()\n",
    "    nn = chamferDist(\n",
    "        pred, gt, bidirectional=True, return_nn=True)\n",
    "    return nn[0].idx, nn[1].idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_0, assignment_1 = calc_pointwise_chamfer_distance(cld_completed_tensor, cld_gt_tensor)\n",
    "assignment_0, assignment_1 = assignment_0.squeeze(), assignment_1.squeeze()\n",
    "print(assignment_0.shape, assignment_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_assignment = torch.gather(assignment_1, 1, assignment_0)\n",
    "\n",
    "cuda = torch.device(\"cuda\")\n",
    "expected = torch.arange(assignment_1.shape[1], device=cuda)\n",
    "expected = expected.repeat(assignment_1.shape[0], 1)\n",
    "\n",
    "print(expected.shape, reverse_assignment.shape)\n",
    "\n",
    "consistency = torch.sum(torch.eq(expected, reverse_assignment).long())/len(reverse_assignment)\n",
    "print(consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "cdbb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
